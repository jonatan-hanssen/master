\documentclass[UKenglish]{uiomasterthesis} %% ... or norsk or nynorsk or USenglish
\usepackage[utf8]{inputenc} %% ... or latin1
\usepackage[T1]{url}\urlstyle{sf}
\usepackage{babel, csquotes, graphicx, textcomp, uiomasterfp, varioref}
\usepackage{amsthm}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{amsmath, amssymb, amsfonts, bm}
\usepackage{acronym}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{array}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{listofitems}
\usepackage{pgfplots}
\usepackage{algpseudocode}
\usepackage{tikzlings}
\usepackage[backend=biber,style=numeric-comp, sorting=none]{biblatex}
\usepackage[hidelinks, hypertexnames=false]{hyperref}


\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

\usetikzlibrary{positioning, matrix, backgrounds}
\usetikzlibrary{3d,decorations.text,shapes.arrows,fit}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{white},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    escapeinside=||,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    frame=single,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    morekeywords={self},
}

\lstset{
	breaklines=true,
	style=pythonstyle,
}


\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}

\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}
  }%
}


\makeatother


\pgfplotsset{colormap={turbo}{
   rgb=(0.19,0.07,0.23)
   rgb=(0.24,0.22,0.58)
   rgb=(0.27,0.37,0.82)
   rgb=(0.28,0.50,0.97)
   rgb=(0.23,0.64,0.99)
   rgb=(0.14,0.76,0.89)
   rgb=(0.09,0.87,0.76)
   rgb=(0.17,0.94,0.62)
   rgb=(0.36,0.99,0.45)
   rgb=(0.56,1.00,0.29)
   rgb=(0.71,0.97,0.21)
   rgb=(0.84,0.90,0.21)
   rgb=(0.94,0.80,0.23)
   rgb=(0.99,0.69,0.21)
   rgb=(0.99,0.54,0.15)
   rgb=(0.95,0.38,0.08)
   rgb=(0.88,0.26,0.04)
   rgb=(0.78,0.16,0.01)
   rgb=(0.65,0.08,0.00)
   rgb=(0.48,0.02,0.01)
}}

\tikzset{pics/fake box/.style args={% #1=color, #2=x dimension, #3=y dimension, #4=z dimension
#1 with dimensions #2 and #3 and #4 and #5}{
code={
\draw[thin]  (0,0,0) coordinate(-front-bottom-left) to
++ (0,#3,0) coordinate(-front-top-right) --++
(#2,0,0) coordinate(-front-top-right) --++ (0,-#3,0) 
coordinate(-front-bottom-right) -- cycle;
\draw[thin] (0,#3,0)  --++ 
 (0,0,#4) coordinate(-back-top-left) --++ (#2,0,0) 
 coordinate(-back-top-right) --++ (0,0,-#4)  -- cycle;
\draw[thin] (#2,0,0) --++ (0,0,#4) coordinate(-back-bottom-right)
--++ (0,#3,0) --++ (0,0,-#4) -- cycle;
\path[decorate,decoration={text effects along path, text={#5}}] (#2/2,{2+(#3-2)/2},0) -- (#2/2,0,0);
}
}}
% from https://tex.stackexchange.com/a/52856/121799
\tikzset{circle dotted/.style={dash pattern=on .05mm off 2mm,
                                         line cap=round}}

\pgfdeclarelayer{foreground}
\pgfsetlayers{background, main, foreground}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\R}{\mathbb{R}}

\providecommand{\mathdefault}[1]{#1}


\definecolor{id}{rgb}{0.4, 0.7607843137254902, 0.6470588235294118}
\definecolor{near}{rgb}{0.5529411764705883, 0.6274509803921569, 0.796078431372549}
\definecolor{far}{rgb}{0.9882352941176471, 0.5529411764705883, 0.3843137254901961}

\definecolor{ca0a0a0}{RGB}{160,160,160}
\definecolor{ce0e0e0}{RGB}{224,224,224}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}

% STYLES
\tikzset{
  >=latex, % for default LaTeX arrow head
  node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
  node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
  node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
  node convol/.style={node,orange!20!black,draw=myorange!30!black,fill=myorange!20},
  node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
  connect/.style={thick,mydarkblue}, %,line cap=round
  connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
  node 1/.style={node in}, % node styles, numbered for easy mapping with \nstyle
  node 2/.style={node hidden},
  node 3/.style={node out}
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3

\title{Explainable Artificial Intelligence for Out-of-Distribution Detection}
\subtitle{Using irregularities in machine learning explanations to detect when a model is faced with unusual data}
\author{Jonatan Hoffmann Hanssen}

\addbibresource{bibliography.bib}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\begin{document}


\uiomasterfp[dept={Department of Informatics},
program={Robotics and Intelligent Systems},
supervisors={Hugo Lewi Hammer \and Kyrre Harald Glette}, image={/home/jona/.config/zsh/cat.jpg},
long]

\frontmatter{}
\begin{abstract}
As Artificial Intelligence becomes a larger and larger part of society, the need for robust and understandable models becomes paramount. When neural networks are used in high-impact settings such as cancer detection or autonomous driving, we must require that they 
\end{abstract}

\begin{xabstract}[Sammendrag]
Here comes the abstract in a different language.
\end{xabstract}


\tableofcontents{}
\listoffigures{}
\listoftables{}

\begin{acronym}[ICANN]
    \acro  {ood}   [OOD]   {Out-of-Distribution}
    \acro  {dl}   [DL]   {Deep Learning}
    \acro  {id} [ID] {In-Distribution}
    \acro  {cam} [CAM] {Class Activation Mapping}
    \acro  {gradcam} [GradCAM] {Gradient Class Activation Mapping}
    \acro  {gap}   [GAP]   {Global Average Pooling}
    \acro  {msp}   [MSP]   {Maximum Softmax Probability}
    \acro  {lime}   [LIME]   {Local Interpretable Model-Agnostic Explanations}
    \acro  {relu}   [ReLU]   {Rectified Linear Unit}
    \acro  {mls}   [MLS]   {Maximum Logit Score}
    \acro  {msp}   [MSP]   {Maximum Softmax Probability}
    \acro  {rmd}   [RMD]   {Relative Mean Absolute Difference}
    \acro  {qcd}   [QCD]   {Quartile Coefficient of Determination}

    \acro  {cnn}   [CNN]   {Convolutional Neural Network}

    \acro  {slic} [SLIC] {Simple Linear Iterative Clustering}

    \acro  {ffnn}[FFNN]{Feed Forward Neural Network}
    \acro  {cv}[CV]{Coefficient of Variance}

    \acro  {gbp}   [GBP]   {Guided Backpropagation}
    \acro  {ml}   [ML]   {Machine Learning}
    \acro  {lrp}   [LRP]   {Layer Relevance Propagation}
    \acro  {xai}   [XAI]   {Explainable Artificial Intelligence}
    \acro  {ai}   [AI]   {Artificial Intelligence}
    \acro  {auroc}   [AUROC]   {Area Under Receiver Operating Characteristic}
    \acro  {aupr}   [AUPR]   {Area Under Precision Recall Curve}
    \acro  {roc}   [ROC]   {Receiver Operating Characteristic}
    \acro  {fpr}   [FPR]   {False Positive Rate}
    \acro  {tpr}   [TPR]   {True Positive Rate}
    \acro  {fpr95}   [FPR95]   {False Positive Rate at 95\% Recall}
\end{acronym}

\begin{preface}
Generative \ac{ai} has not been used to generate or enhance any written text contained in this thesis. However, generative \ac{ai} has been used for some programmatic tasks. Services such as GPT UiO have been used for code debugging, for generating TikZ code used for creating diagrams, and for generating some boilerplate Python code. All data and personal information have been processed in accordance with the University of Oslo's regulations, and I, as the author of the document, take full responsibility for the validity of any generated code used as part of this work.
\end{preface}

\mainmatter{}



\chapter{Introduction}

\section{Motivation}

\ac{ml} generally, and \ac{dl} specifically, have seen a tremendous increase in performance in recent years, performing comparable to humans in tasks such as image classification, speech and handwriting recognition, as well as many others \cite{performance}. Consequently, \ac{dl} methods have been deployed in a multitude of fields and have become a part of our daily lives through their role in web search, text translation, computer vision, and in many other technologies which are taken for granted. In medicine, deep learning has the potential to provide faster and more accurate detection of diseases by being trained on cases from thousands of previous patients \cite{xaisurvey}. Despite this, "surprisingly little in health care is driven by machine learning" \cite{dlmed}.

% TODO: make these citations more specific

To explain this discrepancy, we should consider that despite their impressive performance, the application of deep learning methods is not without drawbacks. Firstly, deep neural networks are inherently unexplainable due to the large number of parameters that any non-trivial network has. State of the art models will perform millions of operations to evaluate a single data point, and it is therefore impossible for humans to comprehend and explain the entire process which lead the model to make a particular decision. In medicine, this is a major limitation of deep learning methods, as both doctors and patients expect to be able to understand why a decision was made \cite{tingsim}.

Secondly, although neural networks may attain high accuracy on test data and appear to have learned great insights about the tasks they are employed in, they often lack robustness and can suffer large drops in performance on data points which are slightly different from the training data. As \cite{intriguing} has shown, it is possible to create data points which are imperceptibly different from normal data points, yet still fool otherwise high performing models. More problematically, unlike humans, who recognize when they are faced with a novel situation where their expertise might be lacking, \ac{dl} methods will predict equally confidently on data points which are far outside the data they have been trained on \cite{tingsim}.

These two problems lead to the fields of \ac{xai}, and \ac{ood} detection. \ac{xai} attempts to explain the reasons why a model came to a decision, which helps to remedy the black-box nature of complicated \ac{dl} models. In a healthcare setting, such explanations can be inspected by medical practitioners to confirm the diagnosis, and can be used to give patients information about why decisions regarding their health were made. \ac{ood} detection attempts to uncover when a data point is too different from the training data to be classified reliably. These methods could alert medical practitioners when such data points occur, thus avoiding potentially fatal misclassifications.

Both of these fields have seen increased interest in recent years, and are vital parts of any integration of \ac{dl} in medical settings. This thesis will focus on \ac{ood} detection, but will attempt to use methods inspired by \ac{xai} to improve detection performance. The overarching intuition is that by inspecting the explanation of a model on a specific data point, we may be able to uncover flaws or irregularities in the explanation which could help us determine whether the data point is \ac{ood}.

% This thesis will attempt to combine the two fields, by using methods inspired by \ac{xai} to improve \ac{ood} detection.

\section{Problem Statement} \label{section:problemstatement}

As explained in the previous segment, \ac{ood} detection is a developing field, which has become more important in recent years as machine learning is being used for higher impact tasks, such as disease detection. Finding novel methods which improve a model's ability to detect when input is \ac{ood} is important to increase the robustness of machine learning models as they are used in these real-world scenarios. The field of \ac{xai} is concerned with understanding the inner workings of a model, and could thus offer insights which could help us detect unusual behaviour in the model as a result of \ac{ood} data points. The problem statement is thus as follows:

\textbf{Can methods from the field of Explainable Artificial Intelligence be used to improve Out-of-Distribution Detection?}
\\

To answer this question, I introduce 2 objectives:

\begin{enumerate}
  % \item Perform comprehensive analysis of the properties of explanations generated by a number of \ac{xai} algorithms on ID and \ac{ood} samples, highlighting differences that could aid \ac{ood} detection.
    \item Develop \ac{ood} detection methods which rely solely on \ac{xai} explanations, and compare these methods 
    \item Develop theoretically sound proof of concept \ac{ood} detection methods that are inspired by insights gained from the field of \ac{xai}, and compare these methods on a variety of different benchmarks against existing \ac{ood} detection methods
\end{enumerate}

\section{Scope} \label{section:scope}

As we will see in chapter \ref{chapter:background}, both the fields of \ac{xai} and \ac{ood} detection are very large, which make it impossible to explore all the possible ways one might combine \ac{xai} and \ac{ood} detection. Thus, it is necessary to restrict the scope of both the \ac{xai} and \ac{ood} detection methods used. In this section, I will describe the choices I've made and give a short explanation. In later chapters, the choices will be justified more thoroughly.

The field of \ac{ood} detection is primarily concerned with image classification tasks. Thus, my project will also deal exclusively with image classification datasets. Given this type of data, the choice of \ac{xai} algorithms naturally gravitates towards post-hoc, saliency based methods. The choice of \ac{ood} detection algorithms is not significantly restricted by the choice to deal exclusively with image data, but I will exclude methods which use outlier exposure to improve performance, or which require retraining of the model. These choices are informed by \cite{openood}, which have found that post-hoc methods

% Use only post-hoc methods, pretrained resnet models. Use well established \ac{ood} datasets first for comparisons. Then focus on medical datasets, hyperkvasir and maybe more. Only use open-source programs. Python and Pytorch. Mainly focus on images. Probably only \ac{cnn} models, as a lot of \ac{xai} is based on that. Maybe also try methods that work on vision transformers as well, but then we are more restricted to post-hoc methods probably.

\section{Research Methods}

Use ACM

\section{Ethical Considerations}


\section{Main Contributions}

Objective 1 is accomplished in chapter \ref{chapter:background}. Here, the fields of \ac{xai} and \ac{ood} detection are introduced, their respective taxonomies are explained and a selection of specific methods are explained in more detail. This chapter provides a good entry for machine learning researchers who wish a decent introduction to the two fields.

\section{Thesis Outline}

Chapter \ref{chapter:background} gives a short introduction to machine learning, followed by a deeper look at the fields of \ac{xai} and \ac{ood} detection. Chapter \ref{chapter:methodology} introduces my methodology; the methods I will use to compare and contrast \ac{xai} explanations on ID and \ac{ood} data, as well as a handful of \ac{ood} detection methods which integrate explanations into their functioning. Furthermore, I introduce the different datasets which will be used in chapter \ref{chapter:experiments}, among them the HyperKvasir gastrointestinal dataset \cite{hyperkvasir}. \ref{chapter:experiments} will first go into the results of the comparison between ID and \ac{ood} explanations, detailing the differences between different methods. Then, I will report the performance of the different \ac{ood} detection algorithms I have developed, on a handful of datasets, including HyperKvasir, CIFAR10 and ImageNet. After the experiments follow a discussion (chapter \ref{chapter:discussion}), where I reflect on the benefits and drawbacks of using \ac{xai} methods for \ac{ood} detection, and summarize the results from \ref{chapter:experiments}. Finally, the conclusion (chapter \ref{chapter:conclusion}) concludes.

\chapter{Background} \label{chapter:background}

In this chapter I give a short introduction to important concepts in the field of machine learning generally, followed by a more in-depth look at the fields of \ac{ood} detection and \ac{xai}. Then, I give an overview of related works; papers which have attempted to use \ac{xai} for \ac{ood} detection. Finally, the datasets used in chapter \ref{chapter:experiments} are covered. 

\section{Machine Learning}

Machine Learning is the field of algorithms that are able to learn from data, as opposed to being explicitly programmed. Such algorithms use statistical methods to learn relationships in data, and use these relationships to generalize to unseen data. More formally, \cite{mitchell} gives the following definition of machine learning algorithms:

\begin{definition}
A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.
\end{definition}

Thus, machine learning is a different paradigm from traditional problem solving, where programs are made to solve problems by following explicit rules. For example, a traditional image classification system attempting to differentiate between malignant and benign tumors might use hand-crafted rules which consider the texture, color and size of a tumor. As one might imagine, such rules will quickly become very complicated when we consider all the possible factors which might influence the appearance of a tumor. Using a machine learning approach, we would instead feed an algorithm with thousands of images of both benign and malignant tumors, and the rules could then be automatically updated until the algorithm predicted the correct category with a high enough accuracy.

Machine Learning is commonly divided into the three subcategories of supervised, unsupervised and reinforcement learning

% A simple comparison is between the two chess engines Stockfish and AlphaZero. Stockfish plays chess by evaluating possible moves according to metrics created by humans\footnote{This is actually no longer the case, as the classical evaluation has been replaced by a neural network (a machine learning approach), however, I will pretend that it still is for the sake of argument}, such as valuing a Queen as equivalent to nine pawns and a 

\subsection{Supervised Learning}

Supervised Learning is a subcategory of machine learning where we have a dataset containing both inputs and desired outputs. In the example above, we could use supervised learning by creating a dataset of images of tumors (the input) and corresponding labels which indicate whether each tumor is malignant or benign (the desired output). The learning goal of the algorithm is then to associate images with the correct label. Because we know the correct answer, we are able to fine-tune the algorithm automatically whenever it makes a mistake. However, supervised learning requires labeled data, which can be very costly, especially in the medical domain, where deciding whether a tumor is malignant or benign requires expert knowledge.

\subsection{Unsupervised Learning}

In unsupervised learning, we do not have any labels. In these cases, we might not know whether data points belong to different classes or not. Instead, we can use machine learning to uncover patterns in the data, for example by attempting to cluster the data into different groups and seeing if these groups are sufficiently separated. An example use case could be for fraud detection in a bank. By feeding financial transaction from many different users into an unsupervised learning model and asking it to perform clustering of the data, it might be possible to find a group of users whose transactions differ substantially from the rest, which might indicate that their transactions are fraudulent.

\subsection{Reinforcement Learning}

Reinforcement Learning deals with problems where we do not know exactly what the correct solution is, but we are able to assess whether a given solution is good or not. For example, when controlling a robot arm, it is difficult to say exactly what angles each joint should be for every millisecond when picking up an object, but if the arm does not pick up the object, we know the algorithm has failed. In these problems, the algorithm is trained through reinforcement, where good attempts are rewarded and bad attempts punished.

\section{Neural Networks}

Neural Networks are a class of machine learning algorithms, which have become the clear state of the art in almost all fields where machine learning is applied. Notable examples are computer vision, image classification, speech recognition, text and image generation and machine translation. Neural networks are loosely inspired by our own brains, where neurons are connected together and send information between each other. By connecting thousands of neurons together, neural networks are able to learn complicated relationships between the input and output.

\subsection{Feed Forward Neural Networks}

The \ac{ffnn}, also known as a Multilayer Perceptron, traces its roots to the very beginning of machine learning, through the work of Frank Rosenblatt \cite{rosenblatt}. It forms the basic structure for neural networks which has been adapted and modified over the years to form more complex architectures such as convolutional, recurrent or residual neural networks. The basic structure of an \ac{ffnn} is that the input values are passed through an affine transformation (a matrix multiplication followed by the addition of a bias), and then passed through an activation function, which produces outputs. These outputs can then go through the same process again, which constitutes a single "layer". By stacking several of these layers, with non-linear activation functions, an \ac{ffnn} is able to learn arbitrarily complex mappings between inputs and outputs\footnote{In fact, by the Universal Approximation Theorem \cite{uat}, only a single hidden layer between the input and output is necessary, although this theorem does not give a way to construct such a network for any given function}. Figure \ref{fig:ffnn}\footnote{Figure by Izaak Neutelings, "Neural Network with coefficients, arrows", TikZ.net, licensed under CC BY-SA 4.0, [\url{https://tikz.net/neural\_networks/}].} shows a simple \ac{ffnn} architecture with three hidden layers. In this case, the bias has been omitted for brevity. Here, we can see how all nodes of a layer are connected to the following layer. By using an activation function on the nodes of the hidden layer, before their values are sent to the next layer, we achieve the non-linearity required to learn complex patterns.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[x=2.2cm,y=1.4cm]
  \message{^^JNeural network with arrows}
  \readlist\Nnod{4,5,5,5,3} % array of number of nodes per layer
  
  \message{^^J  Layer}
  \foreachitem \N \in \Nnod{ % loop over layers
    \edef\lay{\Ncnt} % alias of index of current layer
    \message{\lay,}
    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
    \foreach \i [evaluate={\y=\N/2-\i; \x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
      
      % NODES
      \node[node \n] (N\lay-\i) at (\x,\y) {$a_\i^{(\prev)}$};
      %\node[circle,inner sep=2] (N\lay-\i') at (\x-0.15,\y) {}; % shifted node
      %\draw[node] (N\lay-\i) circle (\R);
      
      % CONNECTIONS
      \ifnum\lay>1 % connect to previous layer
        \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
          \draw[connect arrow] (N\prev-\j) -- (N\lay-\i); % connect arrows directly
          %\draw[connect arrow] (N\prev-\j) -- (N\lay-\i'); % connect arrows to shifted node
        }
      \fi % else: nothing to connect first layer
      
    }
    
  }
  
  % LABELS
  \node[above=0,align=center,mygreen!60!black] at (N1-1.90) {input\\[-0.2em]layer};
  \node[above=0,align=center,myblue!60!black] at (N3-1.90) {hidden layers};
  \node[above=0,align=center,myred!60!black] at (N\Nnodlen-1.90) {output\\[-0.2em]layer};
  

    \end{tikzpicture}
    \end{center}
    \caption[Feed Forward Neural Network]{Figure showing a simple Feed Forward Neural Network, with nodes labeled. The number in parentheses indicates the layer number while the subscript indicates the node number within the layer.}
    \label{fig:ffnn}
\end{figure}



Mathematically, a single layer can then be described as follows:

\begin{equation}
\bm{x_{i+1}} = \sigma_i (A_i \bm{x_i} + \bm{b_i})
\label{ffnn}
\end{equation}

Here, the input $\bm{x_i}$ is linearly transformed by the weights of the matrix $A_i$ from the input space to the output space, then each value of the new vector in the output space is adjusted by an addition of a bias term, and finally an activation function ($\sigma_i$) is applied to each value. The size of the input and output layers is determined by the number of input and output features, respectively. Furthermore, the activation function of the output layer is also determined by the application, most commonly {\it sigmoid} for binary classification, {\it softmax} for multi-class classification and simply identity for regression.


\subsection{Convolutional Neural Networks}

\acp{ffnn} have some inherent flaws which make them unsuitable for working with high dimensional, spatially connected data, such as the pixels which make up an image. Firstly, each input of a \ac{ffnn} is connected to every output of the following layer. If we want to connect the input pixels of a $224$ by $224$ image to a layer of 100 nodes, our first layer will have over 5 million weights, which is already quite a lot for a relatively small image. Furthermore, these weights will have to encode redundant information, because each pixel is considered separately. Consider a network attempting to detect the presence of a cat in an image. We would want the network to detect the cat regardless of whether it is in the middle, the right corner, or any other position in the image. In an \ac{ffnn}, the weights connected to any of these positions in the image would then have to encode a cat detector separately from all the others.

\acp{cnn} solve both these issues by using small kernels of weights which are "slid" across the entire input. By using the same weights across all positions of the image, we do not need to train separate detectors for different positions, giving us translation invariance. Figure \ref{fig:conv} shows the functioning of a convolutional kernel on a 3-channel image. Each value in the output is a weighted sum of a neighbourhood of values in the input image, where the weights are defined by the kernel. As we can see, the same weights are used on all positions, drastically reducing the number of parameters that need to be tuned. In a 2d-convolution, the kernel has the same number of channels as the input, and is only slid across the height and width dimension. The kernel in this figure is a {\it Sobel Operator}, and detects vertical edges. In a \ac{cnn}, the weights of each kernel are not specified manually, but rather learned through backpropagation.

\begin{figure}[H]
    \begin{center}

        \begin{tikzpicture}[
    2d-arr/.style={minimum size=0.8cm, matrix of nodes, row sep=-\pgflinewidth, column sep=-\pgflinewidth, nodes={draw}}
  ]

  \begin{pgfonlayer}{foreground}
      \matrix (mtr) [2d-arr] {
      |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 0 & |[fill=white]| 5 & |[fill=white]| 3\\
      |[fill=white]| 2 & |[fill=white]| 2 & |[fill=white]| 8 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 2\\
      |[fill=white]| 6 & |[fill=white]| 8 & |[fill=white]| 0 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1\\
      |[fill=white]| 9 & |[fill=white]| 3 & |[fill=white]| 7 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 9\\
      |[fill=white]| 1 & |[fill=white]| 3 & |[fill=white]| 6 & |[fill=white]| 5 & |[fill=white]| 2 & |[fill=white]| 8\\
      |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 2 & |[fill=white]| 6 & |[fill=white]| 3 & |[fill=white]| 2\\
      };
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
      \matrix (mtr2) [2d-arr, above right=-7 cm of mtr] {
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
      };
    \end{pgfonlayer}

  \begin{pgfonlayer}{background}
      \matrix (mtr3) [2d-arr, above right=-7 cm of mtr2] {
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
      };
    \end{pgfonlayer}

  \node[below=of mtr-5-3.south east] {Image};

  \node[right=0.2em of mtr3] (str) {$*$};

  \begin{pgfonlayer}{foreground}
  \matrix (K) [2d-arr, right=0.2em of str, nodes={draw, fill=teal!30}] {
    1 & 0 & -1 \\
    2 & 0 & -2 \\
    1 & 0 & -1 \\
  };
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
  \matrix (K2) [2d-arr, below left=-3.965cm of K, nodes={draw, fill=teal!30}] {
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 1 \\
  };
  \end{pgfonlayer}

  \begin{pgfonlayer}{background}
  \matrix (K3) [2d-arr, below left=-3.965cm of K2, nodes={draw, fill=teal!30}] {
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 1 \\
  };
  \end{pgfonlayer}

  \node[below=of K-2-2] {Kernel};

  \node[right=0.2em of K3] (eq) {$=$};

  \matrix (ret) [2d-arr, right=0.2em of eq] {
  4 & 8 & -4 & 2\\
  2 & -2 & 12 & |[fill=blue!80!black!30]| 3\\
  5 & 3 & 8 & 1\\
  3 & 8 & -6 & 8\\
  };
  \node[below=of ret-3-3.south west] {Image $*$ Kernel};

  \begin{pgfonlayer}{foreground}
      \draw[teal] (mtr-4-6.south east) -- (K-3-1.south west);
      \draw[teal] (mtr-2-6.north east) -- (K-1-1.north west);

      \draw[blue!80!black] (K-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
      \draw[teal] (mtr2-4-6.south east) -- (K2-3-1.south west);
      \draw[teal] (mtr2-2-6.north east) -- (K2-1-1.north west);

      \draw[blue!80!black] (K2-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K2-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}

  \begin{pgfonlayer}{background}
      \draw[teal] (mtr3-4-6.south east) -- (K3-3-1.south west);
      \draw[teal] (mtr3-2-6.north east) -- (K3-1-1.north west);

      \draw[blue!80!black] (K3-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K3-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}


\end{tikzpicture}

    \end{center}
    \caption[Convolution example]{Figure showing a convolutional kernel applied to a 3-channel image.}
    \label{fig:conv}
\end{figure}

By using several different kernels, we can detect many different patterns despite each kernel only detecting a single type. By using the outputs of all the kernels as inputs to a new set of kernels, we can use the same type layer structure as in an \ac{ffnn}, allowing us to extract information in a hierarchical manner. It is common to see that trained \acp{cnn} have early layers that detect edges and texture, later layers that use these edge and pattern detections to detect larger shapes, while the final layers combine the shapes to detect entire objects \cite{lenet5}.

By not evaluating every possible position in the input image, \acp{cnn} downsample the image, and are able to reduce the number of operations considerably. Simultaneously, this downsampling enables each subsequent layer to consider a larger area of the input image than the previous (a larger field-of-view), which allows larger patterns to be discovered. Simultaneously, it is common to use a larger and larger amounts of kernels on the new input, thus increasing the channel depth while the spatial dimensions are reduced. Between each layer, we use non-linear activation functions, similarly to how the are used in \acp{ffnn}.

After several such convolutions, we can flatten the output, either by aggregating each channel using \ac{gap} or a similar method, or we can simply flatten all dimensions and consider the three dimensional feature map as a long vector of shape $C \times H \times W$. By doing this, we can pass the output to one or more linear layers, which can perform classification or regression on the extracted features and give us a final prediction. Figure \ref{fig:cnn2} shows a high level overview of this process. Here, the input, which has only 3 channels, has its spatial dimensions reduced while its channel depth increases through consecutive convolutions. Finally, we have a certain number of channels in our final feature map, which are flattened (in this case with \ac{gap}) and processed through a linear layer to give a final prediction.


\begin{figure}[H]
    \begin{center}
    \end{center}


    \newcommand{\drawRectangles}[5]{
    % #1: Starting x-coordinate
    % #2: Starting y-coordinate (top)
    % #3: Rectangle size
    % #4: Spacing between rectangles
    % #5: Number of rectangles


    % Loop to draw rectangles
    \foreach \i in {0, 1, ..., \the\numexpr#5-1} {
        % Calculate x-coordinate for the current rectangle
        \pgfmathsetmacro{\x}{#1 + \i * #4)}
        \pgfmathsetmacro{\y}{#2 - \i * #4)}
        
        \pgfmathsetmacro{\fillColor}{mod(\i, 2) ? "ce0e0e0" : "ca0a0a0"}
        
        % Draw the rectangle
        \path[draw=black, fill=\fillColor, opacity=1, line width=0.0cm] 
            (\x, \y) rectangle (\x + #3, \y - #3);
    }
}


\def \globalscale {0.7}
\begin{tikzpicture}[y=1cm, x=1cm, yscale=\globalscale,xscale=\globalscale, every node/.append, inner sep=0pt, outer sep=0pt]
  \begin{scope}[shift={(3.5, 0.3)}]
    \path[draw=black,fill=red!20,opacity=1, line width=0.0cm] (12.6, 16.2) 
  rectangle (18.6, 10.3);
    \path[draw=black,fill=green!20,opacity=1, line width=0.0cm] (12.7, 16.1) 
  rectangle (18.7, 10.2);
    \path[draw=black,fill=blue!20,opacity=1, line width=0.0cm] (12.8, 16) 
  rectangle (18.8, 10.1);

    \begin{scope}[shift={(14, 11)}]
        \marmot[scale=1.3, body=blue]
        \bear[xshift=2.4cm, yshift=2.5cm, body=blue]
        \coati[rotatehead=-15, xshift=2.6cm, yshift=-0.5cm, body=blue]
    \end{scope}

    \drawRectangles{19.9}{15.0}{3.0}{0.1}{8}

    \drawRectangles{23.9}{14.8}{1.5}{0.12}{16}

    \drawRectangles{26.9}{15.3}{0.4}{0.133}{32}



    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (16.4, 14.3) rectangle (16.6, 14.1);
    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (21.3, 12.4) rectangle (21.7, 12.0);
    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (26.2, 11.9) rectangle (26.4, 11.7);

    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (16.6, 14.1) -- 
  (22.6, 13.3);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (16.6, 14.3) -- 
  (22.6, 13.3);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (21.7, 12.0) -- 
  (26.2, 11.8);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (21.7, 12.4) -- 
  (26.2, 11.8);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (26.4, 11.7) -- 
  (31.1, 11.0);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (26.4, 11.9) -- 
  (31.1, 11.0);

    \path[draw=black,fill=ce0e0e0,opacity=1, line width=0.0mm] (30.8, 13.5) -- 
  (31.1, 13.5) -- (32.0, 12.6) -- (31.7, 12.6) -- (30.8, 13.5)-- cycle;;
    \path[draw=black,fill=ce0e0e0,opacity=1, line width=0.0mm] (32.0, 13.2) -- 
  (32.3, 13.2) -- (32.6, 12.9) -- (32.3, 12.9) -- (32.0, 13.2)-- cycle;;

    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (31.41, 11.15) -- 
  (31.7, 12.6);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (27.3, 15.3) -- 
  (30.8, 13.5);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (32.0, 12.6) -- 
  (32.3, 12.9);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (31.1, 13.5) -- 
  (32.0, 13.2);


    \node[anchor=south west] (text10) at (19.9, 10.0){Conv};
    \node[anchor=south west] (text11) at (24.3, 10.0){Conv};
    \node[anchor=south west] (text12) at (27.9, 10.0){Conv};
    \node[anchor=south west] (text13) at (31.9, 11.0){GAP};
    \node[anchor=south west, align=left] (text14) at (12.6, 16.5){Input RGB Image:\\3@224x224};
    \node[anchor=south west] (text15) at (19.9, 15.4){8@112x112};
    \node[anchor=south west] (text16) at (23.9, 15.2){16@56x56};
    \node[anchor=south west] (text17) at (26.9, 15.7){32@14x14};
    \node[anchor=south west] (text18) at (30.8, 13.9){1x32};
    \node[anchor=south west] (text19) at (32.0, 13.6){1x10};
  \end{scope}



\end{tikzpicture}

    \caption[CNN example]{Figure showing a high level overview of how a CNN functions}
    \label{fig:cnn2}
\end{figure}


% \begin{figure}[H]
%     \begin{center}
%
%
%     \begin{tikzpicture}[x={(1,0)},y={(0,1)},z={({cos(60)},{sin(60)})}, scale=1.5]
% %
% % comment these out if you want to see where the axes point to
% % \draw[-latex] (0,0,0) -- (3,0,0) node[below]{$x$};
% % \draw[-latex] (0,0,0) -- (0,3,0) node[left]{$y$};
% % \draw[-latex] (0,0,0) -- (0,0,3) node[below]{$z$};
% % a plane
% \draw pic (box1-1) at (1,-1.6/2,0) {fake box=white!70!gray with dimensions {1/1.6/1.6} and {2*1.6} and {1*1.6} and Input};
%
% \foreach \X [count=\Y] in {1.4,1.2,1.2,1}
% {
%     \draw pic (box1-\Y) at (\Y+1,-\X/2,0) {fake box=white!70!gray with dimensions {1/\X/\X} and {2*\X} and {1*\X} and Conv};
% }
%
% \foreach \X/\Col in {6.5/blue,6.7/red,6.9/lightgray, 7.1/lightgray, 7.3/green}
% {\draw[canvas is yz plane at x = \X, transform shape, fill =
% \Col!50!white] (0,0.10) rectangle (1,-0.5);}
% % \draw[gray!60,thick] (6.3,-0.1,-1.6) coordinate (1-1) -- (6.3,-0.1,0.6) coordinate (1-2) -- (6.3,2.,0.6) coordinate (1-3) -- (6.3,2.1,-1.6) coordinate (1-4) -- cycle;
% % \draw[gray!60,thick] (7.1,-0.1,-1.6) coordinate (2-1) -- (7.1,-0.1,0.6) coordinate (2-2) -- (7.1,2.,0.6) coordinate (2-3) -- (7.1,2.1,-1.6) coordinate (2-4) -- cycle;
% % \foreach \X in {4,1,3}
% % {\draw[gray!60,thick] (1-\X) -- (2-\X);}
% %
% \node[draw,single arrow, fill=blue!10] at (8,0.5,0) {GAP};
% \node[circle,draw,blue,fill=blue!30] (A1) at (9,1,0) {~~~};
% \node[circle,draw,red,fill=red!30,below=4pt of A1] (A2) {~~~};
% \node[circle,draw,green,fill=green!30,below=18pt of A2] (A3) {~~~};
% \draw[circle dotted, line width=2pt,shorten <=3pt] (A2) -- (A3);
% \node[circle,draw,gray,fill=gray!20] (B1) at (10,1,0) {~~~};
% \node[circle,draw,fill=gray!60,below=4pt of B1] (B2) {~~~};
% \node[circle,draw,gray,fill=gray!20,below=18pt of B2] (B3) {~~~};
% \draw[circle dotted, line width=2pt,shorten <=3pt] (B2) -- (B3);
% \begin{scope}[on background layer]
% \node[orange,thick,rounded corners,fill=blue!10,fit=(A1) (A3)]{};
% \node[gray,thick,rounded corners,fill=gray!10,fit=(B1) (B3)]{};
% \end{scope}
% \foreach \X in {1,2,3}
% {\draw[-latex] (A\X) -- (B2);}
% \end{tikzpicture}
%
%     \end{center}
%     \caption[CNN example]{Figure showing a high level overview of how a CNN functions}
%     \label{fig:cnn2}
% \end{figure}

\section{Model evaluation}

\subsection{Metrics}

\ac{ood} detection is essentially simply a binary classification problem. Thus, the metrics I will use in this thesis are those used for such problems. In the field of \ac{ood} detection, \ac{auroc} and \ac{fpr} are most commonly used. As such I shall focus on these metrics, as opposed to for example the \ac{aupr}.

\subsubsection{Accuracy}

Accuracy is the simplest metric used in binary classification. It is simply the ratio of correct predictions over all instances in the data set. %, as seen in equation \ref{accuracy} below.

% \begin{equation}
% \text{accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
% \label{eq:accuracy}
% \end{equation}

Accuracy has the advantage of being simple to understand and calculate, but it is very often insufficient. A simple (and quite common) scenario where accuracy fails to capture the performance of a model is any situation where there are large class imbalances. For example, imagine we have trained a \ac{cnn} to predict whether a person has lung cancer or not, based on CT-scans of their lungs. As most people do not have lung cancer, we can imagine that such a dataset is highly imbalanced, for example that only 1 in 100 people actually have cancer. If a model simply predicts that no one ever has lung cancer, it will be correct in $99\%$ of cases, and will thus have an accuracy of $99\%$, although it has missed every instance of cancer and is completely unusable in any real context.

For the purposes of \ac{ood} detection, accuracy is thus insufficient, as we have no guarantees that ID and OOD will be balanced. In fact, we expect OOD data to be relatively rare, given that the goal of developing an \ac{ai} model is to ensure high performance during deployment, which necessitates having training data which covers as much as possible of the data seen during inference.

\subsubsection{Metrics utilizing the binary classification confusion matrix} \label{section:aurocfpr95}

Instead of simply considering whether a prediction was correct or not, we should take into account the different combinations of prediction and ground truth, considering positive and negative classes separately. Figure \ref{fig:confusion} shows the possible four possible combinations given a ground truth class and a predicted class.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}
            % Draw the grid
    \draw[thick] (0,0) rectangle (4,4);
    \draw[thick] (2,0) -- (2,4);
    \draw[thick] (0,2) -- (4,2);

    % Label the axes
    \node[align=center] at (1,4.6) {Predicted\\Positive};
    \node[align=center] at (3,4.6) {Predicted\\Negative};

    \node[align=center] at (-1, 3) {Actual\\Positive};
    \node[align=center] at (-1, 1) {Actual\\Negative};

    % Label the cells
    \node[align=center, fill=green!20] at (1,3) {True\\Positive};
    \node[align=center, fill=red!20] at (1,1) {False\\Positive};
    \node[align=center, fill=red!20] at (3,3) {False\\Negative};
    \node[align=center, fill=green!20] at (3,1) {True\\Negative};
    \end{tikzpicture}

\end{center}
    \caption[Binary classification confusion matrix]{Figure showing the binary classification confusion matrix, denoting the four possible combinations created by the ground truth and predicted class. Green cells denote correct predictions, while red cells denote wrong predictions.}
\label{fig:confusion}
\end{figure}

With these possibilities defined, we can begin to gain a clearer picture of the performance of a model.

\noindent \textbf{Precision and Recall}

\noindent Precision is the share of positive predictions that were actually positive. With a high false positive rate, we have a low precision, which means that the model is erroneously flagging many negative classes as positive. In an \ac{ood} detection setting, a model which flags many ID samples as \ac{ood} would have a low precision, if we treat \ac{ood} samples as the positive class.

Recall is the share of actual positive samples that were predicted positive. Recall tells us how many of positive samples we missed. In and \ac{ood} detection context, a model which lets many \ac{ood} samples slip by undetected will have a low recall score.

Precision and recall are often used together because evaluate the model in different ways that complement each other. If a model has both high precision and high recall, it does not erroneously flag many negative classes as positive, nor does it miss many positive classes.

\noindent \textbf{Sensitivity and Specificity}

\noindent Sensitivity and specificity is another pair of metrics that is commonly used for evaluating binary classification. Sensitivity is equivalent to recall; the share of positive samples that were correctly predicted as positive. Specificity is the share of the negative samples that were correctly predicted as negative. Sensitivity and specificity are also known as \ac{tpr} and {\it True Negative Rate}.

\subsubsection{Threshold Independent Metrics}

The previous metrics are a clear improvement over simply using accuracy. However, they still have the problem that they are all dependent on what threshold one sets when predicting something to be a negative or positive class. Thus, it becomes harder to compare different models by using these metrics. Indeed, by simply increasing the threshold of any classifier, we can increase the true negative rate. Similarly, by decreasing the threshold, we can increase the true positive rate.\\

\noindent \textbf{Area under Receiver Operating Characteristic}:

\noindent \ac{auroc} remedies this problem by looking at all possible thresholds, and calculating the \ac{tpr} (equivalent to sensitivity, recall), and the \ac{fpr} (equivalent to $1 -$ specificity) for each possible threshold. With these values calculated, we can plot each point on a graph, giving us an \ac{roc} plot. Figure \ref{fig:auroc} shows this plot, for three different models.

\begin{figure}[H]
    \begin{center}
        \input{figure/auroc.pgf}
    \end{center}
    \caption[Hypothetical ID/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical ID, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the ID and \ac{ood} samples.}
    \label{fig:auroc}
\end{figure}

Once we have done this, we can calculate the integral under this curve, giving us the \ac{auroc}. If a binary classification model can perfectly separate the two classes, then all possible thresholds will either have 100\% \ac{tpr} or 0\% \ac{fpr}, giving an area under the curve of 1. If instead a model has no discriminative power, then the predicted values of positive and negative classes are entirely random, and all changes to the threshold will increase one of the metrics at the expense of the other. Such a model would have \acp{tpr} and \acp{fpr} making a straight line of points, and an \ac{auroc} of 0.5. In between these extremes, we can evaluate different models, without having to consider different thresholds. \ref{fig:auroc} shows what the \ac{roc}-curve of a model with either 0.97 or 0.80 \ac{auroc} looks like, as well as that of a random classifier with \ac{auroc} = 0.50. 

One important thing to note about the \ac{auroc} is that values lower than 0.50 do not mean that a model is worse than random guessing. This is because if a model is consistently wrong, we can simply choose the opposite category of what the model outputs, and gain a new model which is better than random guessing. For example, if a cat versus dog detector gave an actual image dog a higher chance of being a cat than an actual image of a cat in 95\% of cases, it would have an \ac{auroc} of only 0.05. However, if we simply multiplied all outputs of the model by -1, we would suddenly have a model which correctly gives a cat a higher chance of being a cat in 95\% of the cases, and an \ac{auroc} of 0.95. Thus, we really only care about getting \ac{auroc} scores far away from 0.50.\\

\noindent \textbf{False Positive Rate at 95\% Recall}

\noindent \ac{fpr95} is another way of comparing models without having to consider specific thresholds. Instead, we simply select the threshold which gives a recall (equivalent to \ac{tpr}) of 0.95, and calculate the \ac{fpr} at this threshold. The drawback to this metric as opposed to \ac{auroc} is that we do not get a general view of how the model performs. However, if we have a requirement that the model has a very high true positive rate, we may not care about how the model performs at any other threshold, and thus this metric is suitable. It is of course also possible to calculate this metric at any other recall value, depending on the application. However, in the field of \ac{ood} detection, \ac{fpr95} is the metric that is used in the vast majority of cases \cite{oodbaseline, odin, oodoverview, openood, vim}.

\subsection{Statistics: Bootstrapping and T-tests}

Bootstrapping \cite{bootstrap} is a way to get a better estimation of the true generalization error of a model, by performing the same experiments several times on resampled versions of the original dataset.


\section{Explainable Artificial Intelligence} \label{chapter:xai}

Below follows a thorough introduction to \ac{xai}, as well as detailed look at some important methods for explainability for neural networks applied to images. Specifically, saliency methods will be explained in detail, as they constitute a core part of my thesis.

\subsection{The motivation for Explainable Artificial Intelligence}

Given the impressive performance of \ac{dl} methods, one might be convinced that these models do not need to be explainable or interpretable, and that we instead should just place our faith in the model without knowing exactly how it came to a decision. However, as \cite{doshivelez} points out, "a single metric, such as classification accuracy, is an incomplete description of most real-world tasks". Small differences between the data distribution when the test data was collected and when the model is deployed may have a large impact on the model's performance, or the model may have learned artifacts or specificities in the training dataset which were also present in the test dataset, leading to a false belief that the model has gained generalizable knowledge when it has not. By using explainable methods, we may reveal these shortcomings. In my case, this may also have the secondary effect of separating \ac{id} and \ac{ood} data points.

\ac{xai} is also especially important whenever the model is used in settings where its decisions have a high impact. If a model is used by a hospital for disease detection, both the patient and doctor will probably want to be able to understand why the model has found that a disease is present. For them, high performance on a test set of different cases may not be enough. As \cite{xaisurvey} states, "for the regulated healthcare domain, it is utmost important to comprehend, justify, and explain the AI model predictions for a wider adoption of automated diagnosis". In other high impact areas, such as autonomous driving, the impact of wrong decisions by the network can have fatal consequences, and customers and regulators will want to be absolutely sure that the models used are robust and base their decisions on relevant factors as opposed to quirks in the training data. Furthermore, the right to an explanation of an automated decision affecting a person is included in the EU's General Data Protection Regulation, which states that "In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the right [...] to obtain an explanation of the decision reached after such assessment and to challenge the decision." \cite{gdpr}.

% \subsection{The Properties of an Explanation}

\subsection{Taxonomy of Explainable Artificial Intelligence}

This section goes through three axes which define an \ac{xai} method:

\begin{itemize}
  \item Intrinsically explainable models versus post hoc methods
  \item Model dependent versus model agnostic methods
  \item Global versus local explanations
\end{itemize}


\subsubsection{Intrinsically explainable models versus post hoc methods}

Intrinsically explainable models are models which have sufficiently low complexity, such that it is feasible for a human to understand them without further modifications. Examples of such methods are linear regression, logistic regression and decision trees \cite{molnar}. 

Post hoc methods are methods which are applied to the model after training. These methods do not aim to constrain the model to be interpretable, but inspect the model after training. For example, after using a convolutional neural network to classify a CT-scan of a tumour (which gave a prediction of malignant), we could run post hoc algorithms on the network which are able to extract which part of the image contributed the most to the prediction. Thus, post hoc methods remove the need for the model to be simple enough for a human to understand by extracting the relevant information for us.

\subsubsection{Model dependent versus model agnostic methods}

Model dependence/agnosticity denotes whether an \ac{xai} method uses specifics of a particular type of model to generate the explanation, or whether the method can generate an explanation without using specifics of the model at all. Explanations based intrinsically explainable models are clearly model dependent, while methods that only use the input and output of the model instead of looking at the internal operations are model agnostic. An example of a model dependent method (which is not simply an intrinsically explainable model) is Class Activation Mapping, which requires a \ac{cnn} with a specific architecture to function, while an example of a model agnostic method are Shapley values, which use the inputs and outputs to calculate the marginal effect of a single feature on the output value.

\subsubsection{Global versus local explanations}

Global explanations provide general relationships between the input features and outputs learned by the model over the entire dataset \cite{xaioverview}. In this way, they can show how a specific feature affects the output in general, instead of just how it affects the output of a single point. These methods are ideal for finding trends in the data, but may not be suitable for a patient wanting an explanation for their specific case.

Local explanations do not describe general trends, but focus only on a single data point. These methods give insight into how the features influenced the prediction of a single data point, but these relationships may not hold for other data points, and as such these methods do not give the same insight into the general behaviour of the model.

% \subsection{Benchmarking}
%
% In general, it is difficult to evaluate an AI explanations, and there is no clear consensus in the field as to what metrics should be the standard \cite{molnar, evalxai}.

\subsection{\ac{xai} methods adapted to images: Saliency maps and segmentation}


As explained in section \ref{section:scope}, the field of \ac{ood} detection is primarily focused on image data, and as such this is the focus of this thesis as well. Thus, before delving into specific \ac{xai} methods, it is beneficial to elaborate on how \ac{xai} methods are adapted to images. When explaining tabular data made up of categorical and numerical values, it is often common to explain each feature by associating it with some change in the output prediction. For example, one might say that increasing the number of rooms in a house by one increases the predicted sale price by 30 000 NOK, or that the absence of a balcony decreases it by 25 000 NOK. But, given that images are made up of tens or hundreds of thousands of features (pixels), such an approach may not be suitable.

Firstly, given that we have so many features, it may no longer be interesting to know exactly how much each feature contributes to a prediction, given that we don't expect any single pixel to have a very large impact. Furthermore, inspecting the contribution of each individual pixel, like one might do for tabular data, is not even realistically feasible, due the overwhelming number of features. Given that our input is in the form of an image, which is best understood visually, as opposed to numerically, it is then natural to instead present the explanation in the same way.

These visual explanations take the form of saliency maps, as shown in figure \ref{fig:saliency_ex}. Here, we display the saliency values of all pixels in a heatmap, as opposed to numerically. Instead of considering the absolute values, we instead display the saliencies in relation to each other, such that the most important and least important pixel has colors on opposite sides of the colormap. In this way, it is easy and intuitive to see where the important regions of the image are, and we do not need to consider the absolute values of each pixel.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]\textbf{Input image}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/saliency_example/input.png}};
    \node[SIR, label={[align=center]\textbf{Saliency map}}] (id2) [right=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/saliency_example/saliency.png}};


    \begin{scope}[shift={($(id2.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}

    \end{tikzpicture}

    \end{center}
    \caption[Saliency Example]{Figure showing an input image and the corresponding saliency map, generated by an \ac{xai} algorithm which shows the important pixels for the predicted class (Rhodesian Ridgeback). The colorbar and heatmap show the relative saliency for all pixels, with 0 being the least important pixel and 1 being the most important pixel}
    \label{fig:saliency_ex}
\end{figure}

Secondly, many methods which are designed for data tabular data (which have may have 10-20 features) are far too slow when the number of features may be over a hundred thousand. Methods such as \ac{lime}, Occlusion or Shapley fall into this category, as they permute the input at a rate which scales with the number of inputs.

A solution to this is to segment the image into larger regions, which are treated as one. As opposed to asking "how does the prediction change if we change this pixel", we instead ask "how does the prediction change if we change this {\it region}". All pixels in a region are then awarded the same amount of importance. This drastically reduces the dimensionality of the problem, and allows us to use a much larger range of methods.

Regions of pixels can be created in different ways. The simplest option is consider a window of a specific height and width, and slide this window over the image with a specific stride. By adjusting the stride and size, the number of dimensions can be adjusted. The benefit of such a simple approach is that the segmentation itself introduces no extra computation. However, a substantial downside is that each region can contain completely unrelated objects, because the regions are created without considering the underlying image. For example, if a 60 by 60 pixel region contains in its lower right corner a part of a dog, occluding this region could lead to a large change in confidence and thus high importance. However, it is not just the lower right corner that is awarded high importance, but the entire region, which may contain other objects or parts of the background which are not actually important.

By using more sophisticated segmentation methods, we can avoid this problem. Methods such as \ac{slic} \cite{slic} create regions which can be considered more intuitive than simply using a rectangular sliding window. \ac{slic} performs an iterative clustering of pixels, grouping similar pixels into larger regions called superpixels. The downside to this method is that the iterative, CPU-bound process introduces a considerable computational overhead. Figure \ref{fig:segmentationcomp} shows a comparison between the saliency map of the \ac{lime} algorithm applied to an image segmented using rectangular regions and to an image segmented using \ac{slic}. As we can see, the rectangular segmentation means that the some of the grass in the background is given a high saliency, because it happens to be in the same region as the dogs head. Using \ac{slic}, there are few regions which contain both grass and the dog, and thus the high saliency of the dog's face does not affect other parts of the image.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]\textbf{Input image}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_rectangular-img0.png}};
    \node[SIR, label={[align=center]\textbf{\ac{lime} applied to}\\\textbf{rectangular regions}}] (id2) [right=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_rectangular-img1.png}};
    \node[SIR, label={[align=center]\textbf{\ac{lime} applied to}\\\textbf{superpixel regions}}] (id3) [right=of id2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_slic-img1.png}};

    \begin{scope}[shift={($(id3.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}


    \end{tikzpicture}

    \end{center}
    \caption[Segmentation comparison]{Figure showing a comparison between running the \ac{lime} algorithm on rectangular and superpixel regions}
    \label{fig:segmentationcomp}
\end{figure}



\subsection{Specific methods}


The following section goes through several specific \ac{xai} methods. This serves two purposes: Firstly, by explaining how commonly used \ac{xai} methods function in more detail, it is easier to understand how \ac{xai} methods work in practice. Secondly, this section also covers the specific methods that will be analyzed and evaluated as part of the rest of the thesis. By thoroughly detailing the functions of these methods in this chapter, it becomes easier to analyse and explain the proposed methods in chapter \ref{chapter:methodology} and the results in chapter \ref{chapter:experiments}.
\\


\subsubsection{Local Interpretable Model-Agnostic Explanations (LIME)}

\ac{lime} \cite{lime} is a post-hoc, model independent \ac{xai} method. The method is built on the idea that while the decision function of a large neural network (or any other large model) might be far too complex to easily interpret, it can most likely be approximated quite well by a simpler function, as long as we only look at the feature space around a single data point. For example, we could approximate a large feed forward neural network with a simple linear regression model, which can be intrinsically explained due to its low complexity.

To create a locally interpretable model, we need a neighbourhood of data points around our point of interest. To do this, we can sample a number of points from our dataset and weigh them by their distance to our original point. This sampling can be done in many ways, for example by calculating a mean and variance for each feature and sampling from a normal distribution. For image data, we can create new points similar to the image by masking out different regions of the image \cite{molnar}. The distance measure depends on the type of data we are dealing with. Regardless, the distance values are passed through a smoothing kernel which can be tuned to adjust the size of the "neighbourhood".

With these new data points, we can generate new predictions using the original, complex model. Thus, we now have a series of points, each with a weighting based on their distance to our original point, and each with a predicted score from our original model. With such a dataset, we can train a simpler model, which will then approximate the complex model around the point of interest. By inspecting this simple model (for example the learned weights of a linear regression model, or the structure of a decision tree), we can learn approximately how the complex model functions in a region around this single data point.

\subsubsection{Occlusion methods}

Occlusion methods are a family of post-hoc model independent \ac{xai} methods. They function by masking different parts of the image and inspecting the change in output score. If an area leads to a large drop in softmax score for the predicted class when masked, this area must have been important for the network when making the prediction. The mask can be as simple as replacing all masked pixels with a single color, such as gray \cite{occlusion}, or they could use more advanced inpainting methods using generative models, for example by replacing a masked tumor with generated healthy tissue. 

Regardless of the mask, one can easily calculate the importance of any pixel for a prediction by calculating the average change in the output score for all masks which contain the specific pixel \cite{diagnostic}. Occlusion methods have the advantage of being completely model independent, since they do not consider the internals of the model. However, the computation can be expensive, because we need to run a forward pass for each position of the mask on the image. Figure \ref{fig:occlusion} shows the process visually.

\subsubsection{Class Activation Mapping (CAM)}

\ac{cam} \cite{cam} is a model dependent, post hoc \ac{xai} method, which is used on Convolutional Neural Nets (\ac{cnn}s). For a specific output node of a model (for example, the one denoting the presence of a specific class, such as "cat"), \ac{cam} outputs a heat map showing which areas of the input image contributed to this node. In this way, \ac{cam} gives a visual explanation to which parts of an image the model focused on when making a decision to classify an image to a specific class. This method is model dependent, because it requires a specific architecture in the final layers of the network to work.

\ac{cam} is a relatively simple method to understand. It exploits the fact that various convolutional layers of \ac{cnn}s actually behave as object detectors, even when the training objective is classification \cite{cam}. As \cite{lenet5} explains, the earlier layers "extract elementary visual features such as oriented edges, end-points [or] corners", which can be used by subsequent layers to detect higher-order features. In this manner, the final convolutional layer will detect very high level visual features, combining the extracted information from all the previous layers. This layer is composed of several feature maps, where each map can be thought of as denoting the presence of some specific feature across the original image. The authors perform global average pooling (GAP) on these feature maps, giving a single value for each map, which is followed by a single dense layer and the Softmax activation function. In this way, each output node in the final layer is a weighted sum of all the global average pooled feature maps from the final convolutional layer. This means that we can represent the areas of the image which were used to perform the classification by performing the same weighted sum on the actual feature maps instead, which gives us a heat map which we can overlay on the original image (after upsampling the feature maps).

Figure \ref{camimg} shows the process visually. From this we can see that the resulting Class Activation Map (bottom right) gives an intuitive explanation for why the image in the top left gives a high score for the presence of the class "Australian Terrier".

\begin{figure}[h]
\centerline{\includegraphics[width=3.25in]{figure/cam.pdf}}
\caption{Figure taken from \cite{cam}, showing the steps required to create a Class Activation Map}
\label{camimg}
\end{figure}

\begin{figure}[H]
    \begin{center}
    \begin{tikzpicture}[x={(1,0)},y={(0,1)},z={({cos(60)},{sin(60)})}, scale=1.5]
%
% comment these out if you want to see where the axes point to
% \draw[-latex] (0,0,0) -- (3,0,0) node[below]{$x$};
% \draw[-latex] (0,0,0) -- (0,3,0) node[left]{$y$};
% \draw[-latex] (0,0,0) -- (0,0,3) node[below]{$z$};
% a plane
\draw pic (box1-1) at (1,-1.6/2,0) {fake box=white!70!gray with dimensions {1/1.6/1.6} and {2*1.6} and {1*1.6} and Input};

\foreach \X [count=\Y] in {1.4,1.2,1.2,1}
{
    \draw pic (box1-\Y) at (\Y+1,-\X/2,0) {fake box=white!70!gray with dimensions {1/\X/\X} and {2*\X} and {1*\X} and Conv};
}

\foreach \X/\Col in {6.5/blue,6.7/red,6.9/lightgray, 7.1/lightgray, 7.3/green}
{\draw[canvas is yz plane at x = \X, transform shape, fill =
\Col!50!white] (0,0.10) rectangle (1,-0.5);}
% \draw[gray!60,thick] (6.3,-0.1,-1.6) coordinate (1-1) -- (6.3,-0.1,0.6) coordinate (1-2) -- (6.3,2.,0.6) coordinate (1-3) -- (6.3,2.1,-1.6) coordinate (1-4) -- cycle;
% \draw[gray!60,thick] (7.1,-0.1,-1.6) coordinate (2-1) -- (7.1,-0.1,0.6) coordinate (2-2) -- (7.1,2.,0.6) coordinate (2-3) -- (7.1,2.1,-1.6) coordinate (2-4) -- cycle;
% \foreach \X in {4,1,3}
% {\draw[gray!60,thick] (1-\X) -- (2-\X);}
%
\node[draw,single arrow, fill=blue!10] at (8,0.5,0) {GAP};
\node[circle,draw,blue,fill=blue!30] (A1) at (9,1,0) {~~~};
\node[circle,draw,red,fill=red!30,below=4pt of A1] (A2) {~~~};
\node[circle,draw,green,fill=green!30,below=18pt of A2] (A3) {~~~};
\draw[circle dotted, line width=2pt,shorten <=3pt] (A2) -- (A3);
\node[circle,draw,gray,fill=gray!20] (B1) at (10,1,0) {~~~};
\node[circle,draw,fill=gray!60,below=4pt of B1] (B2) {~~~};
\node[circle,draw,gray,fill=gray!20,below=18pt of B2] (B3) {~~~};
\draw[circle dotted, line width=2pt,shorten <=3pt] (B2) -- (B3);
\begin{scope}[on background layer]
\node[orange,thick,rounded corners,fill=blue!10,fit=(A1) (A3)]{};
\node[gray,thick,rounded corners,fill=gray!10,fit=(B1) (B3)]{};
\end{scope}
\foreach \X in {1,2,3}
{\draw[-latex] (A\X) -- (B2);}
\end{tikzpicture}

    \end{center}
    \caption[CNN example]{Figure showing a high level overview of how a CNN functions}
    \label{fig:cam2}
\end{figure}

Although \ac{cam} is an intuitive and effective method of visualizing the inner workings of a \ac{cnn}, it has some downsides. Firstly, it is highly model dependent, requiring that the model only have a single dense layer after the convolutions. Although there are some state of the art models which only use a single dense layer, this still places a limit on what models can be used, or requires the simplification of models that use more than a single dense layer. \cite[4]{cam} notes a 1-2\% drop in classification performance when performing this simplification. Secondly, the output of \ac{cam} is simply a weighted sum of all the feature maps after the final convolutional layer. As we move deeper in a \ac{cnn}, we reduce the spatial resolution by downsampling, while increasing the number of channels (increasing the depth of the output while reducing the height and width). Because of this, the \ac{cam} will have a drastically lower resolution than the original image, often less than $10 \, x \, 10$, while the input image may be hundreds of pixels in both dimensions. Because of this, \ac{cam} can only show general areas, as opposed to pixel wise explanations.
\\

\subsubsection{Gradient Class Activation Mapping (GradCAM)} \label{section:gradcam}

\ac{gradcam} \cite{gradcam} is an improvement on \ac{cam}, which generalizes the method to function with any \ac{cnn} architecture, thus making the method much less model dependent and avoiding the performance drop incurred when simplifying the model with \ac{cam}. Instead of using the weights of a final layer to calculate a weighted sum of feature maps in the last convolutional layer, \ac{gradcam} uses gradients flowing from the relevant output node to the activation maps to calculate the weights for each feature map. Furthermore, the authors prove that this method is a strict generalization of \ac{cam} \cite[5]{gradcam}, so that no information is lost by using gradients instead of weights.

Like the simplicity of the \ac{cam} method, the calculation of the weights using the gradients is also quite simple, as seen in Equation \ref{gradcameq}.

\begin{equation}
\alpha^c_k = \frac{1}{Z} \sum_i \sum_j \frac{\delta y^c}{\delta A^k_{ij}}
\label{gradcameq}
\end{equation}

Here, $c$ represents the index of the class we are interested in, $k$ the index of the feature map, and $i$ and $j$ the width and height of the image. $y^c$ is the element of the output vector $y$ which corresponds to the class $c$, while $A^k$ is the $k$'th feature map. $Z$ is equal to $i * j$, and simply normalizes the sum. Thus, we are actually just performing global average pooling of the gradients of $A^k$ with respect to $y^c$, which gives us a single value we can use as the weight for this feature map. Doing this for all feature maps for a specific class gives us all the weights we need to calculate a weighted sum, which we can upsample and visualize to get an explanation for the decision of the \ac{cnn}.

Thus, \ac{gradcam} improves upon \ac{cam} by making the method less model dependent. However, the explanations are still the same low resolution, which may not be ideal in all cases.
\\

\subsubsection{Guided Backpropagation}

\ac{gbp} \cite{gbp} is another \ac{xai} method which utilizes the gradients of the network to calculate saliencies. In this case, we do not stop at the final feature map, but backpropagate through the entire network to the input image. Simply backpropagating in this way produces a saliency map for the entire input image. However, \cite{gbp} finds that by only backpropagating positive gradients through \acs{relu} functions, they are able to produce "sharper visualizations of descriptive image regions than the previously known methods". Although the choice to simply neglect negative gradients when backpropagating lacks theoretical justification, \ac{gbp} has been shown to be accurate and trustworthy compared to other methods \cite{arras2022clevr, pianpanit2021parkinson}. Compared to the previous methods, \ac{gbp} differs in the sense that it produces a saliency value for every single input feature (every pixel, for example) as opposed to regions.

\subsubsection{Integrated Gradients}

Integrated Gradients \cite{integratedgradients} is another gradient based \ac{xai} method. The method was developed as part of an effort to create an \ac{xai} method which satisfies two axioms; sensitivity, and implementation invariance. Sensitivity states that if an input and a baseline differ in only one input feature, and have different outputs, then this input feature should have non-zero saliency. \cite{integratedgradients} shows that gradients violate this property because "the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline". Implementation invariance states that if two models are functionally identical (their outputs are equal for all inputs), then their attributions should be identical as well. Gradients satisfy this property, but methods which use discrete gradients such as \ac{lrp} \cite{lrp} break this axiom.

Both these axioms represent desirable qualities for an \ac{xai} method; we don't want our attribution method to miss any features which lead to a change in the model prediction, and we don't want the internal structure of the model to affect the attribution if it does not affect the output score in any way. Thus, it is problematic that there are few methods which satisfy both criteria. \cite{integratedgradients} finds that by integrating the gradients of the network in the straight line path between a baseline and the input, both these axioms are satisfied. Mathematically, the saliency for a specific input feature $\mathbf{x}_i$ is defined as follows, given an input $\mathbf{x}$, a baseline $\mathbf{x'}$ and a deep learning model $f$:

\begin{equation}
    \text{IntegratedGradients}_i(\mathbf{x}) := (\mathbf{x}_i - \mathbf{x}_i') \times \int_{\alpha = 0}^1 \frac{\delta f(\mathbf{x}' + \alpha \times (\mathbf{x} - \mathbf{x}'))}{\delta \mathbf{x}_i}d\alpha
\end{equation}

By cumulating the gradients of the network at all points between the baseline and the input, \cite{integratedgradients} manages to combine the implementation invariance of gradients with the sensitivity of techniques like \ac{lrp}. In addition, they prove that if the model $f$ is differentiable "almost everywhere", then the sum of all attributions is equal to the difference in output between the baseline and the input:

\begin{equation}
    \sum_{i=1}^n \text{IntegratedGradients}_i(\mathbf{x}) = f(\mathbf{x}) - f(\mathbf{x'})
\end{equation}

If we then choose a baseline which has $f(\mathbf{x'}) \approx 0$, we see that integrated gradients is equivalent to distributing the output prediction amongst the input features, a very desirable interpretation for an \ac{xai} attribution method.



% \subsubsection{Layer-Wise Relevance Propagation (LRP)}
%
% LRP is another \ac{xai} method which generates a visual explanation of the areas of an image which lead to a classification decision. Unlike \ac{cam} and \ac{gradcam} LRP outputs a map which describes the relevance of every single pixel in the input image, and is thus produces a much more fine grained explanation than these other methods. LRP also differs in that \cite{lrp} does not define it as a specific method, but rather as a concept defined by a certain set of constraints which can be satisfied by different implementations depending on the type of model.
%
% LRP assumes that we can model the relevance $R_i$ for any node $i$ in a neural network, and aims to find the relevance for all the input nodes (the pixels in an image). Relevance is the contribution of any node to the final prediction $f(x)$ of a network, and the idea is to take the relevance of the output layer (simply defined as the output $f(x)$), and iteratively propagate this backwards through the network. Relevance scores are subject to a conservation property, which means that the sum of relevances must be equal for all layers (Equation \ref{lrpeq}). Furthermore, nodes must also conserve relevance, such that the sum of relevances a node receives from the previous layer is equal to the amount it distributes to the next layer. Once the relevance scores have been propagated from the output to the input layer in accordance with these constraints, we have a measure for how each input pixel contributed to the final output.
%
% \begin{equation}
% f(x) = ... = \sum_{d \in l + 1} R^{(l + 1)}_d = \sum_{d \in l + 1} R^{(l + 1)}_d = ... = \sum_d R^{(1)}_d
% \label{lrpeq}
% \end{equation}
%
% To distribute relevance between the nodes in a way which obeys the constraints defined in Equation \ref{lrpeq}, a rule for propagation of relevances must be defined. As \cite{lrp} shows, simply satisfying the constraints is not guaranteed to lead to meaningful explanations, nor is the decomposition of relevance unique. However, they show that by using a suitable propagation rule, we gain a visual explanation which shows which areas contribute to the final decision and which areas make the final decision less likely \cite[28]{lrp}.
% \\



% TODO: Make a picture



\section{Out-of-Distribution Detection} \label{ood_intro}

This section discusses \ac{ood} detection, the field which attempts to tackle the second problem discussed in the introduction; that \ac{ml} models have significantly worse performance on \ac{ood} data points and will often "fail silently", making completely wrong predictions with apparent high confidence \cite{adversarial}. \ac{ood} detection is a developing field, and still in an initial stage \cite{ooddl}. In 2017, \cite{oodbaseline} proposed a baseline \ac{ood} detection method. This section will discuss this method and the methods which follow it.

\subsection{Motivation for Out-of-Distribution Detection}

When training a model using supervised learning, we implicitly use the "closed-world assumption", which means that we assume that test data will be drawn from the same distribution as the training data \cite{oodoverview}. However, when a model is deployed, the data we see may not obey this assumption. Without \ac{ood} detection, the model will behave in the exact same way when encountering \ac{ood} samples or in distribution (ID) samples, and may even claim to be highly confident in its prediction although the sample is far away from the distribution of the training data \cite[1]{energy}. In any system where models make high impact decisions, this is a huge problem. We do not want a model to claim high confidence when predicting if a woman has lung cancer if the model has only been trained on men, nor do we want a model to attempt to classify a rare disease that was not part of the training data. Thus, \ac{ood} detection methods are necessary, so that \ac{ood} samples can be caught before the model makes a prediction and dealt with correctly.

Intuitively, one might assume that distinguishing ID and \ac{ood} samples from each other can be solved by simple binary classification using a dataset of ID samples and one of \ac{ood} samples. Indeed, if one has sufficient amount of high quality \ac{ood} samples, this can be done. However, this can be difficult to obtain in practice \cite[15]{oodoverview}, thus requiring more sophisticated methods of \ac{ood} detection.

\subsection{Semantic versus covariate shift}

The first distinction to make in \ac{ood} detection tasks is whether an \ac{ood} sample is \ac{ood} because of {\it semantic} or {\it covariate} shift. Semantic shift refers to samples with different classes than the ones the model is trained on. A picture of a giraffe would represent a semantic shift for a model trained to differentiate between different breeds of dogs, as a giraffe does not belong to any breed of dog. Covariate shift refers to samples which come from a different distribution while still belonging to one of the classes of the original data set. An image of a Beagle puppy could represent covariate shift for a dog breed classifier despite Beagle being one of the classes, if all ID images were of adult dogs. Likewise, an image of a dog in a dark room could represent covariate shift, if all the ID images were of dogs outside, in well lit conditions. Figure \ref{fig:semanticcovariate} shows this distinction visually. Here, we can easily see the difference between covariate and semantic shift; covariate shifted images come from the same classes as \ac{id} samples, while semantically shifted images come from completely unknown classes.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center, above=0.2cm]\textbf{In Distribution}\\\textbf{images}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id1.jpg}};
    \node[SIR] (id2) [below=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id2.jpg}};
    \node[SIR] (id3) [below=of id2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id3.jpg}};

    \node[SIR, label={[align=center, above=0.2cm]\textbf{Covariate shifted}\\\textbf{images}}, right=of id1] (cov1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov1.jpg}};
    \node[SIR] (cov2) [below=of cov1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov2.jpg}};
    \node[SIR] (cov3) [below=of cov2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov3.jpg}};

    \node[SIR, label={[align=center, above=0.2cm]\textbf{Semantically shifted}\\\textbf{images}}, right=of cov1] (sem1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem1.jpg}};
    \node[SIR] (sem2) [below=of sem1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem2.jpg}};
    \node[SIR] (sem3) [below=of sem2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem3.jpg}};

    \begin{scope}[on background layer={color=id!50}]
        \fill (-2.5,2.15) rectangle (2.5,-12.1);
    \end{scope}

    \begin{scope}[on background layer={color=near!50}]
        \fill (2.5,2.15) rectangle (7.5,-12.1);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (7.5,2.15) rectangle (12.5,-12.1);
    \end{scope}

    \end{tikzpicture}

    \caption[Mean Saliency visual explanation]{Figure showing in-distribution, covariate shifted and semantically shifted images for an imagined dog breed classifier. Images are taken from the ImageNet dataset}
    \label{fig:semanticcovariate}

    \end{center}
\end{figure}

The detection of semantic shift, as opposed to covariate shift, is the main focus of most \ac{ood} detection tasks \cite{oodoverview}. In many applications, it is expected that the model should be able to generalize its prediction to covariate-shifted data, and therefore the focus is on detecting semantic shift. However, the field of medical image classification is one where detecting covariate shift is also important, as the model should only make predictions on data points which are very similar to its training data \cite{oodoverview}.

Given that the detection of semantic shift has been the main focus of most \ac{ood} literature, my work will primarily deal with semantic shift as well. Thus, unless otherwise specified, when I refer to \ac{ood} data points, I mean data points which are semantically shifted, i.e that come from another class than those the model has been trained on.

\subsection{Benchmarking}

The performance of an \ac{xai} is hard to quantify, because the quality of an explanation is not easily reduced to a number. For \ac{ood} detection, performance is much easier to measure, as the problem can be described as a binary classification problem, with \ac{ood} and ID samples as the positive and negative class. Thus, we can calculate many different metrics and compare methods against each other. For \ac{ood} methods, the two most common metrics to report are \ac{fpr95} and the \ac{auroc} (see section \ref{section:aurocfpr95}). It is common to use ImageNet or CIFAR as the ID dataset, and calculate FPR95 and AUROC on other datasets which contain no overlapping class labels. When selecting \ac{ood} datasets, it is common to differentiate between \textbf{near-\ac{ood}} and \textbf{far-\ac{ood}}. Far-\ac{ood} samples are samples which are drastically different from the ID samples, while near-\ac{ood} samples only differ slightly. For our cat-versus-dog classifier, a tiger and a wolf would represent near-\ac{ood} semantic shift, while a plane and a car would represent far-\ac{ood} semantic shift. As one might expect, detecting near-\ac{ood} samples is much harder.

% FPR95 is, as the name implies, the number of false positives (i.e the number of ID data points that the method falsely believes to be \ac{ood}) when 95\% of the \ac{ood} data points are correctly detected. AUROC can defined as the chance that a random ID data point has a higher ID-score than a random \ac{ood} data point \cite{openood}. An AUROC of 0.5 is equivalent to random guessing, while an AUROC of 1 is a perfect model that catches all \ac{ood} data points.


In 2021, \cite{oodoverview} defined a generalized \ac{ood} detection framework, and in 2023 \cite{openood} introduced a comprehensive benchmark of all relevant \ac{ood} methods under this framework, which allows for accurate comparisons of methods within the field. This benchmark is discussed in detail in section \ref{chapter:openood}.

\subsection{Methods}

This section will follow the same outline as section \ref{chapter:xai}; firstly, the overarching categories of methods will be discussed, followed by a more detailed look at a selection of specific methods within the field.
\\

The field of \ac{ood} is separated into four categories of methods \cite{oodoverview}:

\begin{itemize}
  \item Classification-based methods
  \item Density-based methods
  \item Distance-based methods
  \item Reconstruction-based methods
\end{itemize}

All methods can also be categorized by whether they are post-hoc or training based. Post-hoc methods take an already trained network and attempt to extract information which separates ID and \ac{ood} samples out of the network during inference. These methods have the obvious advantage that they can work out of the box with large pre-trained network without requiring expensive training from scratch. Training based methods train the network in ways which maximize the difference between ID and \ac{ood} samples. These methods do not necessarily require \ac{ood} samples, but can train using auxiliary loss functions which amplify the differences in network behaviour when faced with \ac{ood} data as opposed to ID. Regardless, these methods come with a much higher computational requirement than post-hoc methods, as they require training from scratch or at least retraining using the new loss criterion. 

Given the fact that post-hoc methods can be applied to trained networks out of the box, it is quite common to combine both post-hoc and training strategies to achieve the best performance.


Below follows a short explanation of each the four categories mentioned above.
\\

\subsubsection{Classification-based methods}

Classification-based methods usually use the softmax score or logits of a model to attempt to distinguish \ac{ood} and ID samples. \cite{oodbaseline} made the observation that while the softmax score may be a poor indication of the actual confidence of the model on a single data point, it is still higher on average for ID samples as opposed to \ac{ood} samples. By using this simple distinction, they created a baseline model which separated \ac{ood} and ID samples. Using input perturbations and temperature scaling, \cite{odin} further improved on this method, by amplifying the difference in softmax score of ID and \ac{ood} data. 

More generally, classification-based methods do not need to use the softmax score, but may attempt to find any metric which separates the distribution of ID samples from \ac{ood} samples. Figure \ref{fig:ood_metric} shows the probability density for an unspecified metric for both \ac{ood} and ID samples. The goal of classification-based \ac{ood} detection is to find metrics or training methods which make these probability densities have as little overlap as possible, such that they are easily separated by a threshold.

There are several state of the art methods which utilize a classification-based approach, and these make up a large part of the representative methodologies for \ac{ood} detection today \cite[8]{oodoverview}. As such, I shall devote the majority of section \ref{ood_specific} to classification-based methods.

\begin{figure}
    \label{fig:ood_metric}
    \begin{center}
        \input{figure/figure.pgf}
    \end{center}
    \caption[Hypothetical ID/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical ID, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the ID and \ac{ood} samples.}
\end{figure}


\subsubsection{Density-based methods}

Density-based methods explicitly try to model the in-distribution \cite{oodoverview}, which is then used to detect outliers in low likelihood regions. Although the idea is intuitive, learning the distribution of the data set can often be prohibitively expensive, and thus these methods often lag behind classification-based methods \cite{oodoverview}.
\\

\subsubsection{Distance-based methods} \label{section:distancebasedood}

Distance-based methods attempt to detect \ac{ood} samples by calculating their distance to ID samples. Many different distance measures are used, such as Mahalanobis distance to estimated Gaussian distributions, cosine distance to the first singular vector of the data set or Euclidean distance in an embedding space.
\\

\subsubsection{Reconstruction-based methods}

Reconstruction-based methods are based on encoder-decoder frameworks, where the core idea is that the model will be much worse at reconstructing \ac{ood} data than ID. By measuring the reconstruction loss, we can detect \ac{ood} samples.
\\

\subsection{Specific methods} \label{ood_specific}

Below follows a more detailed look a selection of specific \ac{ood} detection methods.

\subsubsection{Baseline model} \label{ood_baseline}

The baseline model created by \cite{oodbaseline} is extremely simple, yet effective. It simply compares the softmax score the predicted class to a threshold, and labels it as \ac{ood} if it falls below this threshold. Let us assume we have a model $f: \bm{x} \rightarrow \R^C$ that takes an input $\bm{x}$ (which may be an image, a vector of values, or something else) and returns a vector of logits with length equal to the number of classes $C$. If we define a softmax score function 

\begin{equation}
    S_i(\bm{x}) = \frac{\text{exp} \, (f_i(\bm{x}))}{\sum^N_{j=1} \text{exp} \, (f_j(\bm{x}))},
\label{softmax}
\end{equation}

then the \ac{ood} detector has the following simple form, given a threshold $\delta$:

\begin{align}
\label{eq:msp}
    g(\bm{x}; \delta)=\begin{cases} 
        \text{in } & \max_i S(\bm{x})\ge \delta \\
        \text{out} & \max_i S(\bm{x}) < \delta 
   \end{cases},
\end{align}

This method reasonably well, because the softmax scores for ID data generally is higher than for \ac{ood} data. Two years later, \cite{mls} showed that this baseline could be improved in settings with larger datasets by forgoing the softmax normalization and instead only looking at the maximum logits, with the even simpler form

\begin{align}
\label{eq:msp}
    g(\bm{x}; \delta)=\begin{cases} 
        \text{in } & \max_i f(\bm{x})\ge \delta \\
        \text{out} & \max_i f(\bm{x}) < \delta 
   \end{cases},
\end{align}

Perhaps surprisingly, both these methods are quite good at detecting \ac{ood} samples. However, there are still many ways to improve these methods, as will be shown in the following sections.
\\

% \subsubsection{Out-of-Distribution Detector for Neural Networks (ODIN)} \label{ood_odin}
%
% \cite{odin} improves on the work of \cite{oodbaseline} by introducing two simple modifications to the method which amplify the difference between the softmax score of ID and \ac{ood} samples. Firstly, they alter the input image slightly by adding small perturbations based on the gradients of the cross-entropy loss, as shown in equation \ref{perturb}
%
% \begin{equation}
% \tilde{\bm{x}} = \bm{x} - \epsilon \, \text{sign}(-\nabla_{\bm{x}} \text{log}_{\hat{y}}(\bm{x};T))
% \label{perturb}
% \end{equation}
%
% Secondly, they add temperature scaling to the softmax calculation, as shown in equation \ref{softmaxtemp}:
%
% \begin{equation}
% S_i(\tilde{\bm{x}} ; T) = \frac{\text{exp} \, (f_i(\tilde{\bm{x}})/T)}{\sum^N_{j=1} \text{exp} \, (f_j(\tilde{\bm{x}})/T)}
% \label{softmaxtemp}
% \end{equation}
%
% Thus, the \ac{ood} detector has the following form, given a threshold $\delta$:
%
% \begin{align}
%     g(\bm{x}; T, \varepsilon, \delta)=\begin{cases} 
%         \text{in } & \max_{i}S(\tilde{\bm{x}};T) \ge \delta \\
%         \text{out} & \max_{i}S(\tilde{\bm{x}};T) < \delta 
%    \end{cases},
% \label{eq:fullodin}
% \end{align}
%
% With these modifications, they report large improvements over the baseline \cite[4]{odin}. To explain this increase, we should look at the mathematical justification for these modifications.
%
% The idea behind perturbing the input image based on the gradient of the cross entropy loss is that ID data points have a higher gradient than \ac{ood} data in general. By moving our data point slightly in the direction of the negative gradient, we should expect to see a higher softmax score than if we did not move, regardless of whether the data point is ID or \ac{ood}. However, because the gradients are larger for ID data, we expect that the difference between the new softmax scores will be larger than they were before the perturbations, because the ID data point has moved further towards higher softmax values, as shown in figure \ref{softmaxmove}, taken from \cite[8]{odin}. Here we see two data points, one ID (red) and one ODD (blue), which are both perturbed. As we can see, the resulting softmax scores after the perturbations differ more than before the perturbation.
%
% \begin{figure}[h]
% \centerline{\includegraphics[width=3.25in]{figure/gradient.png}}
% \caption{Figure taken from \cite{odin}, showing the difference in gradients between ID and \ac{ood} data points}
% \label{softmaxmove}
% \end{figure}
%
% The interpretation of the temperature scaling is slightly more complex. By performing a Taylor expansion and omitting third and higher orders, we can rewrite the softmax score (i.e the value of the predicted class, the highest value) as ${S\propto {(U_{1}-U_{2}/2T)/T}}$, with $U_1$ and $U_2$ defined as follows \cite[4]{odin}:
%
%
% \begin{equation}\label{eq::u1u2}
% U_{1}(\bm{x})=\frac{1}{N-1}\sum_{i\neq\hat{y}}[f_{\hat{y}}(\bm{x})-f_{i}(\bm{x})]
% \end{equation}
%
% \begin{equation}\label{eq::u1u2}
% U_{2}(\bm{x})=\frac{1}{N-1}\sum_{i\neq\hat{y}}[f_{\hat{y}}(\bm{x})-f_{i}(\bm{x})]^{2}
% \end{equation}
%
% $\hat{y}$ is the predicted class, and is thus also the index for the highest value in $f(\bm{x})$. Thus, $U_1$ represents "the extent to which the largest unnormalized output deviates from the remaining outputs", while $U_2$ measures how the remaining outputs deviate from each other \cite[6]{odin}.
%
% \cite{odin} makes the two following observations with regards to these two values: Firstly, they find that the largest unnormalized output tends to deviate more for ID samples, making $U_1$ larger than for \ac{ood} samples, because the model is more confident in its prediction. Secondly, they find that $E[U_2|U_1]$ is larger for ID data samples than for \ac{ood} samples, which shows that ID samples have more separation in the remaining unnormalized inputs than \ac{ood} samples. 
%
% Returning to the Taylor approximated softmax score ${S\propto {(U_{1}-U_{2}/2T)/T}}$, we see that $U_1$ contributes to making the softmax score higher, while $U_2$ reduces the softmax score. Given that both these values are higher for ID data, we will want to reduce the impact of $U_2$ and increase the impact of $U_1$. As $U_1$ is divided by $T$, while $U_2$ is divided by $2T^2$, increasing the temperature achieves this, as $U_2$ will decrease much faster than $U_1$. Thus, we can see how an increased temperature increases the softmax scores for ID data, and thus increases the gap between softmax scores for ID and \ac{ood} samples, making them easier to differentiate.
%
% With these two modifications to the simple baseline proposed by \cite{oodbaseline}, \cite{odin} manages to increase the gap between the softmax scores of ID and \ac{ood} data and thus facilitates much more effective \ac{ood} detection.
% \\
%
% \subsubsection{Energy Based \ac{ood} Detection}
%
% \cite{energy} proposes using an {\it energy score} as opposed to the softmax score. They show mathematically that "the softmax confidence score is a biased scoring function that is not aligned with the density of the inputs" \cite{energy}, and thus seek to use a different measurement which is better aligned with the probability density.
%
% An energy function is a function $E(\bm{x}) : \mathbb{R}^D \rightarrow \mathbb{R}$ which maps any data point into a non-probabilistic scalar called energy. Energy values can be converted to probabilities using the Gibbs distribution defined below (equation \ref{energy}):
%
% \begin{equation} \label{energy}
%     p(y \mid \*x) = \frac{e^{-E(\*x,y)/T}}{\int_{y'} e^{-E(\*x, y')/T}}
%     = \frac{e^{-E(\*x,y)/T}}{e^{-E(\*x)/T}},
% \end{equation}
%
% This equation is quite similar to the softmax function, and we can see that by defining $E(\bm{x}, y) = -f_y(x)$. We can write the Gibbs distribution as the normal softmax output of a neural network:
%
% \begin{equation}\label{eq:softmax}
%     p(y \mid \*x) = \frac{e^{f_y(\*x)/T}}{\sum_{i}^K e^{f_{i}(\*x)/T}},
% \end{equation}
%
% By using the {\it Helmholtz free energy} measurement, we can get an energy score for each data point given to the model, which can be used to detect \ac{ood} data points. Given that we define $E(\bm{x}, y) = -f_y(x)$, we can write the Helmholtz free energy $E(\bm{x})$ as:
%
% \begin{equation}\label{eq:energy_softmax}
%   E(\*x;f)=- T\cdot \text{log}\sum_i^K e^{f_i(\*x)/T}.
% \end{equation}
%
% The authors show that when training with negative log likelihood loss, the optimization will reduce the free energy of ID data points, and that the difference  between ID and \ac{ood} energy scores is higher than the difference in softmax scores \cite{energy}. Thus, thresholding the free energy function is an effective way to separate ID and \ac{ood} data points. Furthermore, they also present a method for fine tuning a pre trained model using a loss function that is based on the energy score. By doing this, the gap between ID and \ac{ood} energy scores can be increased even further.
% \\
%
% \subsubsection{ReAct}
%
% ReAct \cite{react} is a very simple method, which also aims to increase the difference in confidence scores between ID and \ac{ood} data. It does this by rectifying high activations in the penultimate layer, which surprisingly achieves this very effectively. Figure \ref{react} gives an intuition for why this is the case:
%
% \begin{figure}[h]
% \centerline{\includegraphics[width=3.25in]{figure/react.pdf}}
% \caption{Figure taken from \cite{react}, showing the activations for the nodes in the penultimate layer for ID and \ac{ood} data}
% \label{react}
% \end{figure}
%
% From this, we see that \ac{ood} samples have much more irregular activations, with a higher variance and many high value outliers. This gives an explanation for why \ac{ood} samples produce highly confident softmax scores: sharp positive outliers manifest in the model output, producing high logits in the output layer \cite{react}. By using a positive upper limit to rectify these outliers, we can remove their impact and reduce the confidence for \ac{ood} data.
%
% This gives rise to a very simple \ac{ood} detector: Let us denote the feature vector of the penultimate layer as $h(\bm{x})$, where $\bm{x}$ is the input feature vector. The logits of the network would be calculated by the function
%
% \begin{equation}\label{dog}
%   f(\bm{x}) = W \, h(\bm{x}) + \bm{b},
% \end{equation}
%
% where $W$ is a matrix which projects $h(\bm{x})$ down to the output space. $h(\bm{x})$ is the vector which contains the high activations for \ac{ood} data, so by rectifying this vector with $\text{ReAct}(\bm{x}; c) = min(\bm{x}, c)$ for a $c > 0$, we can remove these outlier activations. We then get
%
% \begin{equation}\label{dog}
%   \bar{h}(\bm{x}) = \text{ReAct}(h(\bm{x}; c),
% \end{equation}
%
% which gives us the new output logits 
%
% \begin{align}
% f^{\text{ReAct}}(\*x;\theta) = \*W^\top \bar h(\*x) + \mathbf{b}.
% \end{align}
%
% These logits can be used by any other \ac{ood} method which uses the output values to separate ID and \ac{ood} samples \cite{react}:
%
% \begin{align}
% \label{eq:threshold}
%     g(\bm{x}; f^\text{ReAct}, \delta)=\begin{cases} 
%         \text{in } & S(\bm{x};f^\text{ReAct})\ge \delta \\
%         \text{out} & S(\bm{x};f^\text{ReAct}) < \delta 
%    \end{cases},
% \end{align}
%
% This simple methods performs well on many benchmarks, with the added benefit that it can be combined with many other methods. For example, we can use ODIN or Energy with output scores calculated using ReAct instead of unrectified outputs, which leads to improvements over the methods used by themselves.
% \\
%
% \subsubsection{Virtual Outlier Synthesis (VOS)}
%
% Generating outliers to expose to the model during training is another way to reduce the model's confidence on \ac{ood} data. However, creating realistic \ac{ood} data points can be difficult, especially if the input space is of a high dimension, such as in image classification. \cite{vos} presents a more tractable method, which synthesizes outliers not in the input space, but in the feature space, which can be of a much lower dimensionality.
%
% In this lower dimension space, previously intractable methods are now less computationally expensive. To synthesize outliers, \cite{vos} simply estimates class conditional Gaussian distributions by computing empirical class means and covariances, and sample outliers from the class boundaries between these Gaussians.
%
% Using these outliers, they present a "unknown-aware training objective", which can be used during training to maximize the separability between ID and \ac{ood} data during inference.
% \\
%
% \subsubsection{GradNorm}
%
% As opposed to using the feature or output space, GradNorm \cite{gradnorm} attempts to use the gradient space of a network to calculate \ac{ood}-ness. They find that the gradients of the weights actually contain valuable information that allows for effective separation of ID and \ac{ood} samples, and perform ablation studies which show that this methods outperforms many other methods, including the previously mentioned ODIN and Energy methods.
%
% The gradients are calculated with regards to the Kullback-Leibler divergence between the softmax values and a uniform distribution. An important distinction from other methods is that all the softmax values are used, as opposed to the {\it softmax score} which would be only the score of the predicted class. Thus, this method captures information about the uncertainty across all categories, as opposed to just the most likely class \cite[3]{gradnorm}. Once the gradients have been calculated, the threshold is simply done on the $L_p$-norm of these gradients, giving us the following thresholding function \cite{gradnorm}:
%
% \begin{equation}
% \label{eq:score}
%     S(\*x) = \lVert\frac{\partial D_\text{KL}(\*u~\lVert~\text{softmax}(f(\*x))}{\partial \*w}  \rVert_p
% \end{equation}
%
% As shown in figure \ref{gradnorms}, we see that the gradient norms are consistently lower for \ac{ood} data (gray) than ID data (blue).
%
% \begin{figure}[h]
% \centerline{\includegraphics[width=3.25in]{figure/gradnorm.pdf}}
% \caption{Figure taken from \cite{gradnorm}, showing the difference in gradient norms between ID and \ac{ood} data}
% \label{gradnorms}
% \end{figure}
%
% \cite{gradnorm} find that it is sufficient to only calculate the gradients for the last layer of the network, and that the $L_1$-norm performs the best, as it weights all gradients equally, as opposed to higher norms which place more importance on larger values.
%
% In their mathematical analysis, they show that GradNorm captures joint information from both the feature and output space. By decomposing the $L_1$-norm of gradients of weights of the last layer with regards to the Kullback-Leibler divergence, they reach the following equality:
%
% \begin{equation}
% S(\bm{x}) = \frac{1}{CT}  \left(\sum_{i=1}^m |x_i|\right) \left(\sum_{j=1}^C \left|1 - C \cdot \frac{e^{f_j / T}}{\sum_{j=1}^C e^{{f_{j}} / T}}\right|\right)
% \label{eq:decomp}
% \end{equation}
%
% From this, we see that $S(\bm{x})$ is a product of a factor which is simply the $L_1$-norm of the feature vector $\bm{x}$, and another term which captures information about the softmax values in the output space.
% \\

\subsubsection{Virtual Logit Matching (ViM)}

\cite{vim} attempts to improve \ac{ood} detection by calculating a score based on the feature, the logit and the softmax probability at once, as opposed to just one of them. By looking at all three elements in conjunction, they see an increase in performance over models which only rely on a single input source (such as the previously mentioned ODIN).

The reasoning behind not just looking at the logits or softmax probability is that there is a lot of information that is lost when going from features to logits \cite{vim}. Once we project the features down to logits, we have only class dependent information, and have lost the class agnostic information which is contained within the features. To show how this information is lost, the authors give an example based on null space analysis \cite{nusa}:

Let us assume that we have a simplified network with only a single layer. Then, we have $\hat{\bm{y}} = W \bm{x}$, where $\hat{\bm{y}}$ is the vector containing the logits, $\bm{x}$ is the feature vector of the input (with an additional 1 for the bias term) and $W$ is the matrix containing the weights and biases transforming the feature vector into logits. A null space $\text{Null}(W)$ of a matrix $W$ is the set of all vectors that map to the zero vector, such that $W \bm{a} = \bm{0} \iff \bm{a} \in \text{Null}(W)$. The null space of a matrix may be trivial (empty), but a matrix which projects vectors to a lower dimension have non-trivial null spaces. Given that the final layer of a neural network projects down to logits, which are the same dimension as the number of classes, this will almost always be the case. Because of the distributivity of matrix multiplication, we have the following:

\begin{equation}
W (\bm{x} + \bm{a}) = W \bm{x} + W \bm{a} = W \bm{x} + \bm{0} = W \bm{x}
\label{matrix}
\end{equation}

The vector $\bm{x}$ can be decomposed into $\bm{x}^W + \bm{x}^{\text{Null}(W)}$, where $\bm{x}^W$ is the projection of $\bm{x}$ onto the column space of $W$ and $\bm{x}^{\text{Null}(W)}$ is the projection of $\bm{x}$ onto the null space of $W$. It follows from this and equation \ref{matrix} that when going from features to logits using the projection $W \bm{x}$, we lose all information contained in $\bm{x}^{\text{Null}(W)}$. \cite{nusa} shows how this can be exploited by adversarial methods, by creating images with added noise derived from the null space of a matrix within the network, which are classified as if the noise was not present, despite having no resemblance to the original image. See figure \ref{dog}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.32\linewidth,trim={1.25cm 1.25cm 1.25cm 1.25cm},clip]{figure/OrigImage.pdf}
    \includegraphics[width=0.32\linewidth,trim={1.25cm 1.25cm 1.25cm 1.25cm},clip]{figure/PureNoise.pdf}
    \includegraphics[width=0.32\linewidth,trim={1.25cm 1.25cm 1.25cm 1.25cm},clip]{figure/NoiseAdded.pdf}
    \caption{Image taken from \cite{nusa}. Left: Original image. Center: Additive null space noise. Right: Final image, indistinguishable from original image according to the network the noise in the center column is sampled from.}
    \label{dog}
\end{figure}

From this, we can see that potentially large amounts of information can be lost when going from features to logits. Using this information, it is also possible to perform \ac{ood} detection, as shown by \cite{nusa}. Another method which uses the features performs Principal Component Analysis (PCA) and looks at the residual information lost when using the first $N$ principal components \cite{subspace}. However, the information in the features is still class agnostic, and \cite{vim} aims to go beyond using just one input source and combine several elements of the network.

To do this, they propose using a {\it Virtual Logit}. The Virtual Logit is calculated as follows: First, they center the feature space, so that "it is bias free in the computation of logits" \cite{vim}. They then perform PCA as in \cite{subspace}, and calculate the residual of $\bm{x}$ with regards to the principal components, which is the projection $\bm{x}$ onto the null space of the principal subspace $P$. The residual represents the information lost when using the projection $P$.

\begin{equation}
\text{Residual}(x) = || \bm{x}^{\text{Null}(P)}||
\label{virtuallogit}
\end{equation}

This value is scaled based on the average values of the maximum logit across the dataset, and is appended to the rest of the logits as a Virtual Logit:

\begin{equation}
l_0 := \alpha || \bm{x}^{\text{Null}(P)}||
\label{virtuallogit}
\end{equation}

This now takes part in the computation of the softmax values, and thus is affected by the size of the rest of the logits. They call the softmax value of the Virtual Logit the {\it ViM score}. In this way, the ViM score represents the size of the residual in comparison with the predictions of the model. If the model is very confident, then the norm of the residual will be small in comparison, and the ViM score will be low. If the residual is very large, the ViM score will be higher, and more indicative of an \ac{ood} sample. In this way, \cite{vim} have combined information from the feature, the logit and the softmax probability level to perform \ac{ood} detection.

\section{Related work} \label{section:relatedwork}

While the combination of \ac{xai} and \ac{ood} detection has been explored in many previous works, the majority of them focus on explaining why a data point was marked as \ac{ood}, as opposed to using \ac{xai} to aid the detection itself. Works like \cite{uncertainty}, \cite{generalxaiforood} and \cite{tallon2020explainable} are papers which combine \ac{xai} and \ac{ood} for this purpose. Within network security, \ac{xai} has been as part of anomaly detection systems to detect malicious or faulty network traffic. Here, it has been used to explain detections (\cite{idsxai}, \cite{mahbooba}), but also to aid in detection itself by inspecting the explanations of the detection system (\cite{tcydenova2021detection}, \cite{dnsxai}). These methods thus use \ac{xai} to aid \ac{ood} detection in a similar manner to my work, however they are strictly focused on sequential network traffic data as opposed to images, and are mostly concerned with detection "unnatural" data samples such as intentionally malicious traffic or that generated by faulty equipment, as opposed to natural \ac{ood} data caused by semantic or covariate shift occurring when a model is deployed.

\cite{martinez} is the most relevant previous work. Here, the authors explicitly aim to use \ac{xai} to improve \ac{ood} detection on images. They do this by looking at saliency maps produced by a \ac{gradcam}-based \ac{xai} method (section \ref{section:gradcam}) during inference, i.e the heatmaps that explain which parts of the image was most influential to classify the image as a specific class. Using these heatmaps, they perform distance-based \ac{ood} detection (section \ref{section:distancebasedood}): By collecting all explanations for each image in the ID dataset, they are able to construct archetypical explanations, and can make clusters of explanations. To perform \ac{ood} detection, they simply compare the explanation of a new data point to the clusters of archetypical explanations, and mark it as \ac{ood} if it has a distance which is over a certain threshold.

This method performs decently on toy datasets, achieving scores similar to State-of-the-Art methods when using {\it Fashin MNIST} as ID and {\it MNIST} as \ac{ood}. However, this method fails catastrophically in more complicated scenarios, achieving an AUROC score of only $52\%$ on {\it CIFAR10} vs {\it SVHN}, which is only marginally better than pure guessing and far below any other method. The paper thus ends with the authors concluding that "OoD detection approaches that are specifically designed for the purpose achieve in general better detection scores at the cost of an additional computational burden in the models construction" \cite{martinez}.

For more potential related work, we can look to OpenOOD (\cite{openood}), which aims to provide a comprehensive benchmark of all relevant methods in the field of \ac{ood} detection. Out of all 41 \ac{ood} detection methods included in this benchmark, there are no methods which use \ac{xai}.

From the absence of any relevant method utilizing \ac{xai} in OpenOOD and from the poor results of \cite{martinez}, we can see that the potential for a truly effective \ac{ood} detection system using \ac{xai} has not been fully realized in any previous work.

\section{Summary}

In this chapter, I have given a thorough introduction.

\chapter{Methodology} \label{chapter:methodology}

The core of this work is the 

As shown in the preceding chapter, there exists a plethora of \ac{xai} methods, which exploit gradient information, differences in output scores when altering model inputs, marginal contributions of input features, as well as many other intricacies of deep learning models. The core idea of this master thesis is that these methods, in their attempt to explain a model decision, may also inadvertently extract information which is valuable for \ac{ood} detection. Thus, there may be an unexplored potential in these methods to function not just as explanations, but also as classifiers which allow us to separate ID and \ac{ood} data. Intuitively, we might expect the explanations to be more spread out on \ac{ood} images, given that there are (by definition) no objects of interest in the image that the model can definitely be said to focus on. In contrast, we might expect an explanation on an ID image to be more focused on a specific area, which contains an object of interest. Furthermore, given that saliency methods give a numerical value to each region of the image, we might be able to extract information about the "\ac{ood}-ness" of an image by inspecting the magnitudes of these values. Intuitively, it makes sense that such values should be lower for \ac{ood} than ID, reflecting the higher uncertainty present in \ac{ood} data.

As an example, let us consider \ac{gradcam}. As explained in section \ref{section:gradcam}, \ac{gradcam} uses the gradients of the weights within the final convolutional layer with regard to the output prediction, along with the final feature maps, to generate an explanation.

\section{Proposed \ac{xai} methods for \ac{ood} detection}

In this section, I introduce my proposed methods for \ac{ood} detection, which utilize \ac{xai} methods as part of their detection pipeline.

\subsection{Stand-alone saliency methods}

As we have seen from section \ref{section:relatedwork}, there has been little research into using explanations for \ac{ood} detection, aside from the work of \cite{martinez}. Thus, I begin by presenting a simple baseline method which uses saliency values generated by \ac{xai} methods to calculate an \ac{ood} score.

\subsubsection{Aggregate of Saliency}

As mentioned previously, the field of \ac{ood} detection started with the simple baseline introduced by \cite{oodbaseline}, which simply uses the \ac{msp} (i.e the confidence score of the predicted class) as a way to measure \ac{ood}. As such, I propose a similarly simple baseline when using explanations for \ac{ood} detection. The analogue of the maximum logit in an \ac{xai} context can reasonably be said to be the explanation generated of the predicted class (the class corresponding to the maximum logit). As explained previously, this explanation will take the form of an $n \times m$ saliency map. This saliency map is not a single scalar value, and does thus not make a suitable \ac{ood} score by itself. Instead, we may perform some form of aggregation on the saliency map (such as taking the mean, the max, the variance or some other form), and use this as the \ac{ood} score.

The intuition for this method is informed by the fact that there are many forms of aggregation over saliencies which one might reasonably expect to be different for ID and \ac{ood} data. As an example, let us consider the implications of aggregating in some way which captures the magnitude of the saliencies, such as the {\it mean}, the {\it vector norm}, or the {\it max value}. When we generate a saliency map using the predicted class, the \ac{xai} saliency method attempts to calculate a measure of importance for each region of the input image, with regard to this class. For ID data, as long as the model predicted correctly, we know that there really are regions of the input image which contain the predicted class. If we instead are looking at semantically shifted \ac{ood} data, we know that no input image contains any of the ID classes. Thus, when a neural network makes a prediction on such a data point, it will always be wrong, because it will always predict one of the ID classes. By generating a saliency map of this prediction, we are asking a method to decide how each region contributed to a false decision. Given that there are no objects of the predicted class, in any region, we should expect the saliency values to be lower than in an ID case, where such objects actually are present.

To further illustrate this theoretical justification, I present a simple example scenario (figure \ref{fig:meansaliency}). Here, I imagine a model which has been trained to differentiate between different breeds of dogs. In the first case, it is given an image of a dog, and a prediction of "English Foxhound" is made, which happens to be correct. Generating an explanation for this prediction, each region of the image is given a measure of importance, calculated using gradient information, differences in prediction score on counterfactual examples or by other means. As there actually is an English Foxhound in the image, we expect that these methods generate saliencies which have a magnitude which is higher than if there was no dog present.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]Image with dog present,\\correctly classified as\\"English Foxhound"}] (idimg) {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency0.png}};
    \node[SIR, label={[align=center]Image without dog\\present, wrongly\\classified as "Samyoed"}] (oodimg) [right=of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency2.png}};

    \node[SIR] (idsal) [below=1.5cm of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency1.png}};
    \node[SIR] (oodsal) [below=1.5cm of oodimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency3.png}};

    \node[SIR] (idmean) [below=1.5cm of idsal] {Mean saliency = $0.152$};
    \node[SIR] (oodmean) [below=1.5cm of oodsal] {Mean saliency = $0.034$};

    \draw[->, very thick] (idimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (idsal.north);
    \draw[->, very thick] (oodimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (oodsal.north);

    \draw[->, very thick] (idsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (idmean.north);
    \draw[->, very thick] (oodsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (oodmean.north);

    \node[SIR, align=left, draw=id, fill=id!30] (iddecision) [below=of idmean] {Above threshold:\\classified as ID};
    \node[SIR, align=left, draw=far, fill=far!30] (ooddecision) [below=of oodmean] {Below threshold:\\classified as \ac{ood}};

    \draw[->, very thick] (idmean.south)  to node[midway, fill=white] {Compare with threshold} (iddecision.north);
    \draw[->, very thick] (oodmean.south)  to node[midway, fill=white] {Compare with threshold} (ooddecision.north);

    \begin{scope}[shift={($(oodsal.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=0.345,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.173, 0.345},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}

    \end{tikzpicture}

    \caption[Mean Saliency visual explanation]{Figure showing the functioning of the Aggregate of Saliency \ac{ood} detector, using mean as the aggregation, in a hypothetical scenario where a model trained on images of dogs is shown an image with no dogs present. The heatmaps here show the maximum value of both saliency maps as dark red, reflecting the lack of normalization}
    \label{fig:meansaliency}

    \end{center}
\end{figure}

In the second case, the model is given an image without any dogs present. Given that there is no class for images without dogs in them, the model will classify the image as one of the possible dog breeds. In this case, the model predicted the class of "Samoyed", a decision which can be considered essentially arbitrary. When a explanation is generated for this decision, the methods for calculating importance scores will most likely assign saliencies to most regions, given that no region contains a dog. As such, if we calculate mean saliencies for both the image with the dog and the one without, we expect the image with the dog to have a higher mean saliency. As long as we work with semantically shifted \ac{ood} data, it will always be the case that the prediction on \ac{ood} data is wrong, and thus we may also expect that the generated explanations in general output lower saliencies.

Apart from aggregations which convey information about the magnitude of the saliencies, we may also expect the variation or dispersion of the saliencies to be different between ID and \ac{ood} data. The intuition is that one might expect the heatmap on \ac{ood} data to be less concentrated and more spread out, given that there are no actual objects of interest present.

Figure \ref{fig:varsaliency} shows a hypothetical scenario in which calculating the spread could be beneficial in \ac{ood} detection. Like in the previous case, we imagine a model trained on dogs, which is fed two images, one which contains a dog and one which does not. In this case, we do not care about the magnitudes, but instead only the spread of the values in relation to each other, and thus the heatmap is normalized. As we can see, the heatmap is more spread out in the explanation where there is no dog present than for the image with a dog. By calculating a suitable metric, such as the \ac{rmd}, \ac{cv} or another measure of statistical dispersion, we can get a single number which represents how spread out the heatmap is, regardless of its magnitude. Using these values, we can define a threshold which is below most \ac{id} data and above most \ac{ood} data, allowing us to separate these distributions.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]Image with dog present,\\correctly classified as\\"Golden Retriever"}] (idimg) {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency0.png}};
    \node[SIR, label={[align=center]Image without dog\\present, wrongly\\classified as "Dingo"}] (oodimg) [right=of idimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency2.png}};

    \node[SIR] (idsal) [below=1.5cm of idimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency1.png}};
    \node[SIR] (oodsal) [below=1.5cm of oodimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency3.png}};

    \node[SIR] (idmean) [below=1.5cm of idsal] {GINI = $0.6$};
    \node[SIR] (oodmean) [below=1.5cm of oodsal] {GINI = $0.2$};

    \draw[->, very thick] (idimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (idsal.north);
    \draw[->, very thick] (oodimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (oodsal.north);

    \draw[->, very thick] (idsal.south)  to node[midway, fill=white, align=center] {Calculate variance\\of normalized heatmap} (idmean.north);
    \draw[->, very thick] (oodsal.south)  to node[midway, fill=white, align=center] {Calculate variance\\of normalized heatmap} (oodmean.north);

    \node[SIR, align=left, draw=id, fill=id!30] (iddecision) [below=of idmean] {Above threshold:\\classified as ID};
    \node[SIR, align=left, draw=far, fill=far!30] (ooddecision) [below=of oodmean] {Below threshold:\\classified as \ac{ood}};

    \draw[->, very thick] (idmean.south)  to node[midway, fill=white] {Compare with threshold} (iddecision.north);
    \draw[->, very thick] (oodmean.south)  to node[midway, fill=white] {Compare with threshold} (ooddecision.north);

    \begin{scope}[shift={($(oodsal.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}

    \end{tikzpicture}

    \caption[Spread of Saliency visual explanation]{Figure showing the functioning of the Spread of Saliency \ac{ood} detector in a hypothetical scenario where a model trained on images of dogs is shown an image with no dogs present. The heatmaps are here based on the normalized values of each saliency map, meaning that magnitude information is ignored}
    \label{fig:varsaliency}
    \end{center}
\end{figure}

With the intuition explained, we may now formalize the method. To define this detector mathematically, let us first define the necessary components. As in chapter \ref{chapter:background}, we assume we have a model $f: \bm{x} \to \R^C$. In this case, $\bm{x} \in \R^{D \times H \times W}$, i.e a $D$ channel image of height $H$ and width $W$. In addition, we define a \ac{xai} saliency mapping method $s: (f, \bm{x}) \to \R^{H_s \times W_s}$. This function takes the model $f$ and an input $\bm{x}$ and returns a $H_s$ by $W_s$ saliency map for the predicted class; the class corresponding to the highest logit. Finally, we define an aggregation function $A: \bm{x} \rightarrow \R$, where $\bm{x}$ can be of any shape.

Thus, the \ac{ood} detector has the following form, given a threshold $\delta$:

\begin{align}
    g(\bm{x}; s, A, \delta)=\begin{cases} 
        \text{in } & A(s(\bm{x}, f)) \ge \delta \\
        \text{out} & A(s(\bm{x}, f)) < \delta 
   \end{cases}
\label{eq:aggregate}
\end{align}

An astute reader may note that aggregation functions are permutation invariant, meaning that all positional information from the two-dimensional saliency maps is lost when aggregating. This may seem strange, as it is primarily the positions of the different values that is important when using \ac{xai} methods for explaining model predictions on images. However, there is good reason to believe that for many image classification tasks, the positions of the points of interest in an \ac{xai} saliency map does not carry much discriminative potential. For datasets such as CIFAR10 and ImageNet200, there is a huge variety in the positions of the ground truth class (a dog may appear in the middle, the top right corner, or any other position and still be of the class 'dog'). As such, it is not a given that the removal of any positional information will be massively detrimental. Indeed, when \cite{martinez} reflects upon the poor results of their saliency heatmap clustering for detecting \ac{ood} samples on CIFAR10, it is exactly this variability in position they highlight: "Indeed, the CIFAR10-C dataset does not afford the positional bias and low intra-class variability observed in the previous case studies: informative objects for the classes to be predicted appear in arbitrary parts of the image and have a high degree of compositional variability" \cite{martinez}.

Given the exploratory nature of this thesis, it is reasonable to try many different forms of aggregations. In the experiments, the following aggregations have been used. \\

\textbf{Magnitude:}
\begin{itemize}
    \itemsep0em
    \item Mean
    \item Median
    \item Vector norm
    \item Range
    \item Maximum
    \item Minimum
\end{itemize}

\textbf{Statistical dispersion:}
\begin{itemize}
    \itemsep0em
    \item \ac{cv}
    \item \ac{qcd}
    \item \ac{rmd}
\end{itemize}

These two categories follow the intuition explained above, and will test the two hypotheses that (1) \ac{id} saliency maps have higher magnitude than \ac{ood}, and that (2) \ac{id} saliency maps are more concentrated and less evenly spread out than \ac{ood}, and thus have higher statistical dispersion.

% The magnitude based aggregations are essentially self-explanatory. For the distribution based methods, some explanation is necessary. The \acf{cv} is an alternative to using standard deviation, which removes the effect of the magnitude of the values over which the standard deviation is calculated. The \ac{cv} of a set of values is simply the standard deviation divided by the mean: $CV(\mathbf{x}) = \sigma_x / \mu_x$. For exploring whether the saliencies of \ac{ood} data is more dispersed or spread out than \ac{id} data, such a normalization is required, given that we do not expect the saliencies of \ac{id} and \ac{ood} data to have the same magnitude. Skewness is a measure of asymmetry of a distribution, and can tell us something about the outliers of the saliency maps and their location in relation to the rest of the values. The Gini coefficient is another measure of statistical dispersion, most commonly used to measure income and wealth inequality. The Gini-coefficient of a data set where all values are the same is zero, while it is 1 for a data set where all values are 0 except for one. 

% Algorithm \ref{algo:meansaliency} shows the pseudo-code for the Mean Saliency \ac{ood} Detector. Like the baseline Maximum Softmax and Logit methods introduced by \cite{oodbaseline} and \cite{mls}, it is very simple:
%
% \begin{algorithm}
%     \caption{Mean Saliency \ac{ood} Detector}
%     \label{algo:meansaliency}
%     \begin{algorithmic}
%     \Require Trained Neural Network $f$, Saliency Method $s$, Input Data Sample $x$
%     \Ensure ID Score
%
%     \Function{mean\_saliency}{$f$, $s$, $x$}
%         \State $pred \gets f(x)$
%         \State $class \gets \arg\max(pred)$
%         \State $saliency\_map \gets s(f, x, class)$
%         \State $id\_score \gets \text{mean}(saliency\_map)$
%         \State \Return $id\_score$
%     \EndFunction
%
%     \end{algorithmic}
% \end{algorithm}




% Below (algorithm \ref{algo:varsaliency}), we see the pseudo-code for the Spread of Saliency \ac{ood} Detector, which is equivalent to the Mean Saliency method apart from the aggregation of each saliency map being a measure of spread as opposed to the mean.
%
%
% \begin{algorithm}
%     \caption{Spread of Saliency \ac{ood} Detector}
%     \label{algo:varsaliency}
%     \begin{algorithmic}
%     \Require Trained Neural Network $f$, Saliency Method $s$, Measure of Spread $\sigma$, Input Data Sample $x$
%     \Ensure ID Score
%
%     \Function{spread\_of\_saliency}{$f$, $s$, $\sigma$, $x$}
%         \State $pred \gets f(x)$
%         \State $class \gets \arg\max(pred)$
%         \State $saliency\_map \gets s(f, x, class)$
%         \State $id\_score \gets \sigma(saliency\_map)$
%         \State \Return $id\_score$
%     \EndFunction
%
%     \end{algorithmic}
% \end{algorithm}




\subsubsection{Explanation Clustering}


\subsection{Saliency integrated into existing \ac{ood} detection algorithms}

Given the poor results of \cite{martinez}, one might expect that saliency maps on their own are insufficient to differentiate ID and \ac{ood} data.

\subsubsection{Virtual Logit Matching with Saliencies}


\section{Datasets} \label{section:datasets}

With a thorough introduction to both \ac{xai} and \ac{ood} detection as well as methods which combine the two, I shall present the datasets that will be used to test the new method that I introduce in chapter \ref{chapter:newmethod}. The two datasets used are OpenOOD (\cite{openood}) and HyperKvasir (\cite{hyperkvasir}). OpenOOD is a comprehensive benchmark which can test the new method's performance in a wide range of \ac{ood} scenarios, while HyperKvasir is a medicinal dataset which can be used to test the method in the specific use case of gastrointestinal \ac{ood} detection.


\subsection{OpenOOD} \label{chapter:openood}

As mentioned previously, OpenOOD was introduced in 2023 by \cite{openood} as an attempt to unify the performance metrics of the field, such that accurate comparisons of different methods could be made. Prior to this work, different methods were tested on different datasets, with different image preprocessing procedures, and with other externalities which inhibited effective comparison between methods \cite{openood}.

OpenOOD includes 11 different benchmarks across Anomaly Detection, Open Set Recognition and \ac{ood} detection, three fields which are very closely related. Of these, 6 benchmarks are used to test methods for \ac{ood} detection. Each benchmark is defined by an ID dataset, with 6 or more corresponding \ac{ood} datasets, separated into Near-\ac{ood} and Far-\ac{ood}.

\subsubsection{CIFAR10}

CIFAR10 \cite{cifar} is a 10-class dataset for general object recognition \cite{openood}. This dataset is commonly used in \ac{ai} research \cite{pouyanfar2018survey}. Each image is $32 \times 32$, and there are 50 000 training images and 10 000 test images. The ten classes are as follows: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. OpenOOD presents a series of Near-\ac{ood} and Far-\ac{ood} datasets which are used in conjunction with this dataset for evaluation. Table \ref{table:cifar10} presents a short description of each dataset and the number of samples.

\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c| } 
    \hline
    Dataset & Size & Description \\
    \hline
    \rowcolor{id!50}
    CIFAR10 & 10 000 & General objects \\ 
    \hline
    \rowcolor{near!50}
    CIFAR100 & 10 000 & General objects from same collection as CIFAR10 \\ 
    \rowcolor{near!50}
    TinyImageNet & 10 000 & General objects \\ 
    \hline
    \rowcolor{far!50}
    MNIST & 10 000 & Handwritten digits from 0 to 9 \\ 
    \rowcolor{far!50}
    SVHN & 10 000 & Street view house numbers \\ 
    \rowcolor{far!50}
    Texture & 10 000 & Street view house numbers \\ 
    \rowcolor{far!50}
    Places365 & 10 000 & Places, scenes, locations \\ 
    \hline
    \end{tabular}
    \caption{Hello}
    \label{table:cifar10}
\end{center}
\end{table}

In addition, a sample of images from CIFAR10 and the corresponding \ac{ood} datasets are shown in figure \ref{fig:cifar10}.

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]


    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    \def\rowpadding{0.8cm}

    \pgfmathtruncatemacro{\offset}{\cellheight + \rowpadding}

    % Loop through the grid
    \foreach \row in {0, 1, 2, 3, 4, 5, 6} {
        \foreach \col in {0, 1, 2} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 3 + \col}
            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth, -\row * \offset - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/cifar10_examples/image-img\imagenumber.png}};
        }
    }
    \node [above=0.1cm of 1] {ID: CIFAR10};
    \node [above=0.1cm of 4] {Near-\ac{ood}: CIFAR100};
    \node [above=0.1cm of 7] {Near-\ac{ood}: TinyImageNet};
    \node [above=0.1cm of 10] {Far-\ac{ood}: MNIST};
    \node [above=0.1cm of 13] {Far-\ac{ood}: SVHN};
    \node [above=0.1cm of 16] {Far-\ac{ood}: Texture};
    \node [above=0.1cm of 19] {Far-\ac{ood}: Places365};

    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.2,-3.6);
    \end{scope}

    \begin{scope}[on background layer={color=near!50}]
        \fill (-1.5,-3.6) rectangle (6.2,-9.8);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (-1.5,-9.8) rectangle (6.2,-22.3);
    \end{scope}

    \end{tikzpicture}

    \caption[CIFAR10 dataset example images]{Figure showing three example images from CIFAR10 and from the corresponding \ac{ood} datasets}
    \label{fig:cifar10}

    \end{center}
\end{figure}



\subsubsection{ImageNet200}

\begin{figure}[H]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]


    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    \def\rowpadding{0.8cm}

    \pgfmathtruncatemacro{\offset}{\cellheight + \rowpadding}

    % Loop through the grid
    \foreach \row in {0, 1, 2, 3, 4, 5} {
        \foreach \col in {0, 1, 2} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 3 + \col}
            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth, -\row * \offset - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/imagenet200_examples/image-img\imagenumber.png}};
        }
    }
    \node [above=0.1cm of 1] {ID: ImageNet200};
    \node [above=0.1cm of 4] {Near-\ac{ood}: SSB-Hard};
    \node [above=0.1cm of 7] {Near-\ac{ood}: NINCO};
    \node [above=0.1cm of 10] {Far-\ac{ood}: iNaturalist};
    \node [above=0.1cm of 13] {Far-\ac{ood}: Texture};
    \node [above=0.1cm of 16] {Far-\ac{ood}: OpenImage-O};

    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.2,-3.6);
    \end{scope}

    \begin{scope}[on background layer={color=near!50}]
        \fill (-1.5,-3.6) rectangle (6.2,-9.8);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (-1.5,-9.8) rectangle (6.2,-19.1);
    \end{scope}

    \end{tikzpicture}

    \caption[ImageNet200 dataset example images]{Figure showing three example images from ImageNet200 and from the corresponding \ac{ood} datasets}
    \label{fig:imagenet200}

    \end{center}
\end{figure}
\begin{table}

\begin{center}
\begin{tabular}{ |c|c|c| } 
    \hline
    Dataset & Size & Description \\
    \hline
    \rowcolor{id!50}
    ImageNet200 & 10 000 & General objects \\ 
    \hline
    \rowcolor{near!50}
    SSB-Hard & 10 000 & General objects \\ 
    \rowcolor{near!50}
    NINCO & 10 000 & General objects \\ 
    \hline
    \rowcolor{far!50}
    iNaturalist & 10 000 & Handwritten digits from 0 to 9 \\ 
    \rowcolor{far!50}
    Texture & 10 000 & Street view house numbers \\ 
    \rowcolor{far!50}
    OpenImage-O & 10 000 & General objects \\ 
    \hline
    \end{tabular}
    \caption{Hello}
    \label{table:imagenet200}
\end{center}
\end{table}

\subsection{HyperKvasir} \label{chapter:hyperkvasir}

HyperKvasir is the largest gastrointestinal (GI) machine learning datasets, containing $110 \,079$ images and $374$ videos collected during gastro- and colonoscopy examinations \cite{hyperkvasir}. It is of utmost importance to detect and correctly classify disease found in the GI-tract, as the different diseases can range from minor annoyances to highly deadly diseases such as GI-cancer, which has a mortality rate of $63 \%$ \cite{hyperkvasir}. Furthermore, the effectiveness of endoscopy in locating these diseases is highly dependent on the skill and knowledge of the human operator, with polyps in the colon having a $20 \%$ miss rate \cite{kaminski2010quality}. Thus, this is a field which could benefit greatly from computer aided diagnosis, but such an integration requires robust and trustworthy AI-systems which do not make erroneous decisions with high confidence. This makes HyperKvasir an ideal dataset to test a new \ac{ood} detection method in a more practical real-world scenario.

Of the $100 \, 079$ images, $10 \, 662$ have been labeled, with a total of 23 classes. HyperKvasir also contains segmentations and labeled video, however, I shall limit myself to the labeled images and to the task of classification, as per the scope of this thesis as outlined in section \ref{section:scope}.

\section{Networks}

While it may be interesting to see how these new methods function on different network architectures, the combination of several different novel \ac{ood} detection algorithms and three different datasets already presents a considerable amount of evaluation. Thus, to focus the thesis on comparing different \ac{ood} detection methods against each other, I believe it is best to fix other parameters such as network architecture. With this in mind, I choose to limit myself to the ResNet \cite{resnet} family of neural networks. In particular, I use ResNet18 for all tests, either one trained on $32 \times 32$ images (for Cifar10), or one trained on $224 \times 224$ images (for ImageNet200, HyperKvasir).

Aside from \acp{cnn}, Vision Transformers also perform exceptionally on computer vision tasks, and achieve state-of-the-art results in many settings \cite{vit}. On ImageNet, they are even dominant, and the top 10 models when considering (top-1) accuracy are all based on vision transformers, as opposed to \acp{cnn}.\footnote{https://paperswithcode.com/sota/image-classification-on-imagenet} As such one might question the choice of a \ac{cnn} model, when newer and better models have been developed.

However, given that a large part of the \ac{xai} methods have been developed in the \ac{cnn} paradigm, they are not easily adapted to vision transformers. Methods such as \ac{gradcam} and \ac{lrp} exploit specific parts of \ac{cnn} architectures when generating explanations \cite{legrad}, and are thus difficult to use with different architectures. To be able to use a broad section of the representative \ac{xai} methods in use today, it is thus preferable to use \acp{cnn} as opposed to vision transformers.

\section{\ac{xai} Saliency Methods}

In the following section, I will describe the saliency methods I have chosen to use, and the specific ways I have used them.

\subsection{\ac{lime}}

As is common when applying \ac{lime} to images, I have used segmentations of the input image to reduce the dimensionality of the input that is fed into the \ac{lime} algorithm. During

\subsection{Occlusion}

\subsection{\ac{gradcam}}

\ac{gradcam} is a natural choice of \ac{xai} method, given that is a seminal work which has inspired a large number of other methods \cite{gradcamplusplus, xgradcam, hirescam}.

\subsection{Guided Backpropagation}

\subsection{Integrated Gradients}

\section{Evaluation}

\subsection{Metrics}

\ac{auroc} and \ac{fpr95} are the most common metrics used for \ac{ood} detection \cite{oodbaseline, odin, oodoverview, openood, vim}. In OpenOOD (\cite{openood}), \ac{auroc} is chosen as the primary metric used to rank methods against each other. As \cite{openood} by far presents the most comprehensive complete benchmark of all \ac{ood} detection methods to date, I have followed their methodology and used \ac{auroc} when evaluating my methods, while also calculating \ac{fpr95} as a secondary metric.

\ac{auroc} is agnostic to which class is the positive and which is the negative. Whether we consider \ac{ood} samples as 0 or 1, the result will be the same. However, the same is not true for \ac{fpr95}, necessitating a choice of positive and negative classes. There is no correct answer, but \cite{openood} chooses to consider \ac{ood} samples as the positive class, to "align with ML convention". It is common to consider abnormalities, anomalies, or the unexpected as the positive class (for example, a cancer detection system would consider the presence of cancer to be part of the positive class). This aligns with the goal of \ac{ood} detection, which is to detect abnormal inputs with regard to the data the model is trained on. Thus, I have also followed this convention and considered \ac{ood} samples as positive, and \ac{id} samples as negative. In this case, \ac{fpr95} has the following interpretation: "If a specific threshold leads to 95\% of \ac{ood} samples being correctly classified as \ac{ood}, what percentage of \ac{id} samples are then incorrectly classified as \ac{ood}"? In other words, \ac{fpr95} measures the rate of "false alarms" when 95\% of actual anomalies are caught.

For both of these metrics, the calculations are done on each \ac{ood} dataset individually, as opposed to comparing all \ac{ood} samples to the \ac{id} dataset. Afterwards, the metrics of all Near-\ac{ood} and Far-\ac{ood} are averaged, giving us two general performance metrics which tell us how a method functions on either Near-\ac{ood} or Far-\ac{ood}.

\subsection{Development and Test Sets}

Given the scope of my thesis, which focuses on post-hoc \ac{ood} methods which do not use outlier exposure or require any form of training, there is no need for a training dataset. Despite this, there is still a chance of overfitting, due to the exploratory nature of developing an entirely new class of methods. By trying out different methods and by tuning different parameters to find out how one might best separate \ac{id} and \ac{ood} data using XAI, I am also biasing the results. Therefore, it may be possible that potential improvements in performance are not due to the increased quality of the method, but instead because of peculiarities in the specific datasets that the method is used on.

Thus, I have split all the datasets mentioned in \ref{section:datasets} into development and test sets, and have performed all development of the methods exclusively on the development set. Only when calculating the final metrics have I used the test set, to remove any bias associated with performing multiple experiments on the same data. Due to the large amount of data available in the CIFAR10 and ImageNet200 datasets, and their associated \ac{ood} datasets, I have simply split all datasets into two equally large parts.

\subsection{Bootstrapping}

When evaluating a new method, it is not enough to simply report the results from a single experiment. Instead one should run the same experiment multiple times and perform statistical analysis to ensure that the results are robust. Therefore, I have bootstrapped the test set ten times during the calculation of the final test results. This makes it possible to report means and standard deviations of the results as opposed to a single point estimate. With these repeated experiments, it is also possible to perform t-tests comparing the new methods against baseline \ac{ood} detection methods.

\section{Implementation}

This section goes over the implementation details of my thesis.

\subsection{Basic hardware and software}

Python is the most popular programming language for data science and \ac{ml} research \cite{nguyen2019machine}, and as such it is my language of choice as well. Key libraries that I have used are {\it PyTorch}, {\it OpenOOD}, {\it Captum}, {\it Matplotlib}, and others. Below is a table of the key libraries and their version numbers.

\begin{center}
    \begin{tabular}{ |c|c|c| } 
    \hline
    Library & Version number & Short description \\
    \hline
    PyTorch & 2.4.1 & GPU-accelerated \ac{ml} library \\ 
    OpenOOD & 1.5 & Comprehensive \ac{ood} detection framework \cite{openood} \\ 
    Captum & 0.7.1 & \ac{xai} methods integrated with PyTorch \\ 
    Scikit-Learn & 1.5.2 & Various \ac{ml} methods \\ 
    NumPy & 1.26.4 & Efficient matrix multiplication and scientific computing \\ 
    Matplotlib & 3.9.2  & Visualization library \\ 
    Seaborn & 0.13.2 & Visualization library based on Matplotlib \\
    \hline
    \end{tabular}
\end{center}

All development and computation was done on a single computer with an \texttt{Intel i7-8700K} CPU and an \texttt{Nvidia GeForce RTX 3090} GPU.

\subsection{Method Evaluation: OpenOOD}

As explained previously, OpenOOD \cite{openood} represents the most comprehensive benchmark for \ac{ood} detection methods. It is also a framework which easily allows for development and benchmarking of new methods, and is thus the ideal framework for the purposes of this thesis.

In particular, OpenOOD includes a "unified, easy to use evaluator" \cite{openood15} that makes evaluating new methods very simple. All that is required is that new methods inherit from a base class (\texttt{BasePostprocessor}\footnote{In OpenOOD, \texttt{Postprocessor}s are the \ac{ood} detection algorithms that generate an \ac{ood} score during inference}), and override the calculation of \ac{ood} scores. Code listing \ref{code:aggregate} shows all the code required to create the Aggregate of Saliency \ac{ood} detector.

\begin{lstlisting}[style=pythonstyle, caption={Source code listing for the Aggregate of Saliency \ac{ood} detector}, captionpos=b, label={code:aggregate}]
class SaliencyAggregatorPostprocessor(BasePostprocessor):
    def __init__(self, config, saliency_generator, aggregator):
        super().__init__(config)

        self.saliency_generator = saliency_generator
        self.aggregator = aggregator

    def postprocess(self, net: nn.Module, data: Any):
        predictions = torch.argmax(net(data), dim=-1)

        saliencies = self.saliency_generator(net, data)
        score_ood = self.aggregator(saliencies, dim=-1)

        return predictions, score_ood
\end{lstlisting}

With this \texttt{postprocessor} defined, evaluating it on a specific dataset is similarly simple (listing \ref{code:eval}):

\begin{lstlisting}[style=pythonstyle, caption={Source code listing for evaluating methods within the OpenOOD framework}, captionpos=b, label={code:eval}]
resnet18_pretrained = get_network('cifar10')

ood_detector = SaliencyAggregatorPostprocessor(None, GradCAM, torch.mean)

evaluator = Evaluator(
    net=resnet18_pretrained,
    id_name='cifar10',
    postprocessor=ood_detector,
)

metrics = evaluator.eval_ood()

print(metrics)
\end{lstlisting}

This code will calculate the \ac{ood} scores for all data samples in both the \ac{id} and \ac{ood} datasets, and subsequently calculate the \ac{auroc} and \ac{fpr95} for all the \ac{ood} datasets when comparing their \ac{ood} values to the values of the \ac{id} datasets. Code listing \ref{code:msp} shows this output when using the baseline \ac{msp} method.

\begin{lstlisting}[style=pythonstyle, caption={Output of calling \texttt{evaluator.eval\_ood} with CIFAR10 as the dataset and \ac{msp} as the detector}, captionpos=b, label={code:msp}]
           FPR@95  AUROC  AUPR_IN  AUPR_OUT   ACC
cifar100    61.36  86.51    84.20     85.05 95.56
tin         42.02  88.88    88.81     85.57 95.56
nearood     51.69  87.69    86.51     85.31 95.56 # average of two above
mnist       19.38  93.86    79.72     98.89 95.56
svhn        24.78  91.38    84.26     95.49 95.56
texture     43.31  88.68    91.01     80.97 95.56
places365   41.62  89.21    68.49     96.28 95.56
farood      32.27  90.78    80.87     92.91 95.56 # average of four above
\end{lstlisting}

As we can see, OpenOOD allows for very easy evaluation of new methods. Furthermore, it allows for easy comparisons between methods, one of the stated goals of the framework \cite{openood}. This makes it an ideal framework for this thesis.


\subsubsection{Modifications to OpenOOD}

As explained previously, continually testing new methods on the same datasets will bias the final results. As of the writing of this thesis, there are no functionalities in OpenOOD which allow for creating development or test sets, all evaluations are done with entire datasets. For my purposes, which involve continuous exploration of different methods and careful inspection of the datasets, this is inadequate. Thus, I have made modifications to the \texttt{Evaluator} class such that it takes a \texttt{data\_split} parameter during initialization, and have also modified the function \texttt{get\_id\_ood\_dataloader} to accept this parameter and return the correct split accordingly.

Furthermore, there is no functionality for sampling from the datasets as opposed to using them as they are, which is necessary to perform bootstrapping and calculate the statistical significance of the results. This has been done by passing a seed to the \texttt{Evaluator} class, which, if defined, will be used to seed a random sampling operation on the datasets. By instantiating several \texttt{Evaluator} classes with different seeds, we can bootstrap the evaluation and perform statistical analysis on the results.

% Listing \ref{code:split} shows the relevant snippet of code from  \texttt{get\_id\_ood\_dataloader} which enables both dataset splitting and bootstrapping:
%
% \begin{lstlisting}[style=pythonstyle, caption={Code snippet from \texttt{get\_id\_ood\_dataloader} which contains the modifications necessary for splitting and bootstrapping the datasets}, captionpos=b, label={code:split}]
% # This code is run once for the ID dataset and once for all OOD datasets
% sampler = None
% if data_split is not None:
%     # Always seed with the same value to ensure
%     # development and test sets always are the same
%     generator = Generator().manual_seed(0)
%     val_set, test_set = random_split(
%         dataset, [0.5, 0.5], generator=generator
%     )
%
%     if data_split == 'val':
%         dataset = val_set
%     elif data_split == 'test':
%         dataset = test_set
%     else:
%         raise ValueError("data_split only accepts 'val' or 'test'")
%
%     # Create a sampler if bootstrap seed is set, otherwise it is None
%     if bootstrap_seed is not None:
%         # Variable seed allows for new randomly sampled 
%         # datasets each time
%         generator = Generator().manual_seed(bootstrap_seed)
%         sampler = RandomSampler(
%             dataset, replacement=True, generator=generator)
% # If sampler=None, this just makes a dataloader, if sampler 
% # is a RandomSampler the dataloader samples with replacement
% # from the original dataset
% dataloader = DataLoader(dataset, sampler=sampler, **loader_kwargs)
% \end{lstlisting}

\subsection{Implementation of Saliency Methods}

There are many libraries which implement \ac{xai} methods \cite{lime, captum, jacobgilpytorchcam}. When these implementations were suitable for my purposes, I used them. However, given I am not using these explanations for their original purpose (elucidating why a model came to a specific decision), there are many cases where the current implementations are inadequate. The two main problems are lack of access to the raw saliency values and slow speeds.

\subsubsection{\ac{gradcam}}

\ac{gradcam} has been implemented from scratch in \texttt{PyTorch}. There are several libraries which implement \ac{gradcam} \cite{jacobgilpytorchcam, captum}. However, given that these libraries are concerned with simply producing heatmaps that users can inspect, they do not output the raw saliency values, but upscale the saliencies to match the input image dimensions. As explained in \ref{section:gradcam}, \ac{gradcam} uses the final feature map to generate an explanation, which usually has a spatial dimension which is far smaller than the input image (e.g. $7 \times 7$ versus $224 \times 224$). When overlaying these values on the input image, it is common to use bilinear interpolation \cite{jacobgilpytorchcam}, which interpolates all $224 \times 224$ positions based on the original $7 \times 7$ saliency map. For visualizations, this is reasonable. When attempting to use this data to separate \ac{id} and \ac{ood} data however, this is undesirable. Bilinear interpolation introduces new values, which changes many statistical qualities of the saliency values. This may reduce the separability of different samples. Furthermore, given that these new values do not add any new information, it is inefficient to involve these upscaled saliency maps in any computational operation.

Although it is relatively simple to modify the source code of these libraries to remove the interpolation, \ac{gradcam} is not very difficult to implement, and as such I have simply used my own implementation.

% Listing \ref{code:gradcam} shows the Python code.
%
% \begin{lstlisting}[style=pythonstyle, caption={Source code listing for my \ac{gradcam} implementation}, captionpos=b, label={code:gradcam}]
% class GradCAM(torch.nn.Module):
%     def __init__(self, model, target_layer):
%         super().__init__()
%         self.model = model
%         self.target_layer = target_layer
%         self.grads = None
%         self.acts = None
%
%         self.handles = list()
%         self.handles.append(
%             self.target_layer.register_full_backward_hook(self.grad_hook)
%         )
%         self.handles.append(
%             self.target_layer.register_forward_hook(self.act_hook)
%         )
%
%     def __del__(self):
%         for handle in self.handles:
%             handle.remove()
%
%     def grad_hook(self, module, grad_input, grad_output):
%         self.grads = grad_output[0]
%
%     def act_hook(self, module, input, output):
%         self.acts = output
%
%     def forward(self, x):
%         batch_size = x.shape[0]
%
%         # self.act_hook is called here
%         preds = self.model(x)
%
%         self.model.zero_grad(set_to_none=True)
%
%         idxs = torch.argmax(preds, dim=1)
%
%         # backward pass, this gets gradients for each prediction
%         # self.grad_hook is called here
%         torch.sum(preds[torch.arange(batch_size), idxs]).backward()
%
%         # average last two dimensions so that we have Batch x Channel
%         average_gradients = self.grads.mean(-1).mean(-1)
%
%         # make shape Batch x Channel x 1 x 1 to allow for broadcasting
%         average_gradients = average_gradients.unsqueeze(-1).unsqueeze(-1)
%
%         saliency = self.acts * average_gradients
%         saliency = torch.sum(saliency, dim=1)
%         # remove negative saliencies in accordance with original paper
%         saliency = torch.nn.functional.relu(saliency)
%
%         return saliency
% \end{lstlisting}


\subsubsection{\ac{lime}}

\ac{lime} has an implementation for Python, written by the original authors \cite{lime}. However, this implementation is not suitable for my purposes. The main issue is that it is far too slow, being implemented with \texttt{NumPy} which restricts the computation to the CPU. Furthermore, \cite{lime} also returns full size heatmaps, although the actual number of saliency values used to create this heatmap is far smaller than the number of pixels in the image.

Thus I have implemented \ac{lime} myself, using PyTorch whenever possible.

\subsubsection{Occlusion}

Captum \cite{captum} is a library of \ac{xai} methods implemented in PyTorch, and this library contains a suitable implementation of occlusion. As explained previously, occlusion occludes parts of the image and compares the prediction scores before and after the occlusion. By occluding all parts of the image, we can get a saliency value for all positions. Occlusion is usually done using a sliding window, similar to a convolutional kernel, which is slid over all parts of the image. Such a window is rarely a single pixel, because it is often not interesting to see how a single pixel contributes to a prediction, but rather a larger region. What this means is that the final heatmap, although it is the same size as the input image, actually contains far fewer unique values. To avoid performing computations on thousands of repeated values, I reduce the size of the heatmap by sampling one pixel from each of the positions the sliding window has been applied, which can be efficiently done using a $1 \times 1$ MaxPooling kernel with a stride equal to the stride used during the occlusion.

\subsubsection{Guided Backpropagation and Integrated Gradients}

Captum also contains a suitable implementation for guided backpropagation and integrated gradients, which I have utilized in essentially unmodified forms. By definition (see section \ref{section:guidebackpropagation} and \ref{section:integratedgradients}), these methods return saliency maps over the entire input image dimensions. This separates them from the other methods, which return a far lower number of distinct saliencies, which are upscaled or transformed to the entire image (\ac{lime} and Occlusion output the same values for all pixels within a segment, \ac{gradcam} outputs an amount of saliencies corresponding to the final feature map, which is then upscaled). Because of this, the saliencies returned are exactly equivalent to the raw values I require, and no modifications are necessary.

However, this lack of dimensionality reduction also poses a technical challenge when we wish to store these saliency maps for later data analysis. For example, storing all the saliencies, without dimensionality reduction, for ImageNet200 and its associated \ac{ood} datasets would require 28 GB. Thus, instead of storing the saliencies themselves, I calculate the aggregate values during saliency generation and store them instead. This has the downside that if a new form of aggregation is to be tested, the whole dataset of saliencies has to be generated again. However, the decrease in storage requirements is immense, being between a 1000 and 10 000 times decrease depending on the number of aggregates stored.




% Listing \ref{code:occlusion} shows the code listing for this implementation.
%
% \begin{lstlisting}[style=pythonstyle, caption={Source code listing for the Captum occlusion implementation}, captionpos=b, label={code:occlusion}]
% def occlusion(data, model, block_size):
%     targets = torch.argmax(model(data), dim=-1)
%     lrp = captum.attr.Occlusion(model)
%
%     attributions = lrp.attribute(
%         data,
%         target=targets,
%         sliding_window_shapes=(3, block_size, block_size),
%         strides=(3, block_size, block_size),
%     )
%
%     attributions = attributions.sum(dim=1)
%     attributions = torch.nn.MaxPool2d(1, stride=block_size)(attributions)
%
%     return attributions
% \end{lstlisting}



\chapter{Experiments and Results} \label{chapter:experiments}

This chapter contains my findings. In the first section, I show how different forms of aggregation over saliency maps separate \ac{id} and \ac{ood} data points. This step is important, because it informs the choice of aggregation used in the Aggregation-of-Saliency \ac{ood} detection method. I report \ac{auroc} for different forms of aggregation over the three datasets. This is done on the validation sets, to avoid biasing the final results.

In the second section, \ac{ood} detection on the test sets is performed, and the \ac{auroc} and \ac{fpr95} for each method on each dataset is reported. The choice of methods are informed by the findings from the preceding section. In addition, I perform statistical analyses to investigate whether my \ac{xai} \ac{ood} detection methods outperform baseline methods.

\section{Data Analysis of Saliency Maps}

This section will detail how various \ac{xai} methods generate explanations which differ between \ac{id} and \ac{ood}. The section considers each dataset individually. For each dataset, I first present the level of separation achieved by the two baseline methods \ac{msp} and \ac{mls}, to give a general intuition about how easily the \ac{id} and \ac{ood} datasets are to separate. Following this, I go through the different \ac{xai} methods and their different statistical qualities.

% I present graphs of a selection of aggregations, giving a broad overview of what type of statistical qualities of explanations work best for separating \ac{id} and \ac{ood} data points. Finally, I present a table of all aggregations for all forms of saliency generation


\subsection{ImageNet200}

ImageNet200 is a suitable dataset to begin with, given ImageNet's ubiquity in \ac{ai} research. Figure \ref{fig:imagenet200logits} shows the distribution of the maximum softmax score and the maximum logit. Here, we see that there is a decent amount of separation between \ac{id} and \ac{ood} data points, even with the simple baseline methods introduced by \cite{oodbaseline} and \cite{mls}. Separating the distributions using \ac{msp}, we get an \ac{auroc} score of 0.834 for Near-\ac{ood} and 0.915 for Far-\ac{ood}. Using \ac{mls}, we get an \ac{auroc} score of 0.833 for Near-\ac{ood} and 0.903 for Far-\ac{ood}.

\begin{figure}
    \begin{center}
        \input{figure/imagenet200_logits_distribution.pgf}
    \end{center}
    \caption[Hypothetical ID/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical ID, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the ID and \ac{ood} samples.}
    \label{fig:imagenet200logits}
\end{figure}

\subsubsection{\ac{lime}}

With the baselines reported, we turn our attention to the first \ac{xai} saliency method, \ac{lime}. Figure \ref{fig:imagenet200limemeangini} presents graphs representing the two main forms of aggregation that has been applied; those which consider the magnitude of the saliencies and those which consider the statistical spread. These two forms are here represented by the mean and the Gini-coefficient.

\begin{figure}
    \begin{center}
        \input{figure/imagenet200_lime_triple_metrics.pgf}
    \end{center}
    \caption[Hypothetical ID/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical ID, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the ID and \ac{ood} samples.}
    \label{fig:imagenet200limemeangini}
\end{figure}

From the figure we can see that for \ac{lime} on ImageNet200, it is indeed the case that saliencies are higher for \ac{id} data points than for \ac{ood} data points. Using the vector norm to distinguish \ac{id} and \ac{ood}, we get an \ac{auroc} of 0.814 on Near-\ac{ood}, which is slightly lower than the baseline methods, and 0.925 on Far-\ac{ood}, which is actually higher than the baselines. These results are far better than those attained by \cite{martinez}, the only other related work which has attempted to use saliency maps to separate \ac{id} and \ac{ood} data. This work achieved an \ac{auroc} score of only 0.52 when used on datasets which were not just small toy datasets, which is for all practical purposes equivalent to guessing. This large improvement suggests that the usage of raw saliency values, rather than normalized heatmaps (as was used by \cite{martinez}) is highly consequential for the performance of \ac{xai} \ac{ood} detection algorithms.

Measuring the statistical spread using \ac{rmd}, we find that there is indeed also a higher spread in \ac{id} data when compared to \ac{ood} data. However, in this case the overlap is substantial, which is reflected in the \ac{auroc} scores attained when discriminating using \ac{rmd}: The Near-\ac{ood} \ac{auroc} score was 0.594, and the Far-\ac{ood} score was 0.695. In both cases, the scores attained are far lower than the both of the baselines.



Table \ref{table:imagenet200_lime_metrics} shows the \ac{auroc} scores for all forms of aggregation on \ac{lime}, as well as the previously mentioned \ac{auroc} of the baseline methods. In addition, the correlation between the aggregates and the maximum logit and maximum softmax score is reported, which gives insight into how \ac{xai} saliency maps are related to the prediction confidence of the model.

\begin{table}[H]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
    \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
     Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min$\downarrow$ & \ac{cv}$\downarrow$ & \ac{rmd} & \ac{qcd}$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 &\textbf{ 83.4 }& 78.8 & 69.3 & 81.4 & 77.1 & 78.1 & 54.3 & 53.7 & 59.4 & 53.2  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 88.1 & 75.2 &\textbf{ 92.5 }& 90.2 & 91.4 & 58.8 & 49.3 & 69.5 & 49.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.75 & 0.60 & 0.71 & 0.45 & 0.54 & 0.08 & -0.01 & 0.08 & -0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.61 & 0.48 & 0.61 & 0.41 & 0.48 & 0.02 & -0.01 & 0.09 & -0.01  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for lime on imagenet200. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_lime_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

From this table, we can note several interesting observations. Firstly, the aggregates which capture information about the magnitude of the saliencies are highly correlated with both the maximum logit and the maximum softmax score of the predicted class. This is not unexpected, as \ac{lime} generates saliencies using differences in prediction values as different parts of the image is masked. If the predicted value is higher on average for \ac{id} data, then it is likely that the drop in predicted value when masking parts of the image is larger as well, leading to higher saliencies. Regardless, it seems clear that it is not only the correlation to the model output which explains the discriminative power of these aggregates, as we see that vector norm aggregation has lower correlation with \ac{mls} and equal correlation with \ac{msp} when compared to the mean, but has higher scores.

Secondly, we see that the magnitude based aggregations in general perform quite well, with vector norm in this case being the best. In contrast, the methods based on statistical dispersion perform poorly, putting into question the hypothesis that the saliency maps of \ac{id} data is more concentrated and less spread out than those on \ac{ood} data. Indeed, we see that while the \ac{rmd} is higher on average for \ac{id} data, the \ac{qcd} and \ac{cv} is lower on average when generating saliencies using \ac{lime}. This further puts doubt on the idea that the spread of saliency maps is a reliable indicator of \ac{ood}-ness.

In general, the results from these aggregations are promising, and show that there is definite potential for \ac{ood} detection algorithms based on \ac{xai} outputs. However, the reader should note that these results are done on the validation set, and that no statistical tests have been done at this point. The statistical significance of using \ac{xai} saliency maps for \ac{ood} detection will be revealed in section \ref{section:results}, when the final testing is done on the test set and bootstrapping is performed.

\subsubsection{Occlusion}

Now, we turn our attention to occlusion saliency mapping. Looking at figure \ref{fig:imagenet200_occlusion_mean_rmd}, we see that there is far more overlap between the \ac{id} and \ac{ood} densities than with \ac{lime}.

\begin{figure}
    \begin{center}
        \input{figure/imagenet200_occlusion_triple_metrics.pgf}
    \end{center}
    \caption[Hypothetical ID/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical ID, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the ID and \ac{ood} samples.}
    \label{fig:imagenet200_occlusion_mean_rmd}
\end{figure}

From table \ref{table:cifar10_occlusion_metrics} we see that this trend is apparent over all of the different forms of aggregation, not just the vector norm and \ac{rmd}. Interestingly, we see that in this case, all the statistical dispersion aggregates are higher for \ac{id} data, as was the original hypothesis. Regardless, the \ac{auroc} scores are very poor, and thus these metrics do not seem suitable to discriminate between \ac{id} and \ac{ood}.

\begin{table}[H]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min$\downarrow$ & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 &\textbf{ 83.4 }& 58.6 & 54.2 & 65.7 & 69.1 & 66.8 & 54.3 & 55.0 & 57.7 & 55.4  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.5 }& 90.3 & 69.7 & 62.9 & 77.8 & 84.6 & 83.5 & 54.0 & 63.8 & 64.7 & 62.7  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.37 & 0.31 & 0.42 & 0.44 & 0.47 & 0.07 & 0.01 & 0.14 & 0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.29 & 0.24 & 0.37 & 0.41 & 0.41 & 0.03 & 0.01 & 0.13 & 0.01  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for occlusion on imagenet200. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_occlusion_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac{gradcam}}

After having inspected two completely model independent \ac{xai} methods, we turn our attention to the first of the three gradient based methods, \ac{gradcam}. As we have seen density plots for both \ac{lime} and occlusion, I will omit these here, as the table contains the necessary information. From table \ref{table:cifar10_gradcam_metrics}, we see that vector norm aggregation of \ac{gradcam} saliencies actually beats the baselines on both Near- and Far-\ac{ood} detection. The increase on Far-\ac{ood} is particularly impressive, with an increase of 1.4 percentage points. However, we should keep in mind that these results are done on the validation set and that the final results and their statistical significance will only be explored in section \ref{section:results}.

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Statistical dispersion} \\
    \hline
     Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min & \ac{cv} & \ac{rmd} & \ac{qcd}  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 & 83.3 & 80.3 &\textbf{ 83.8 }& 80.5 & 81.9 & 59.2 & 50.8 & 51.8 & 51.7  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 91.5 & 87.6 &\textbf{ 92.9 }& 92.2 & 92.8 & 56.1 & 63.6 & 64.9 & 64.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 1.00 & 0.94 & 0.97 & 0.69 & 0.81 & 0.49 & -0.16 & -0.12 & -0.12  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.83 & 0.78 & 0.82 & 0.62 & 0.70 & 0.37 & -0.11 & -0.07 & -0.07  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for gradcam on imagenet200. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_gradcam_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

A second interesting observation is that the mean value is completely correlated with the maximum logit. The vector norm is also almost completely correlated, but slightly less, and has a higher score than the mean.

\subsubsection{Integrated Gradients}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min$\downarrow$ & CV$\downarrow$ & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 &\textbf{ 83.4 }& 82.1 & 55.3 & 67.3 & 63.9 & 63.5 & 63.6 & 66.1 & 51.3 & 50.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.5 }& 90.3 & 90.5 & 56.0 & 87.8 & 86.7 & 85.9 & 86.2 & 49.8 & 39.1 & 53.4  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.94 & 0.14 & 0.40 & 0.31 & 0.30 & -0.30 & -0.16 & 0.01 & 0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.79 & 0.10 & 0.36 & 0.29 & 0.28 & -0.27 & -0.15 & 0.00 & 0.00  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for integratedgradients on imagenet200. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_integratedgradients_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac*{gbp}}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean$\downarrow$ & Median$\downarrow$ & Norm & Range & Max & Min$\downarrow$ & CV & RMD$\downarrow$ & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 &\textbf{ 83.4 }& 73.7 & 55.7 & 77.4 & 71.3 & 71.8 & 69.9 & 50.2 & 52.8 & 51.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 81.7 & 51.1 &\textbf{ 92.3 }& 90.7 & 90.6 & 89.7 & 43.5 & 72.3 & 50.7  \\
    \hline
    Correlation with \ac{mls}& - & - & -0.33 & -0.08 & 0.45 & 0.25 & 0.25 & -0.24 & 0.00 & -0.01 & -0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & -0.30 & -0.08 & 0.42 & 0.25 & 0.24 & -0.24 & 0.01 & -0.00 & -0.00  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for gbp on imagenet200. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_gbp_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}


\subsection{CIFAR10}


\subsubsection{\ac{lime}}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min & CV$\downarrow$ & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 &\textbf{ 87.7 }& 81.2 & 73.0 & 77.7 & 67.1 & 76.1 & 62.8 & 63.6 & 61.0 & 59.0  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.4 }& 90.8 & 86.1 & 77.9 & 84.0 & 74.5 & 81.9 & 59.6 & 59.0 & 61.2 & 53.2  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.40 & 0.28 & 0.34 & 0.22 & 0.31 & 0.08 & -0.00 & 0.11 & -0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.29 & 0.20 & 0.23 & 0.15 & 0.22 & 0.06 & 0.01 & 0.08 & -0.00  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for lime on cifar10. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_lime_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Occlusion}


\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean$\downarrow$ & Median$\downarrow$ & Norm$\downarrow$ & Range$\downarrow$ & Max$\downarrow$ & Min & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 &\textbf{ 87.7 }& 79.2 & 69.3 & 84.5 & 79.5 & 75.6 & 83.7 & 68.3 & 66.3 & 51.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.4 }& 90.8 & 79.2 & 72.4 & 84.9 & 78.8 & 74.6 & 86.6 & 70.5 & 71.2 & 52.5  \\
    \hline
    Correlation with \ac{mls}& - & - & -0.17 & -0.28 & -0.39 & -0.39 & 0.00 & 0.68 & 0.01 & 0.71 & 0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.07 & -0.16 & -0.18 & -0.24 & 0.26 & 0.87 & 0.02 & 0.68 & 0.01  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for occlusion on cifar10. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_occlusion_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac{gradcam}}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Min & CV$\downarrow$ & RMD$\downarrow$ & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 &\textbf{ 87.7 }& 86.8 & 85.9 & 86.7 & 71.8 & 86.5 & 87.5 & 75.3 & 76.7 & 74.2  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 &\textbf{ 91.4 }& 91.2 & 91.4 & 80.7 & 91.2 & 90.7 & 78.8 & 80.1 & 74.5  \\
    \hline
    Correlation with \ac{mls}& - & - & 1.00 & 0.99 & 1.00 & 0.68 & 0.96 & 0.90 & -0.47 & -0.48 & -0.45  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.79 & 0.78 & 0.79 & 0.57 & 0.77 & 0.70 & -0.42 & -0.42 & -0.41  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for gradcam on cifar10. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_gradcam_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac*{gbp}}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm$\downarrow$ & Range$\downarrow$ & Max$\downarrow$ & Min & CV & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 &\textbf{ 87.7 }& 52.4 & 51.9 & 86.0 & 84.8 & 84.8 & 84.4 & 51.4 & 54.1 & 50.3  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.4 }& 90.8 & 46.2 & 44.7 & 83.6 & 82.5 & 82.4 & 82.1 & 51.9 & 53.1 & 50.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.01 & 0.03 & -0.24 & -0.21 & -0.20 & 0.20 & 0.00 & 0.02 & 0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.02 & 0.03 & -0.16 & -0.15 & -0.14 & 0.14 & 0.00 & 0.03 & 0.00  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for gbp on cifar10. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_gbp_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Integrated Gradients}

\begin{table}[H]
\setlength\tabcolsep{4pt}
\begin{center}
\begin{tabular}{ |m{5em}|c c|c c c c c c|c c c| }
    \hline
     Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{c|}{Spread of saliencies} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm$\downarrow$ & Range$\downarrow$ & Max$\downarrow$ & Min & CV & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 &\textbf{ 87.7 }& 52.4 & 51.9 & 86.0 & 84.8 & 84.8 & 84.4 & 51.4 & 54.1 & 50.3  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} &\textbf{ 91.4 }& 90.8 & 46.2 & 44.7 & 83.6 & 82.5 & 82.4 & 82.1 & 51.9 & 53.1 & 50.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.01 & 0.03 & -0.24 & -0.21 & -0.20 & 0.20 & 0.00 & 0.02 & 0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.02 & 0.03 & -0.16 & -0.15 & -0.14 & 0.14 & 0.00 & 0.03 & 0.00  \\
    \hline
    \end{tabular}
    \caption{\ac{auroc} scores for gbp on cifar10. The highest value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_gbp_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\chapter{Discussion} \label{chapter:discussion}

\chapter{Conclusion} \label{chapter:conclusion}

\section{Future work}
























% \chapter{Temporary chapter} \label{chapter:newmethod}
%
%
% \section{Overall plan}
%
% I will integrate \ac{xai} methods into a specific method from each of the four methodologies discussed in chapter \ref{chapter:background}; classification-, density-, distance-, and reconstruction-based methods. For each method, I shall compare the AUROC and FPR95-scores before and after integrating \ac{xai} values.
%
% Furthermore, I will perform data analysis on 
%
% \section{Motivation} \label{chapter:tempmotivation}
%
% Many \ac{ood}-detection methods use the final features of the network, i.e the values right before we generate logits for each class. In convolutional neural networks values typically contain a lot of semantic information, but little spatial information. For example, the ResNet family of convolutional networks use global average pooling on the feature map generated by the last convolutional layer to generate the penultimate features. When doing this, all positional information is lost, as we average over the pixel values in each channel, so that a 512 channel 7 by 7 feature map becomes a vector of 512 values. Thus, we have only the average activation of each channel, which gives us information about what is in the image, but not where.
%
% Explainability methods, especially local saliency methods, give information about where a model is focused when making a prediction. Such information could be valuable to deciding whether a data point is \ac{ood}. For example, by comparing explanations for \ac{ood} and ID data, we may find that the model is focused on several different areas of the image in an \ac{ood} data point, while it is focused on a single area in ID samples. The reasoning for this is that if a data point is ID, it must contain an object of the class the model has been trained on. It is then likely that this object will generate a higher response from the network than other parts of the image, which will be picked up by \ac{xai} algorithms. For an \ac{ood} data point, there is, by definition, no object of any of the classes that the model has been trained on. In this case, the predicted class will be essentially random, and it is less likely that there is a single object which elicits a very high response from the network. Thus, we expect to see an explanation which is more spread out.
%
% Furthermore, the explanations also contain magnitude information about the saliencies of different regions of the image, which one might expect to be higher for ID data, where there should be clear regions which contribute greatly to the final prediction. In \ac{ood} data points, we would expect there to be no region which is particularly important to the predicted class the model happened to output, given that (in the case of semantic shift) this class is not even present in the image.
%
% \section{Proof of Concept, preliminary investigations: ImageWoof}
%
%
% \subsection{Results}
%
%
%
% \section{Combine \ac{xai} and \ac{ood}}
%
% Given the poor results of \cite{martinez} in their attempt to perform \ac{ood} detection using only explanations, it is likely that explanations on their own will not be sufficient to discriminate between ID and \ac{ood} samples. Instead, I believe that by integrating explanations into \ac{ood} methods which use other features of the network, we could increase the gap between \ac{ood} and ID and increase the discriminatory power of these methods.
%
% \subsection{Virtual Logit Matching (ViM)}
%
% As a first choice of \ac{ood} method, I choose ViM. The reasoning is that ViM already is a method which attempts to use multiple sources of information from a network at once to determine an \ac{ood}-score. The baseline ViM uses the logits, features and softmax probabilities to determine \ac{ood}-ness. I propose to adapt this method to also include saliency values, which can come from different \ac{xai} methods.
%
% The methods I choose are LIME, Occlusion and \ac{gradcam}. The values are appended to the logit values for each sample. In this way, we gain some positional information along with the strictly semantic information of the logits of ResNet model.
%
% \section{Implemented method}
%
% Although there exists many libraries for \ac{xai} methods such as LIME, Occlusion and \ac{gradcam}, I have implemented them all from scratch. Given that I am using \ac{xai}-methods for an entirely different purpose than the libraries are designed for (\ac{ood} detection versus explainability), this is not so surprising.
%
% Lime and Occlusion have been implemented from scratch to work best in the context of \ac{ood} detection. Specifically, they have been implemented in such a way that the calculations can be batched, to take advantage of large 
%
% \ac{gradcam} has been implemented from scratch to allow for certain
%
% \section{Analysis of \ac{xai} methods}
%
%
% In this chapter, I will conduct a thorough analysis of the explanations generated by different \ac{xai} methods. The focus will be on the possible differences between explanations made on ID and \ac{ood} data points. With this in mind, I will compare and contrast the different methods, attempting to identify possible candidate methods for use in \ac{ood} detection.
%
% \section{Choice of \ac{xai} methods}
%
% While there exists a large multitude of \ac{xai} methods, the problem statement (\ac{ood} detection on image data) imposes a number of restrictions which restrict the choice of methods.
%
% Firstly, given the wish to use \ac{xai} methods for \ac{ood} detection, it is natural to favour local methods, i.e those that give explanations to single samples. Global methods, which describe the relation between the input features over a whole dataset, give a greater understanding of how the model functions in general but are of little use when attempting to decide whether a single data point is \ac{ood}, which is the goal during deployment of \ac{ood} methods.
%
% Secondly, since I am working with image classification problems, it is most reasonable to use post-hoc methods, as opposed to intrinsically explainable models.  Image classification is complex problem which can not be satisfactorily solved by simple methods such as logistic regression or random forests, and as such intrinsically explainable models will have unacceptably poor classification performance. Instead, the field of image classification is dominated by complex models such as Vision Transformers and Convolutional Neural Networks, which are not intrinsically explainable. Indeed, as shown by \cite{xaioverview}, the vast majority of \ac{xai} methods applied to images in the medical field are post-hoc.
%
% The majority of \ac{xai} methods applied to image models are so-called {\it saliency methods}, also known as {\it pixel attribution methods} \cite{molnar}. These methods produce a heatmap denoting which regions of the image the model was "focused on" when making the prediction, attributing a numerical value of importance to each pixel or region in the image. For example, when producing an explanation for why a classification model chose to label an image as "cat", we would expect the explanation to highlight the regions where the cat is, and not parts of the background.For my purposes, these methods are well-suited, as they output numerical values which are easy to compare between examples. In contrast, image captioning methods generate textual explanations, which are much harder to compare.
%
% In conclusion, given the requirements set by the type of data (images) and the task (\ac{ood} detection), the \ac{xai} methods I choose will:
%
% \begin{itemize}
%     \item be local rather than global
%     \item be post-hoc rather than intrinsically explainable
%     \item output numerical values (such as saliency values) rather than text
% \end{itemize}
%
% Based on these criteria, I choose N number of methods, all of which have been introduced and explained in chapter \ref{chapter:xai}.
%
% \begin{itemize}
%     \item Gradient Class Activation Mapping (GradCAM)
%     \item Occlusion
%     \item Local Interpretable Model-agnostic Explanations (LIME)
%     \item Layer Relevance Propagation (LRP)
% \end{itemize}
%
% \section{Analysis of methods on a proof of concept dataset}
%
% To investigate the potential for \ac{xai} methods to aid in separating ID from \ac{ood} data, I introduce a simple collection of datasets, which form an \ac{ood} detection problem. This collection consists of an ID, Near-\ac{ood} and Far-\ac{ood} dataset. The ID dataset is ImageWoof \cite{imagewoof}, a subset of ImageNet which contains ten different breeds of dogs which are to be classified. For Near-\ac{ood} data points, the obvious choice would be pictures of dogs which are of different breeds than the 10 breeds used in ImageWoof. Thus, I use the Stanford Dog Dataset \cite{stanforddogs}, and simply remove any classes which overlap with the 10 from ImageWoof. Given that dogs from two different breeds can have many similarities in terms of size, colour, texture and body and face structure, the Stanford Dog Dataset represents a very small semantic shift and consequently, a difficult \ac{ood} detection task.
%
% For a far-\ac{ood} dataset, I use Places365 \cite{places365}, a dataset used for scene recognition. This dataset has categories such as {\it Classroom}, {\it Forest road} or {\it Conference Room}, and thus does not contain anything resembling dogs or even any animal at all. 
%
% Below (figure \ref{fig:imagewoof_examples}) is a set of examples for each of these datasets. As we can see, ImageWoof and Stanford Dogs are quite similar, however, no breed is present in both datasets.
%
% \begin{figure}[h]
% \centerline{\includegraphics[width=5.25in]{figure/imagewoof_near_far_examples.pdf}}
% \caption{Figure}
% \label{fig:imagewoof_examples}
% \end{figure}
%
% With this collection, I train a ResNet-50 model on the ID dataset and subsequently run inference on all three datasets, generating explanations for all data points.
%
% \section{Results}
%
% When inspecting the generated saliency heatmaps, we can either normalize the values (making the maximum value equal to one and the minimum equal to 0) before displaying them, or keep the saliency values unnormalized. Because the intended purpose of saliency methods is simply to show which regions are more important than others when making a prediction, it is common to normalize the values before visualization \cite{molnar, gradcam}. However, as I will show, there is much understanding to be gained by looking at the unnormalized saliencies directly, especially for \ac{ood} detection. In fact, I posit that one of the reasons for the poor performance attained by \cite{martinez} is the presence of normalization in their implementation.
%
% For each method, I will 
%
% \subsection{\ac{gradcam}}
%
%
%
%
% \chapter{\ac{xai} for \ac{ood} Detection: Integration of Select \ac{xai} Methods}
%
% % \section{Hypothetical \ac{ood} Detection Problem: Dog Breed Classification Neural Network}
% %
% % Complicated concepts are often more easily understood when the reader is given examples. Thus, to aid the reader in their understanding of my methodology, I will introduce a simple hypothetical \ac{ood} detection task, which will be used as the context whenever I illustrate a method with an example. This \ac{ood} detection task consists of a Neural Network which is trained on a dataset of ten different dog breeds, henceforth known as {\it Dogs10}. The task of the network is to correctly classify an image of a dog into one of the ten classes. In addition to the ID dataset Dogs10, I imagine a Near-\ac{ood} dataset called {\it DifferentDogs} which contains dogs of different breeds than the 10 the model is trained on. Finally I imagine a Far-\ac{ood} dataset called {\it NoDogs}, which contain images where no dog is present.
% %
% % \section{Data Analysis of Explanations}
% %
% % As part of my effort to integrate \ac{xai} methods into \ac{ood} detection, I plan to perform detailed data analysis on the properties of explanations generated by different \ac{xai} methods. The core of this analysis will be comparisons between explanations generated on ID and \ac{ood} data.
% %
% % \subsection{Data Generation}
% %
% % As explained previously, I limit myself to the study of saliency mapping \ac{xai} methods. This means that each explanation is a two-dimensional grid of real values, which denote the importance of each region, as determined by the \ac{xai} method used. By iterating over the entire ID dataset, as well as all associated \ac{ood} datasets, we thus have explanations corresponding to all data points, both ID and \ac{ood}.
% %
% % Specifically, given a trained model $f$, an \ac{xai} saliency mapping method $s$ and a data set $D$ containing images $\{ \mathbf{x} \mid \mathbf{x} \in \R^{c, h, w} \}$, I will generate a dataset of explanations $D_{sal}$ in the following way:
% %
% % {\large
% % \begin{equation}
% %     D_{sal} = \{ \mathbf{x}_{sal} \mid \mathbf{x}_{sal} = s(f, \mathbf{x}), \, \mathbf{x} \in D \}
% % \end{equation}
% % }
% %
% % In general, $\mathbf{x}_{sal} \in \R^{n, m}$, where $n \leq h$ and $m \leq w$. For methods such as \ac{gradcam}, the output saliency map $\mathbf{x}_{sal}$ may be as small as $7 \times 7$ despite the input image being $224 \times 224$, while other methods such as LRP will maintain the entire input dimension.
%
%
%
% \section{Saliency methods}
%
%
%
% \section{Methods}






\backmatter{}
\printbibliography{}
\end{document}
