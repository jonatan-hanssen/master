\documentclass[UKenglish]{uiomasterthesis} %% ... or norsk or nynorsk or USenglish
\usepackage[utf8]{inputenc} %% ... or latin1
\usepackage[T1]{url}\urlstyle{sf}
\usepackage{babel, csquotes, graphicx, textcomp, uiomasterfp, varioref}
\usepackage{amsthm}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{amsmath, amssymb, amsfonts, bm}
\usepackage{acronym}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{listofitems}
\usepackage{pgfplots}
\usepackage{algpseudocode}
\usepackage{tikzlings}
\usepackage[backend=biber,style=numeric-comp, sorting=none]{biblatex}
\usepackage[hidelinks, hypertexnames=false]{hyperref}

\newtheorem{theorem}{Theorem}

\usetikzlibrary{arrows.meta} % for arrow size
\usepackage[outline]{contour} % glow around text
\contourlength{1.4pt}

\usetikzlibrary{positioning, matrix, backgrounds}
\usetikzlibrary{3d,decorations.text,shapes.arrows,fit}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    language=Python,
    backgroundcolor=\color{white},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    escapeinside=||,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    frame=single,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    morekeywords={self},
}

\lstset{
	breaklines=true,
	style=pythonstyle,
}


\let\origthelstnumber\thelstnumber
\makeatletter
\newcommand*\Suppressnumber{%
  \lst@AddToHook{OnNewLine}{%
    \let\thelstnumber\relax%
     \advance\c@lstnumber-\@ne\relax%
    }%
}

\newcommand*\Reactivatenumber[1]{%
  \setcounter{lstnumber}{\numexpr#1-1\relax}
  \lst@AddToHook{OnNewLine}{%
   \let\thelstnumber\origthelstnumber%
   \refstepcounter{lstnumber}
  }%
}


\makeatother


\pgfplotsset{colormap={turbo}{
   rgb=(0.19,0.07,0.23)
   rgb=(0.24,0.22,0.58)
   rgb=(0.27,0.37,0.82)
   rgb=(0.28,0.50,0.97)
   rgb=(0.23,0.64,0.99)
   rgb=(0.14,0.76,0.89)
   rgb=(0.09,0.87,0.76)
   rgb=(0.17,0.94,0.62)
   rgb=(0.36,0.99,0.45)
   rgb=(0.56,1.00,0.29)
   rgb=(0.71,0.97,0.21)
   rgb=(0.84,0.90,0.21)
   rgb=(0.94,0.80,0.23)
   rgb=(0.99,0.69,0.21)
   rgb=(0.99,0.54,0.15)
   rgb=(0.95,0.38,0.08)
   rgb=(0.88,0.26,0.04)
   rgb=(0.78,0.16,0.01)
   rgb=(0.65,0.08,0.00)
   rgb=(0.48,0.02,0.01)
}}

\tikzset{pics/fake box/.style args={% #1=color, #2=x dimension, #3=y dimension, #4=z dimension
#1 with dimensions #2 and #3 and #4 and #5}{
code={
\draw[thin]  (0,0,0) coordinate(-front-bottom-left) to
++ (0,#3,0) coordinate(-front-top-right) --++
(#2,0,0) coordinate(-front-top-right) --++ (0,-#3,0) 
coordinate(-front-bottom-right) -- cycle;
\draw[thin] (0,#3,0)  --++ 
 (0,0,#4) coordinate(-back-top-left) --++ (#2,0,0) 
 coordinate(-back-top-right) --++ (0,0,-#4)  -- cycle;
\draw[thin] (#2,0,0) --++ (0,0,#4) coordinate(-back-bottom-right)
--++ (0,#3,0) --++ (0,0,-#4) -- cycle;
\path[decorate,decoration={text effects along path, text={#5}}] (#2/2,{2+(#3-2)/2},0) -- (#2/2,0,0);
}
}}
% from https://tex.stackexchange.com/a/52856/121799
\tikzset{circle dotted/.style={dash pattern=on .05mm off 2mm,
                                         line cap=round}}

\pgfdeclarelayer{foreground}
\pgfsetlayers{background, main, foreground}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\R}{\mathbb{R}}

\providecommand{\mathdefault}[1]{#1}


\definecolor{id}{rgb}{0.4, 0.7607843137254902, 0.6470588235294118}
\definecolor{near}{rgb}{0.5529411764705883, 0.6274509803921569, 0.796078431372549}
\definecolor{far}{rgb}{0.9882352941176471, 0.5529411764705883, 0.3843137254901961}

\definecolor{ca0a0a0}{RGB}{160,160,160}
\definecolor{ce0e0e0}{RGB}{224,224,224}
\colorlet{myred}{red!80!black}
\colorlet{myblue}{blue!80!black}
\colorlet{mygreen}{green!60!black}
\colorlet{myorange}{orange!70!red!60!black}
\colorlet{mydarkred}{red!30!black}
\colorlet{mydarkblue}{blue!40!black}
\colorlet{mydarkgreen}{green!30!black}

% STYLES
\tikzset{
  >=latex, % for default LaTeX arrow head
  node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
  node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
  node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
  node convol/.style={node,orange!20!black,draw=myorange!30!black,fill=myorange!20},
  node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
  connect/.style={thick,mydarkblue}, %,line cap=round
  connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
  node 1/.style={node in}, % node styles, numbered for easy mapping with \nstyle
  node 2/.style={node hidden},
  node 3/.style={node out}
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3

\title{Explainable Artificial Intelligence for Out-of-Distribution Detection}
\subtitle{Using irregularities in machine learning explanations to detect when a model is faced with unusual data}
\author{Jonatan Hoffmann Hanssen}

\addbibresource{bibliography.bib}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\begin{document}


\uiomasterfp[dept={Department of Informatics},
program={Robotics and Intelligent Systems},
supervisors={Hugo Lewi Hammer \and Kyrre Harald Glette \and Michael Riegler}, long]

\frontmatter{}
\begin{abstract}
    As \ac{ai} becomes a larger and larger part of society, the need for robust and understandable models becomes paramount. When neural networks are used in high-impact settings such as cancer detection or autonomous driving, we must not only require that they make predictions with high accuracy, but also that they are aware of their shortcomings, and alert us when faced with unusual data. The need for models which "know what they do not know" leads to the field of \ac{ood} detection, which attempts to detect when models are exposed to data points that are far outside of their training data and thus unlikely to be classified correctly. Effective \ac{ood} detection makes \ac{ai} models safer and more robust, and can enable them to be used for tasks where the consequence of failure is severe. \ac{ood} detection is a young and developing field, and there are, to date, no methods which achieve superior performance on all benchmarks. Thus, there is a need for novel techniques which push the field forward.

    \ac{xai} is another field which is concerned with making \ac{ai} models more trustworthy and robust. Like \ac{ood} detection, this field has seen increased interest as \ac{ai} methods are used for a wider array of tasks than previously. \ac{xai} methods attempt to understand how \ac{ai} models come to a particular decision. To do this, they use gradient information, counterfactuals, and model internals to create explanations of predictions which can be inspected by humans. These methods, although they are intended primarily for human inspection, may also capture information from \ac{ai} models which could be used to detect \ac{ood} data points. To date, only one work has attempted to combine \ac{xai} and \ac{ood} detection in this way, with limited results. This thesis further explores the possibility of using \ac{xai} explanations for \ac{ood} detection.

    In this thesis, three \ac{xai} \ac{ood} detection frameworks have been developed, and methods under these three frameworks have been rigorously tested on all four standard \ac{ood} detection benchmarks. The results are very promising, and show that \ac{xai} methods can indeed be used for \ac{ood} detection, contrary to previous research. \ac{xai} \ac{ood} detection methods can compete with baseline \ac{ood} detection methods, and even surpass them in many cases. In addition, on all four benchmarks, a method developed under one of the frameworks created in this thesis achieves results which are within 4.1 percentage points of post-hoc \ac{sota} \ac{ood} detection methods on either Near- or Far-\ac{ood}.
\end{abstract}

% \begin{xabstract}[Sammendrag]
% Here comes the abstract in a different language.
% \end{xabstract}


\tableofcontents{}

\newpage

\thispagestyle{plain}
{\noindent \Huge \fontfamily{phv}\selectfont \textbf{List of Acronyms}}
\\
\\
\\

\begin{acronym}[ICANN]
    \acro  {ai}   [AI]   {Artificial Intelligence}
    \acro  {dl}   [DL]   {Deep Learning}
    \acro  {acm} [ACM] {Association for Computing Machinery}
    \acro  {ml}   [ML]   {Machine Learning}
    \acro  {cnn}   [CNN]   {Convolutional Neural Network}
    \acro  {ffnn}[FFNN]{Feed Forward Neural Network}
    \acro  {relu}   [ReLU]   {Rectified Linear Unit}
    \acro  {sota}   [SoTA]   {State-of-the-Art}
    \acro  {aupr}   [AUPR]   {Area Under Precision Recall Curve}
    \acro  {auroc}   [AUROC]   {Area Under Receiver Operating Characteristic}
    \acro  {roc}   [ROC]   {Receiver Operating Characteristic}
    \acro  {fpr}   [FPR]   {False Positive Rate}
    \acro  {tpr}   [TPR]   {True Positive Rate}
    \acro  {fpr95}   [FPR95]   {False Positive Rate at 95\% Recall}
    \acro  {xai}   [XAI]   {Explainable Artificial Intelligence}
    \acro  {gbp}   [GBP]   {Guided Backpropagation}
    \acro  {lrp}   [LRP]   {Layer Relevance Propagation}
    \acro  {cam} [CAM] {Class Activation Mapping}
    \acro  {gradcam} [GradCAM] {Gradient Class Activation Mapping}
    \acro  {gap}   [GAP]   {Global Average Pooling}
    \acro  {lime}   [LIME]   {Local Interpretable Model-Agnostic Explanations}
    \acro  {slic} [SLIC] {Simple Linear Iterative Clustering}
    \acro  {id} [ID] {In-Distribution}
    \acro  {ood}   [OOD]   {Out-of-Distribution}
    \acro  {msp}   [MSP]   {Maximum Softmax Probability}
    \acro  {mls}   [MLS]   {Maximum Logit Score}
    \acro  {msp}   [MSP]   {Maximum Softmax Probability}
    \acro  {vim}   [VIM]   {Virtual Logit Matching}
    \acro  {pca}   [PCA]   {Principal Component Analysis}
    \acro  {rmd}   [RMD]   {Relative Mean Absolute Difference}
    \acro  {qcd}   [QCD]   {Quartile Coefficient of Determination}
    \acro  {cv}[CV]{Coefficient of Variation}
\end{acronym}

\thispagestyle{plain}
\listoffigures{}
\listoftables{}


\begin{preface}
Generative \ac{ai} has \textbf{not} been used to generate or enhance any written text contained in this thesis. However, generative \ac{ai} has been used for some programmatic tasks. Services such as GPT UiO have been used for code debugging, for generating TikZ code used for creating diagrams, and for generating some boilerplate Python code. All data and personal information have been processed in accordance with the University of Oslo's regulations, and I, as the author of the document, take full responsibility for the validity of any generated code used as part of this work.

\vspace{5mm}

\noindent I would like to thank my supervisors Hugo Hammer, Kyrre Glette and Michael Riegler for their excellent guidance and support throughout the writing of this thesis.
\end{preface}

\mainmatter{}



\chapter{Introduction}

\section{Motivation}

\ac{ml} generally, and \ac{dl} specifically, have seen a tremendous increase in performance in recent years, performing comparable to humans in tasks such as image classification, speech and handwriting recognition, as well as many others \cite{performance}. Consequently, \ac{dl} methods have been deployed in a multitude of fields, and have become a part of our daily lives through their role in web search, text translation, computer vision, and in many other technologies which are taken for granted. In the medical field, deep learning has the potential to provide faster and more accurate detection of diseases by being trained on cases from thousands of previous patients \cite{xaisurvey}. Despite this, the adoption \ac{dl} in high impact fields, such as medicine, has been slow, with \cite{dlmed} stating that: "surprisingly little in health care is driven by machine learning".

% TODO: make these citations more specific

To explain this discrepancy, we should consider that despite their impressive performance, the application of deep learning methods is not without drawbacks. Firstly, deep neural networks are inherently unexplainable due to the large number of parameters that any non-trivial network has. \ac{sota} models will perform millions of operations to evaluate a single data point, and it is therefore impossible for humans to comprehend and explain the entire process which led the model to make a particular decision. In medicine, this is a major limitation of deep learning methods, as both doctors and patients expect to be able to understand why a decision was made \cite{tingsim}. In other high-impact fields, such as autonomous driving, this lack of transparency also has serious practical and legal ramifications. % TODO cite this

Secondly, although neural networks may attain high accuracy on test data and appear to have learned great insights about the tasks they are employed in, they often lack robustness and can suffer large drops in performance on data points which are slightly different from the training data. As \cite{intriguing} has shown, it is possible to create data points which are imperceptibly different from normal data points, yet still fool otherwise high performing models. More problematically, unlike humans, who recognize when they are faced with a novel situation where their expertise might be lacking, \ac{dl} methods will predict equally confidently on data points which are far outside the data they have been trained on \cite{tingsim}.

These two problems lead to the fields of \ac{xai}, and \ac{ood} detection. \ac{xai} attempts to explain the reasons why a model came to a decision, which helps to remedy the black-box nature of complicated \ac{dl} models. In a healthcare setting, such explanations can be inspected by medical practitioners to confirm the diagnosis, and can be used to give patients information about why decisions regarding their health were made. In autonomous driving or other automated high impact fields, \ac{xai} can be used to detect failure modes or to understand and improve trained models. \ac{ood} detection attempts to uncover when a data point is too different from the training data to be classified reliably. These methods could alert medical practitioners when such data points occur, thus avoiding potentially fatal misclassifications. In autonomous driving, the system could detect novel situations and cede control back to the user, avoiding accidents.

Both of these fields have seen increased interest in recent years, and are vital parts of any integration of \ac{dl} in high impact settings. As two vibrant fields of study, there is great potential to combining insights from one field to improve performance in the other, an area which is underexplored. This thesis will focus on \ac{ood} detection, but will attempt to use \ac{xai} methods to improve detection performance. The overarching intuition is that by inspecting the explanation of a model on a specific data point, we may be able to uncover flaws or irregularities in the explanation which could help us determine whether the data point is \ac{ood}. This methodology is essentially non-existent in the literature: OpenOOD \cite{openood, openood15}, the standard \ac{ood} detection benchmarking framework which includes over 40 different methods, contains no method which use \ac{xai} as part of its functioning.

\section{Problem Statement} \label{section:problemstatement}

As explained in the previous segment, \ac{ood} detection is a developing field, which has become more important in recent years as machine learning is being used for higher impact tasks, such as disease detection, autonomous driving or infrastructure inspection. As reported by \cite{openood15}, there are to date no \ac{ood} detection methods which outperform all others on different benchmarks, which means that there is great potential for further research. Finding novel methods which improve a model's ability to detect when input is \ac{ood} is important to increase the robustness of machine learning models as they are used in real-world scenarios. The field of \ac{xai} is concerned with understanding the inner workings of a model, and could thus offer insights which can help us detect unusual behaviour in the model as a result of \ac{ood} data points. The problem statement is thus as follows:

\textbf{To what degree can methods from the field of Explainable Artificial Intelligence be used to improve Out-of-Distribution Detection?}
\\

To answer this question, I introduce 3 objectives:

\begin{enumerate}
    \item Develop \ac{ood} detection frameworks which utilize \ac{xai} as part of their functionality, either by combining traditional \ac{ood} detection methods and \ac{xai} explanations, or by using \ac{xai} explanations on their own.
    \item Analyse the behaviour of different \ac{xai} methods on \ac{id} and \ac{ood} data, to be able to develop effective \ac{ood} detectors under the benchmarks created as part of objective 1.
    \item Perform comprehensive tests on the developed \ac{ood} detectors on \ac{sota} \ac{ood} detection benchmarks, to be able to accurately assess the performance of \ac{xai} based \ac{ood} detection in comparison with existing \ac{ood} detection methods.
\end{enumerate}

\section{Scope} \label{section:scope}

As we will see in chapter \ref{chapter:background}, both the fields of \ac{xai} and \ac{ood} detection are very large, which makes it impossible to explore all the possible ways one might combine \ac{xai} and \ac{ood} detection. Thus, it is necessary to restrict the scope of both the \ac{xai} and \ac{ood} detection methods used. In this section, I will describe the choices I've made and give a short explanation. In later chapters, the choices will be justified more thoroughly.

The field of \ac{ood} detection is primarily concerned with image classification tasks. Thus, my project will also deal exclusively with image classification datasets. Given this type of data, the choice of \ac{xai} algorithms naturally gravitates towards post-hoc, saliency based methods. The choice of \ac{ood} detection methods is not significantly restricted by the choice to deal exclusively with image data, but I will exclude methods which use outlier exposure to improve performance, or which require retraining of the model. These choices are informed by \cite{openood}, which have found that outlier data is not necessary to achieve \ac{sota} performance, and that "post-hoc methods [...] are generally no worse than methods that require training". Excluding methods which require training drastically decreases the time required for development and testing of new methods, and is thus suitable for an exploratory thesis such as this one. For image classifiers, I limit myself to \ac{cnn} based models, as they are compatible with a far larger amount of \ac{xai} methods than other computer vision models, such as vision transformers.

\section{Research Methods}

\ac{acm} \cite{acm} defines three paradigms for conducting research in the field of computing: {\it theory}, {\it abstraction} and {\it design}. The research methods in this thesis most closely align with the abstraction paradigm. This paradigm has the following four stages:

\begin{enumerate}
    \item Form a hypothesis
    \item Construct a model and make a prediction
    \item Design an experiment and collect data
    \item Analyse the results
\end{enumerate}

In this thesis, I hypothesize that \ac{xai} methods could enhance \ac{ood} detection algorithms. Informed by this hypothesis, I construct three \ac{xai} based \ac{ood} detection frameworks. I perform several experiments across four \ac{ood} detection benchmarks, and finally analyze the results of these experiments.

\section{Ethical Considerations}

When developing machine learning models, it is paramount to always consider the ethical implications of the work that is conducted. \ac{ml} algorithms are prone to bias, have potentially large impacts on the environment when trained, and may be opaque and difficult to comprehend, weakening the rights of those that are subjected to \ac{ai} decision-making. Both the field of \ac{ood} detection and \ac{xai} are attempts to remedy some of these issues, and this work could therefore have positive societal impacts. \ac{ood} detection is concerned with alerting users when an \ac{ai} model is faced with unexpected data, and novel methods developed in this field may thus mitigate problems associated with biased datasets and improve the safety of \ac{ai} models integrated in high-impact decision-making. 

In this work, only publicly available datasets like ImageNet, CIFAR and Places365 have been utilized, minimizing any potential impact of using sensitive or private data. Furthermore, the methods developed in this thesis do not require training, reducing the environmental impact. However, extensive testing has been done on multiple datasets, over many hours of compute time, which has incurred a moderate amount of energy usage. In addition, \ac{ood} detection methods make \ac{ai} models more robust in real-world use cases, which can lead to job displacement, creating detrimental societal effects that must be dealt with.

\section{Main Contributions}

In this thesis, the goal has been to investigate whether \ac{xai} methods can be used for \ac{ood} detection, and to introduce proof-of-concept \ac{ood} detection frameworks which use \ac{xai} explanations as part of their functioning. As part of this work, I have developed three frameworks for \ac{xai} based \ac{ood} detection methods, which use the saliency maps (heatmaps) generated by \ac{xai} methods applied to images: {\it Saliency Aggregation}, {\it Saliency Aggregation plus Logit} and {\it Saliency\acs*{vim}}. These three frameworks have been thoroughly tested on all four \ac{ood} detection benchmarks included in OpenOOD \cite{openood, openood15}, the de-facto standard \ac{ood} detection benchmarking tool.

Through the testing of these three methods, I show that \ac{xai} methods can indeed be used for \ac{ood} detection, contrary to the results found by previous research \cite{martinez}. The key takeaways of this thesis are as follows:

\begin{itemize}
    \item \ac{xai} saliency mapping methods generate values which can be used to predict \ac{ood} samples comparable to baseline methods. However, most \ac{xai} methods output normalized saliencies, which removes valuable information necessary to perform \ac{ood} detection. By removing this normalization and using raw saliency values, I show substantial improvements over previous methods \cite{martinez}.
    \item A simple aggregation of raw saliency values performs slightly below the baseline methods of \ac{mls} and \ac{msp} on Near-\ac{ood} datasets, and sometimes outperforms the baselines on Far-\ac{ood} datasets. This shows that \ac{xai} saliency maps, on their own, capture enough information about a network's response to a data sample to effectively discriminate between \ac{id} and \ac{ood} data points.
    \item Depending on the choice of \ac{xai} method, \ac{xai} saliency maps output values which are relatively uncorrelated with traditional baseline \ac{ood} methods. This means that their outputs can be combined with other \ac{ood} detection methods to increase \ac{ood} detection performance. Inspired by the work of \cite{combood}, I combine \ac{xai} saliency aggregates with \ac{mls} by a simple addition of Z-scores. Under this framework, I find that the performance of \ac{xai} based \ac{ood} detection is increased by several percentage points over the baselines. In addition, on all four benchmarks, one method under this framework performs within 4.1 percentage points of \ac{sota} post-hoc \ac{ood} detection models on either Near- of Far-\ac{ood}.
    \item Saliency values can also be appended directly to the model logits in \ac{ood} detection methods such as \ac{vim} \cite{vim}, leading to statistically significant improvements over using the method without \ac{xai} saliency values across several benchmarks.
\end{itemize}

These three frameworks are highly general and enable further research into the integration of \ac{xai} and \ac{ood} detection. The performance attained methods under these frameworks show great potential, and comes close \ac{sota} methods in several instances.

In addition, I have had code merged into the codebase for OpenOOD, the definitive \ac{ood} detection framework and benchmarking tool. By improving the code used when benchmarking new \ac{ood} detection methods, I have contributed to the broader field of \ac{ood} detection.

\section{Thesis Outline}

Chapter \ref{chapter:background} gives a short introduction to machine learning, followed by a deeper look at the fields of \ac{xai} and \ac{ood} detection. Chapter \ref{chapter:methodology} introduces my methodology; the methods I have used to compare and contrast \ac{xai} explanations on \ac{id} and \ac{ood} data, as well as the three general \ac{ood} detection frameworks which integrate explanations into their functioning. Furthermore, I introduce the different benchmarks which have been used to test each method, the statistical methods used to ensure that the results are statistically significant and the software and hardware architecture used. Chapter \ref{chapter:experiments} first goes into the results of the comparison between \ac{id} and \ac{ood} explanations, detailing the differences between different \ac{xai} methods. These investigations have been done on validation benchmarks which are not used during testing, to ensure unbiased results. Based on these results, I have chosen a selection of \ac{xai} \ac{ood} detection methods under the developed frameworks. These frameworks are then tested on bootstrapped testing benchmarks and compared against baseline methods using statistical analysis. After the experiments follow a discussion (chapter \ref{chapter:discussion}), where I discuss the results in relation to the problem statement, and conduct deeper analyses on the overall performance of the developed methods. Finally, the conclusion (chapter \ref{chapter:conclusion}) gives a short conclusion of my findings, and envisions a way forward for future work.

\chapter{Background} \label{chapter:background}

In this chapter, I first give a short introduction to important concepts in the field of machine learning generally, followed by a more in-depth look at the fields of \ac{ood} detection and \ac{xai}. Finally, I give an overview of related works; papers which have attempted to use \ac{xai} for \ac{ood} detection.

\section{Machine Learning}

Machine Learning is the field of algorithms that are able to learn from data, as opposed to being explicitly programmed. Such algorithms use statistical methods to learn relationships in data, and use these relationships to generalize to unseen data. More formally, \cite{mitchell} gives the following definition of machine learning algorithms:

\begin{definition}
A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$ if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.
\end{definition}

Thus, machine learning is a different paradigm from traditional problem solving, where programs are made to solve problems by following explicit rules. For example, a traditional image classification system attempting to differentiate between malignant and benign tumors might use hand-crafted rules which consider the texture, color and size of a tumor, developed by medical professional with years of experience. As one might imagine, such rules will quickly become very complicated when we consider all the possible factors which might influence the appearance of a tumor. Using a machine learning approach, we would instead feed an algorithm with thousands of images of both benign and malignant tumors, and the rules could then be automatically updated until the algorithm predicted the correct category with a high enough accuracy.

Machine Learning is commonly divided into the three subcategories of supervised, unsupervised and reinforcement learning

\subsection{Supervised Learning}

Supervised Learning is a subcategory of machine learning, where we have a dataset containing both inputs and desired outputs. In the example above, we could use supervised learning by creating a dataset of images of tumors (the input) and corresponding labels which indicate whether each tumor is malignant or benign (the desired output). The learning goal of the algorithm is then to associate images with the correct label. Because we know the correct answer, we are able to fine-tune the algorithm automatically whenever it makes a mistake. However, supervised learning requires labeled data, which can be very costly, especially in the medical domain, where deciding whether a tumor is malignant or benign requires expert knowledge.

\subsection{Unsupervised Learning}

In unsupervised learning, we do not have any labels. In these cases, we might not know whether data points belong to different classes or not. Instead, we can use machine learning to uncover patterns in the data, for example by attempting to cluster the data into different groups and seeing if these groups are sufficiently separated. An example use case could be for fraud detection in a bank. By feeding financial transaction from many different users into an unsupervised learning model and asking it to perform clustering of the data, it might be possible to find a group of users whose transactions differ substantially from the rest, which might indicate that their transactions are fraudulent.

\subsection{Reinforcement Learning}

Reinforcement Learning deals with problems where we do not know exactly what the correct solution is, but we are able to assess whether a given solution is good or not. For example, when controlling a robot arm, it is difficult to say exactly what angles each joint should be for every millisecond when picking up an object, but if the arm does not pick up the object, we know the algorithm has failed. In these problems, the algorithm is trained through reinforcement, where good attempts are rewarded and bad attempts are punished.

\section{Neural Networks}

Neural Networks constitute a class of machine learning algorithms which have become the clear \ac{sota} in almost all fields where machine learning is applied. Notable examples are computer vision, image classification, speech recognition, text and image generation and machine translation. Neural networks are loosely inspired by our own brains, where neurons are connected together and send information between each other. By connecting thousands of neurons together, neural networks are able to learn complicated relationships between the input and output.

\subsection{Feed Forward Neural Networks}

The \ac{ffnn}, also known as a Multilayer Perceptron, traces its roots to the very beginning of machine learning, through the work of Frank Rosenblatt \cite{rosenblatt}. It forms the basic structure for neural networks which has been adapted and modified over the years to form more complex architectures such as convolutional, recurrent or residual neural networks. In an \ac{ffnn} the input values are passed through an affine transformation (a matrix multiplication followed by the addition of a bias), and then passed through an activation function. The output of this activation function can then go through the same process again, which constitutes a single "layer". By stacking several of these layers, with non-linear activation functions, an \ac{ffnn} is able to learn arbitrarily complex mappings between inputs and outputs\footnote{In fact, by the Universal Approximation Theorem \cite{uat}, only a single hidden layer between the input and output is necessary, although this theorem does not give a way to construct such a network for any given function}. Figure \ref{fig:ffnn}\footnote{Figure by Izaak Neutelings, "Neural Network with coefficients, arrows", TikZ.net, licensed under CC BY-SA 4.0, [\url{https://tikz.net/neural\_networks/}].} shows a simple \ac{ffnn} architecture with three hidden layers. In this case, the bias has been omitted for brevity. Here, we can see how all nodes of a layer are connected to the following layer. By using an activation function on the nodes of the hidden layer, before their values are sent to the next layer, we achieve the non-linearity required to learn complex patterns.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[x=2.2cm,y=1.4cm]
  \message{^^JNeural network with arrows}
  \readlist\Nnod{4,5,5,5,3} % array of number of nodes per layer
  
  \message{^^J  Layer}
  \foreachitem \N \in \Nnod{ % loop over layers
    \edef\lay{\Ncnt} % alias of index of current layer
    \message{\lay,}
    \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
    \foreach \i [evaluate={\y=\N/2-\i; \x=\lay; \n=\nstyle;}] in {1,...,\N}{ % loop over nodes
      
      % NODES
      \node[node \n] (N\lay-\i) at (\x,\y) {$a_\i^{(\prev)}$};
      %\node[circle,inner sep=2] (N\lay-\i') at (\x-0.15,\y) {}; % shifted node
      %\draw[node] (N\lay-\i) circle (\R);
      
      % CONNECTIONS
      \ifnum\lay>1 % connect to previous layer
        \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
          \draw[connect arrow] (N\prev-\j) -- (N\lay-\i); % connect arrows directly
          %\draw[connect arrow] (N\prev-\j) -- (N\lay-\i'); % connect arrows to shifted node
        }
      \fi % else: nothing to connect first layer
      
    }
    
  }
  
  % LABELS
  \node[above=0,align=center,mygreen!60!black] at (N1-1.90) {input\\[-0.2em]layer};
  \node[above=0,align=center,myblue!60!black] at (N3-1.90) {hidden layers};
  \node[above=0,align=center,myred!60!black] at (N\Nnodlen-1.90) {output\\[-0.2em]layer};
  

    \end{tikzpicture}
    \end{center}
    \caption[Feed Forward Neural Network]{Figure showing a simple Feed Forward Neural Network, with nodes labeled. The number in parentheses indicates the layer number while the subscript indicates the node number within the layer.}
    \label{fig:ffnn}
\end{figure}



Mathematically, a single layer can then be described as follows:

\begin{equation}
\bm{x_{i+1}} = \sigma_i (A_i \bm{x_i} + \bm{b_i})
\label{ffnn}
\end{equation}

\noindent Here, the input $\bm{x_i}$ is linearly transformed by the weights of the matrix $A_i$ from the input space to the output space, then each value of the new vector in the output space is adjusted by an addition of a bias term, and finally an activation function ($\sigma_i$) is applied to each value. The size of the input and output layers is determined by the number of input and output features, respectively. The activation function of the output layer is determined by the application, most commonly {\it sigmoid} for binary classification, {\it softmax} for multi-class classification and simply identity for regression.


\subsection{Convolutional Neural Networks} \label{section:cnn}

\acp{ffnn} have some inherent flaws which make them unsuitable for working with high dimensional, spatially connected data, such as the pixels which make up an image. Firstly, each input of a \ac{ffnn} is connected to every output of the following layer. If we want to connect the input pixels of a $224$ by $224$ image to a layer of 100 nodes, our first layer will have over 5 million weights, which is already quite a lot for a relatively small image. Furthermore, these weights will have to encode redundant information, because each pixel is considered separately. Consider a network attempting to detect the presence of a cat in an image: We would want the network to detect the cat regardless of whether it is in the middle, the right corner, or any other position in the image. In an \ac{ffnn}, the weights connected to any of these positions in the image would then have to encode a cat detector separately from all the others.

\acp{cnn} solve both these issues by using small kernels of weights which are "slid" across the entire input. By using the same weights across all positions of the image, we do not need to train separate detectors for different positions, giving us translation invariance. Figure \ref{fig:conv} shows the functioning of a convolutional kernel on a 3-channel image. Each value in the output is a weighted sum of a neighbourhood of values in the input image, where the weights are defined by the kernel. As we can see, the same weights are used on all positions, drastically reducing the number of parameters that need to be tuned. In a 2d-convolution, the kernel has the same number of channels as the input, and is only slid across the height and width dimension. The kernel in this figure is a {\it Sobel Operator}, and detects vertical edges. In a \ac{cnn}, the weights of each kernel are not specified manually, but rather learned through backpropagation.

\begin{figure}[hbtp]
    \begin{center}

        \begin{tikzpicture}[
    2d-arr/.style={minimum size=0.8cm, matrix of nodes, row sep=-\pgflinewidth, column sep=-\pgflinewidth, nodes={draw}}
  ]

  \begin{pgfonlayer}{foreground}
      \matrix (mtr) [2d-arr] {
      |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 0 & |[fill=white]| 5 & |[fill=white]| 3\\
      |[fill=white]| 2 & |[fill=white]| 2 & |[fill=white]| 8 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 2\\
      |[fill=white]| 6 & |[fill=white]| 8 & |[fill=white]| 0 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1\\
      |[fill=white]| 9 & |[fill=white]| 3 & |[fill=white]| 7 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 9\\
      |[fill=white]| 1 & |[fill=white]| 3 & |[fill=white]| 6 & |[fill=white]| 5 & |[fill=white]| 2 & |[fill=white]| 8\\
      |[fill=white]| 8 & |[fill=white]| 8 & |[fill=white]| 2 & |[fill=white]| 6 & |[fill=white]| 3 & |[fill=white]| 2\\
      };
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
      \matrix (mtr2) [2d-arr, above right=-7 cm of mtr] {
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
      };
    \end{pgfonlayer}

  \begin{pgfonlayer}{background}
      \matrix (mtr3) [2d-arr, above right=-7 cm of mtr2] {
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 1 & |[fill=orange!30]| 1 & |[fill=orange!30]| 0 & |[fill=orange!30]| 0\\
          |[fill=white]| 0 & |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
          |[fill=white]| 1 & |[fill=white]| 1 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0 & |[fill=white]| 0\\
      };
    \end{pgfonlayer}

  \node[below=of mtr-5-3.south east] {Image};

  \node[right=0.2em of mtr3] (str) {$*$};

  \begin{pgfonlayer}{foreground}
  \matrix (K) [2d-arr, right=0.2em of str, nodes={draw, fill=teal!30}] {
    1 & 0 & -1 \\
    2 & 0 & -2 \\
    1 & 0 & -1 \\
  };
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
  \matrix (K2) [2d-arr, below left=-3.965cm of K, nodes={draw, fill=teal!30}] {
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 1 \\
  };
  \end{pgfonlayer}

  \begin{pgfonlayer}{background}
  \matrix (K3) [2d-arr, below left=-3.965cm of K2, nodes={draw, fill=teal!30}] {
    1 & 0 & 1 \\
    0 & 1 & 0 \\
    1 & 0 & 1 \\
  };
  \end{pgfonlayer}

  \node[below=of K-2-2] {Kernel};

  \node[right=0.2em of K3] (eq) {$=$};

  \matrix (ret) [2d-arr, right=0.2em of eq] {
  4 & 8 & -4 & 2\\
  2 & -2 & 12 & |[fill=blue!80!black!30]| 3\\
  5 & 3 & 8 & 1\\
  3 & 8 & -6 & 8\\
  };
  \node[below=of ret-3-3.south west] {Image $*$ Kernel};

  \begin{pgfonlayer}{foreground}
      \draw[teal] (mtr-4-6.south east) -- (K-3-1.south west);
      \draw[teal] (mtr-2-6.north east) -- (K-1-1.north west);

      \draw[blue!80!black] (K-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}

  \begin{pgfonlayer}{main}
      \draw[teal] (mtr2-4-6.south east) -- (K2-3-1.south west);
      \draw[teal] (mtr2-2-6.north east) -- (K2-1-1.north west);

      \draw[blue!80!black] (K2-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K2-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}

  \begin{pgfonlayer}{background}
      \draw[teal] (mtr3-4-6.south east) -- (K3-3-1.south west);
      \draw[teal] (mtr3-2-6.north east) -- (K3-1-1.north west);

      \draw[blue!80!black] (K3-1-3.north east) -- (ret-2-4.north west);
      \draw[blue!80!black] (K3-3-3.south east) -- (ret-2-4.south west);
  \end{pgfonlayer}


\end{tikzpicture}

    \end{center}
    \caption[Convolution example]{Figure showing a convolutional kernel applied to a 3-channel image.}
    \label{fig:conv}
\end{figure}

By using several different kernels, we can detect many different patterns despite each kernel only detecting a single type. By using the outputs of all the kernels as inputs to a new set of kernels, we can use the same type layer structure as in an \ac{ffnn}, allowing us to extract information in a hierarchical manner. It is common to see that trained \acp{cnn} have early layers that detect edges and texture, later layers that use these edge and pattern detections to detect larger shapes, while the final layers combine the shapes to detect entire objects \cite{lenet5}.

By not evaluating every possible position in the input image, \acp{cnn} downsample the image, and are able to reduce the number of operations considerably. Simultaneously, this downsampling enables each subsequent layer to consider a larger area of the input image than the previous (a larger field-of-view), which allows larger patterns to be discovered. Simultaneously, it is common to use a larger and larger amounts of kernels on the new input, thus increasing the channel depth while the spatial dimensions are reduced. Between each layer, we use non-linear activation functions, similarly to how the are used in \acp{ffnn}.

After several such convolutions, we can flatten the output, either by aggregating each channel using \ac{gap} or a similar method, or we can simply flatten all dimensions and consider the three dimensional feature map as a long vector of shape $C \times H \times W$. By doing this, we can pass the output to one or more linear layers, which can perform classification or regression on the extracted features and give us a final prediction. Figure \ref{fig:cnn2} shows a high level overview of this process. Here, the input, which has only 3 channels, has its spatial dimensions reduced while its channel depth increases through consecutive convolutions. Finally, we have a certain number of channels in our final feature map, which are flattened (in this case with \ac{gap}) and processed through a linear layer to give a final prediction.


\begin{figure}[hbtp]
    \begin{center}
    \end{center}


    \newcommand{\drawRectangles}[5]{
    % #1: Starting x-coordinate
    % #2: Starting y-coordinate (top)
    % #3: Rectangle size
    % #4: Spacing between rectangles
    % #5: Number of rectangles


    % Loop to draw rectangles
    \foreach \i in {0, 1, ..., \the\numexpr#5-1} {
        % Calculate x-coordinate for the current rectangle
        \pgfmathsetmacro{\x}{#1 + \i * #4)}
        \pgfmathsetmacro{\y}{#2 - \i * #4)}
        
        \pgfmathsetmacro{\fillColor}{mod(\i, 2) ? "ce0e0e0" : "ca0a0a0"}
        
        % Draw the rectangle
        \path[draw=black, fill=\fillColor, opacity=1, line width=0.0cm] 
            (\x, \y) rectangle (\x + #3, \y - #3);
    }
}


\def \globalscale {0.7}
\begin{tikzpicture}[y=1cm, x=1cm, yscale=\globalscale,xscale=\globalscale, every node/.append, inner sep=0pt, outer sep=0pt]
  \begin{scope}[shift={(3.5, 0.3)}]
    \path[draw=black,fill=red!20,opacity=1, line width=0.0cm] (12.6, 16.2) 
  rectangle (18.6, 10.3);
    \path[draw=black,fill=green!20,opacity=1, line width=0.0cm] (12.7, 16.1) 
  rectangle (18.7, 10.2);
    \path[draw=black,fill=blue!20,opacity=1, line width=0.0cm] (12.8, 16) 
  rectangle (18.8, 10.1);

    \begin{scope}[shift={(14, 11)}]
        \marmot[scale=1.3, body=blue]
        \bear[xshift=2.4cm, yshift=2.5cm, body=blue]
        \coati[rotatehead=-15, xshift=2.6cm, yshift=-0.5cm, body=blue]
    \end{scope}

    \drawRectangles{19.9}{15.0}{3.0}{0.1}{8}

    \drawRectangles{23.9}{14.8}{1.5}{0.12}{16}

    \drawRectangles{26.9}{15.3}{0.4}{0.133}{32}



    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (16.4, 14.3) rectangle (16.6, 14.1);
    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (21.3, 12.4) rectangle (21.7, 12.0);
    \path[draw=black,fill,fill opacity=0.0,draw opacity=1, line width=0.2mm] 
  (26.2, 11.9) rectangle (26.4, 11.7);

    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (16.6, 14.1) -- 
  (22.6, 13.3);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (16.6, 14.3) -- 
  (22.6, 13.3);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (21.7, 12.0) -- 
  (26.2, 11.8);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (21.7, 12.4) -- 
  (26.2, 11.8);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (26.4, 11.7) -- 
  (31.1, 11.0);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (26.4, 11.9) -- 
  (31.1, 11.0);

    \path[draw=black,fill=ce0e0e0,opacity=1, line width=0.0mm] (30.8, 13.5) -- 
  (31.1, 13.5) -- (32.0, 12.6) -- (31.7, 12.6) -- (30.8, 13.5)-- cycle;;
    \path[draw=black,fill=ce0e0e0,opacity=1, line width=0.0mm] (32.0, 13.2) -- 
  (32.3, 13.2) -- (32.6, 12.9) -- (32.3, 12.9) -- (32.0, 13.2)-- cycle;;

    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (31.41, 11.15) -- 
  (31.7, 12.6);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (27.3, 15.3) -- 
  (30.8, 13.5);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (32.0, 12.6) -- 
  (32.3, 12.9);
    \path[draw=black,fill,draw opacity=1, line width=0.2mm] (31.1, 13.5) -- 
  (32.0, 13.2);


    \node[anchor=south west] (text10) at (19.9, 10.0){Conv};
    \node[anchor=south west] (text11) at (24.3, 10.0){Conv};
    \node[anchor=south west] (text12) at (27.9, 10.0){Conv};
    \node[anchor=south west] (text13) at (31.9, 11.0){GAP};
    \node[anchor=south west, align=left] (text14) at (12.6, 16.5){Input RGB Image:\\3@224x224};
    \node[anchor=south west] (text15) at (19.9, 15.4){8@112x112};
    \node[anchor=south west] (text16) at (23.9, 15.2){16@56x56};
    \node[anchor=south west] (text17) at (26.9, 15.7){32@14x14};
    \node[anchor=south west] (text18) at (30.8, 13.9){1x32};
    \node[anchor=south west] (text19) at (32.0, 13.6){1x10};
  \end{scope}



\end{tikzpicture}

    \caption[CNN example]{Figure showing a high level overview of how a CNN functions}
    \label{fig:cnn2}
\end{figure}


% \begin{figure}[hbtp]
%     \begin{center}
%
%
%     \begin{tikzpicture}[x={(1,0)},y={(0,1)},z={({cos(60)},{sin(60)})}, scale=1.5]
% %
% % comment these out if you want to see where the axes point to
% % \draw[-latex] (0,0,0) -- (3,0,0) node[below]{$x$};
% % \draw[-latex] (0,0,0) -- (0,3,0) node[left]{$y$};
% % \draw[-latex] (0,0,0) -- (0,0,3) node[below]{$z$};
% % a plane
% \draw pic (box1-1) at (1,-1.6/2,0) {fake box=white!70!gray with dimensions {1/1.6/1.6} and {2*1.6} and {1*1.6} and Input};
%
% \foreach \X [count=\Y] in {1.4,1.2,1.2,1}
% {
%     \draw pic (box1-\Y) at (\Y+1,-\X/2,0) {fake box=white!70!gray with dimensions {1/\X/\X} and {2*\X} and {1*\X} and Conv};
% }
%
% \foreach \X/\Col in {6.5/blue,6.7/red,6.9/lightgray, 7.1/lightgray, 7.3/green}
% {\draw[canvas is yz plane at x = \X, transform shape, fill =
% \Col!50!white] (0,0.10) rectangle (1,-0.5);}
% % \draw[gray!60,thick] (6.3,-0.1,-1.6) coordinate (1-1) -- (6.3,-0.1,0.6) coordinate (1-2) -- (6.3,2.,0.6) coordinate (1-3) -- (6.3,2.1,-1.6) coordinate (1-4) -- cycle;
% % \draw[gray!60,thick] (7.1,-0.1,-1.6) coordinate (2-1) -- (7.1,-0.1,0.6) coordinate (2-2) -- (7.1,2.,0.6) coordinate (2-3) -- (7.1,2.1,-1.6) coordinate (2-4) -- cycle;
% % \foreach \X in {4,1,3}
% % {\draw[gray!60,thick] (1-\X) -- (2-\X);}
% %
% \node[draw,single arrow, fill=blue!10] at (8,0.5,0) {GAP};
% \node[circle,draw,blue,fill=blue!30] (A1) at (9,1,0) {~~~};
% \node[circle,draw,red,fill=red!30,below=4pt of A1] (A2) {~~~};
% \node[circle,draw,green,fill=green!30,below=18pt of A2] (A3) {~~~};
% \draw[circle dotted, line width=2pt,shorten <=3pt] (A2) -- (A3);
% \node[circle,draw,gray,fill=gray!20] (B1) at (10,1,0) {~~~};
% \node[circle,draw,fill=gray!60,below=4pt of B1] (B2) {~~~};
% \node[circle,draw,gray,fill=gray!20,below=18pt of B2] (B3) {~~~};
% \draw[circle dotted, line width=2pt,shorten <=3pt] (B2) -- (B3);
% \begin{scope}[on background layer]
% \node[orange,thick,rounded corners,fill=blue!10,fit=(A1) (A3)]{};
% \node[gray,thick,rounded corners,fill=gray!10,fit=(B1) (B3)]{};
% \end{scope}
% \foreach \X in {1,2,3}
% {\draw[-latex] (A\X) -- (B2);}
% \end{tikzpicture}
%
%     \end{center}
%     \caption[CNN example]{Figure showing a high level overview of how a CNN functions}
%     \label{fig:cnn2}
% \end{figure}

\section{Model evaluation}

\ac{ood} detection is essentially a binary classification problem. Thus, the metrics I will use in this thesis are those used for such problems. In the field of \ac{ood} detection, \ac{auroc} and \ac{fpr} are most commonly used. As such I shall focus on these metrics, as opposed to for example the \ac{aupr}.

\subsection{Accuracy}

Accuracy is the simplest metric used in binary classification. It is simply the ratio of correct predictions over all instances in the data set. This metric has the advantage of being simple to understand and calculate, but it is very often insufficient. A simple (and quite common) scenario where accuracy fails to capture the performance of a model is any situation where there are large class imbalances. For example, imagine we have trained a \ac{cnn} to predict whether a person has lung cancer or not, based on CT-scans of their lungs. As most people do not have lung cancer, we can imagine that such a dataset is highly imbalanced, for example that only 1 in 100 people actually have cancer. If a model simply predicts that no one ever has lung cancer, it will be correct in $99\%$ of cases, and will thus have an accuracy of $99\%$, although it has missed every instance of cancer and is completely unusable in any real context.

For the purposes of \ac{ood} detection, accuracy is thus insufficient, as we have no guarantees that \ac{id} and OOD will be balanced. In fact, we expect OOD data to be relatively rare, given that the goal of developing an \ac{ai} model is to ensure high performance during deployment, which necessitates having training data which covers as much as possible of the data seen during inference.

\subsection{Metrics utilizing the binary classification confusion matrix} \label{section:aurocfpr95}

Instead of simply considering whether a prediction was correct or not, we should take into account the different combinations of prediction and ground truth, considering positive and negative classes separately. Figure \ref{fig:confusion} shows the possible four possible combinations given a ground truth class and a predicted class.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}
            % Draw the grid
    \draw[thick] (0,0) rectangle (4,4);
    \draw[thick] (2,0) -- (2,4);
    \draw[thick] (0,2) -- (4,2);

    % Label the axes
    \node[align=center] at (1,4.6) {Predicted\\Positive};
    \node[align=center] at (3,4.6) {Predicted\\Negative};

    \node[align=center] at (-1, 3) {Actual\\Positive};
    \node[align=center] at (-1, 1) {Actual\\Negative};

    % Label the cells
    \node[align=center, fill=green!20] at (1,3) {True\\Positive};
    \node[align=center, fill=red!20] at (1,1) {False\\Positive};
    \node[align=center, fill=red!20] at (3,3) {False\\Negative};
    \node[align=center, fill=green!20] at (3,1) {True\\Negative};
    \end{tikzpicture}

\end{center}
    \caption[Binary classification confusion matrix]{Figure showing the binary classification confusion matrix, denoting the four possible combinations created by the ground truth and predicted class. Green cells denote correct predictions, while red cells denote wrong predictions.}
\label{fig:confusion}
\end{figure}

With these possibilities defined, we can begin to gain a clearer picture of the performance of a model.

\subsubsection{Precision and Recall}

Precision is the share of positive predictions that were actually positive. With a high false positive rate, we have a low precision, which means that the model is erroneously flagging many negative classes as positive. In an \ac{ood} detection setting, a model which flags many \ac{id} samples as \ac{ood} would have a low precision, if we treat \ac{ood} samples as the positive class.

Recall is the share of actual positive samples that were predicted positive. Recall tells us how many of positive samples we missed. In and \ac{ood} detection context, a model which lets many \ac{ood} samples slip by undetected will have a low recall score.

Precision and recall are often used together because evaluate the model in different ways that complement each other. If a model has both high precision and high recall, it does not erroneously flag many negative classes as positive, nor does it miss many positive classes.

\subsubsection{Sensitivity and Specificity}

Sensitivity and specificity is another pair of metrics that is commonly used for evaluating binary classification. Sensitivity is equivalent to recall; the share of positive samples that were correctly predicted as positive. Specificity is the share of the negative samples that were correctly predicted as negative. Sensitivity and specificity are also known as \ac{tpr} and {\it True Negative Rate}.

\subsection{Threshold Independent Metrics}

The previous metrics are a clear improvement over simply using accuracy. However, they still have the problem that they are all dependent on what threshold one sets when predicting something to be a negative or positive class. Thus, it becomes harder to compare different models by using these metrics. Indeed, by simply increasing the threshold of any classifier, we can increase the true negative rate. Similarly, by decreasing the threshold, we can increase the true positive rate.\\

\subsubsection{Area under Receiver Operating Characteristic}

\ac{auroc} remedies this problem by looking at all possible thresholds, and calculating the \ac{tpr} (equivalent to sensitivity, recall), and the \ac{fpr} (equivalent to $1 -$ specificity) for each possible threshold. With these values calculated, we can plot each point on a graph, giving us an \ac{roc} plot. Figure \ref{fig:auroc} shows this plot, for three different models.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/auroc.pgf}
    \end{center}
    \caption[AUROC example figure]{Figure showing the \ac{auroc} curve for two imagined classifiers; one which has an \ac{auroc} of 0.97 and one which has an \ac{auroc} of 0.80. In addition, the \ac{auroc} curve of pure guessing is shown in gray.}
    \label{fig:auroc}
\end{figure}

Once we have done this, we can calculate the integral under this curve, giving us the \ac{auroc}. If a binary classification model can perfectly separate the two classes, then all possible thresholds will either have 100\% \ac{tpr} or 0\% \ac{fpr}, giving an area under the curve of 1. If instead a model has no discriminative power, then the predicted values of positive and negative classes are entirely random, and all changes to the threshold will increase one of the metrics at the expense of the other. Such a model would have \acp{tpr} and \acp{fpr} making a straight line of points, and an \ac{auroc} of 0.5. In between these extremes, we can evaluate different models, without having to consider different thresholds. Figure \ref{fig:auroc} shows what the \ac{roc}-curve of a model with either 0.97 or 0.80 \ac{auroc} looks like, as well as that of a random classifier with \ac{auroc} = 0.50. 

One important thing to note about the \ac{auroc} is that values lower than 0.50 do not mean that a model is worse than random guessing. This is because if a model is consistently wrong, we can simply choose the opposite category of what the model outputs, and gain a new model which is better than random guessing. For example, if a cat versus dog detector gave an actual image of a dog a higher chance of being a cat than an actual image of a cat in 95\% of cases, it would have an \ac{auroc} of only 0.05. However, if we simply multiplied all outputs of the model by -1, we would suddenly have a model which correctly gives a cat a higher chance of being a cat in 95\% of the cases, and an \ac{auroc} of 0.95. Thus, we really only care about getting \ac{auroc} scores far away from 0.50.\\

\subsubsection{False Positive Rate at 95\% Recall}

\ac{fpr95} is another way of comparing models without having to consider specific thresholds. Instead, we simply select the threshold which gives a recall (equivalent to \ac{tpr}) of 0.95, and calculate the \ac{fpr} at this threshold. The drawback to this metric as opposed to \ac{auroc} is that we do not get a general view of how the model performs. However, if we have a requirement that the model has a very high true positive rate, we may not care about how the model performs at any other threshold, and thus this metric is suitable. It is of course also possible to calculate this metric at any other recall value, depending on the application. However, in the field of \ac{ood} detection, \ac{fpr95} is the metric that is used in the vast majority of cases \cite{oodbaseline, odin, oodoverview, openood, vim}.


\section{Explainable Artificial Intelligence} \label{chapter:xai}

Below follows a thorough introduction to \ac{xai}, as well as detailed look at some important methods for explainability for neural networks applied to images. Specifically, saliency methods will be explained in detail, as they constitute a core part of my thesis.

\subsection{The motivation for Explainable Artificial Intelligence}

Given the impressive performance of \ac{dl} methods, one might be convinced that these models do not need to be explainable or interpretable, and that we instead should just place our faith in the model without knowing exactly how it came to a decision. However, as \cite{doshivelez} points out, "a single metric, such as classification accuracy, is an incomplete description of most real-world tasks". Small differences between the data distribution when the test data was collected and when the model is deployed may have a large impact on the model's performance, or the model may have learned artifacts or specificities in the training dataset which were also present in the test dataset, leading to a false belief that the model has gained generalizable knowledge when it has not. By using explainable methods, we may reveal these shortcomings. Relevant to this thesis, this may also have the secondary effect of separating \ac{id} and \ac{ood} data points.

\ac{xai} is also especially important whenever the model is used in settings where its decisions have a high impact. If a model is used by a hospital for disease detection, both the patient and doctor will probably want to be able to understand why the model has found that a disease is present. For them, high performance on a test set of different cases may not be enough. As \cite{xaisurvey} states, "for the regulated healthcare domain, it is utmost important to comprehend, justify, and explain the AI model predictions for a wider adoption of automated diagnosis". In other high impact areas, such as autonomous driving, the impact of wrong decisions by the network can have fatal consequences, and customers and regulators will want to be absolutely sure that the models used are robust and base their decisions on relevant factors as opposed to quirks in the training data. Furthermore, the right to an explanation of an automated decision affecting a person is included in the EU's General Data Protection Regulation, which states that "In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the right [...] to obtain an explanation of the decision reached after such assessment and to challenge the decision." \cite{gdpr}.

% \subsection{The Properties of an Explanation}

\subsection{Taxonomy of Explainable Artificial Intelligence}

This section goes through three axes which define an \ac{xai} method:

\begin{itemize}
  \item Intrinsically explainable models versus post hoc methods
  \item Model dependent versus model agnostic methods
  \item Global versus local explanations
\end{itemize}


\subsubsection{Intrinsically explainable models versus post hoc methods}

Intrinsically explainable models are models which have sufficiently low complexity, such that it is feasible for a human to understand them without further modifications. Examples of such methods are linear regression, logistic regression and decision trees \cite{molnar}. 

Post hoc methods are methods which are applied to the model after training. These methods do not aim to constrain the model to be interpretable, but inspect the model after training. For example, after using a convolutional neural network to classify a CT-scan of a tumour (which gave a prediction of malignant), we could run post hoc algorithms on the network which are able to extract which part of the image contributed the most to the prediction. Thus, post hoc methods remove the need for the model to be simple enough for a human to understand by extracting the relevant information for us.

\subsubsection{Model dependent versus model agnostic methods}

Model dependence/agnosticity denotes whether an \ac{xai} method uses specifics of a particular type of model to generate the explanation, or whether the method can generate an explanation without using specifics of the model at all. Explanations based intrinsically explainable models are clearly model dependent, while methods that only use the input and output of the model instead of looking at the internal operations are model agnostic. An example of a model dependent method (which is not simply an intrinsically explainable model) is Class Activation Mapping, which requires a \ac{cnn} with a specific architecture to function, while an example of a model agnostic method is Shapley \cite{shapley}, which treats the underlying model as a black box and uses the inputs and outputs to calculate the marginal effect of a single feature on the output value.

\subsubsection{Global versus local explanations}

Global explanations provide general relationships between the input features and outputs learned by the model over the entire dataset \cite{xaioverview}. In this way, they can show how a specific feature affects the output in general, instead of just how it affects the output of a single point. These methods are ideal for finding trends in the data, but may not be suitable for a patient wanting an explanation for their specific case.

Local explanations do not describe general trends, but focus only on a single data point. These methods give insight into how the features influenced the prediction of a single data point, but these relationships may not hold for other data points, and as such these methods do not give the same insight into the general behaviour of the model.

% \subsection{Benchmarking}
%
% In general, it is difficult to evaluate an AI explanations, and there is no clear consensus in the field as to what metrics should be the standard \cite{molnar, evalxai}.

\subsection{\ac{xai} methods adapted to images: Saliency maps and segmentation} \label{section:saliencymapbackground}


As explained in section \ref{section:scope}, the field of \ac{ood} detection is primarily focused on image data, and as such this is the focus of this thesis as well. Thus, before delving into specific \ac{xai} methods, it is beneficial to elaborate on how \ac{xai} methods are adapted to images. When explaining tabular data made up of categorical and numerical values, it is often common to explain each feature by associating it with some change in the output prediction. For example, one might say that increasing the number of rooms in a house by one increases the predicted sale price by 150 000 NOK, or that the absence of a balcony decreases it by 25 000 NOK. But, given that images are made up of tens or hundreds of thousands of features (RGB pixel values), such an approach may not be suitable.

Firstly, given that we have so many features, it may no longer be interesting to know exactly how much each feature contributes to a prediction, given that we don't expect any single pixel to have a very large impact. Furthermore, inspecting the contribution of each individual pixel, like one might do for tabular data, is not even realistically feasible, due the overwhelming number of features. Given that our input is in the form of an image, which is best understood visually as opposed to numerically, it is then natural to instead present the explanation in the same way.

These visual explanations take the form of saliency maps, as shown in figure \ref{fig:saliency_ex}. Here, we display the saliency values of all pixels in a heatmap. Instead of considering the absolute values, we instead display the saliencies in relation to each other, such that the most important and least important pixel has colors on opposite sides of the colormap. In this way, it is easy and intuitive to see where the important regions of the image are, and we do not need to consider the absolute values of each pixel.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]\textbf{Input image}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/saliency_example/input.png}};
    \node[SIR, label={[align=center]\textbf{Saliency map}}] (id2) [right=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/saliency_example/saliency.png}};


    \begin{scope}[shift={($(id2.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}

    \end{tikzpicture}

    \end{center}
    \caption[Saliency Example]{Figure showing an input image and the corresponding saliency map, generated by an \ac{xai} algorithm which shows the important pixels for the predicted class (Rhodesian Ridgeback). The colorbar and heatmap show the relative saliency for all pixels, with 0 being the least important pixel and 1 being the most important pixel}
    \label{fig:saliency_ex}
\end{figure}

Secondly, many methods which are designed for data tabular data (which have may have 10-20 features) are far too slow when the number of features may be over a hundred thousand. Methods such as \ac{lime}, Occlusion, or Shapley fall into this category, as they permute the input at a rate which scales with the number of inputs.

A solution to this is to segment the image into larger regions, which are treated as one. As opposed to asking "how does the prediction change if we change this pixel", we instead ask "how does the prediction change if we change this {\it region}". All pixels in a region are then awarded the same amount of importance. This drastically reduces the dimensionality of the problem, and allows us to use a much larger range of methods. The output of such methods is also a saliency map, but one where each pixel is not given an individual saliency score, but rather a score derived from the larger region in which it is contained.

Regions of pixels can be created in different ways. The simplest option is consider a window of a specific height and width, and slide this window over the image with a specific stride. By adjusting the stride and size, the number of dimensions can be adjusted. The benefit of such a simple approach is that the segmentation itself introduces no extra computation. However, a substantial downside is that each region can contain completely unrelated objects, because the regions are created without considering the underlying image. For example, if a 60 by 60 pixel region contains in its lower right corner a part of a dog, occluding this region could lead to a large change in confidence and thus high importance. However, it is not just the lower right corner that is awarded high importance, but the entire region, which may contain other objects or parts of the background which are not actually important.

By using more sophisticated segmentation methods, we can avoid this problem. Methods such as \ac{slic} \cite{slic} create regions which can be considered more intuitive than simply using a rectangular sliding window. \ac{slic} performs an iterative clustering of pixels, grouping similar pixels into larger regions called superpixels. The downside to this method is that the iterative CPU-bound process which is used introduces a considerable computational overhead. Figure \ref{fig:segmentationcomp} shows a comparison between the saliency map of the \ac{lime} algorithm applied to an image segmented using rectangular regions and to an image segmented using \ac{slic}. As we can see, the rectangular segmentation means that the some of the grass in the background is given a high saliency, because it happens to be in the same region as the dog's head. Using \ac{slic}, there are few regions which contain both grass and the dog, and thus the high saliency of the dog's face does not affect other parts of the image.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]\textbf{Input image}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_rectangular-img0.png}};
    \node[SIR, label={[align=center]\textbf{\ac{lime} applied to}\\\textbf{rectangular regions}}] (id2) [right=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_rectangular-img1.png}};
    \node[SIR, label={[align=center]\textbf{\ac{lime} applied to}\\\textbf{superpixel regions}}] (id3) [right=of id2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/segmentation_comparison/lime_slic-img1.png}};

    \begin{scope}[shift={($(id3.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}


    \end{tikzpicture}

    \end{center}
    \caption[Segmentation comparison]{Figure showing a comparison between generating saliency maps using the \ac{lime} algorithm on rectangular and superpixel regions. The colorbar and heatmap show the relative saliency for all regions, with 0 being the least important region and 1 being the most important region}
    \label{fig:segmentationcomp}
\end{figure}



\subsection{Specific methods} \label{section:xaimethodsbackground}


The following section goes through several specific \ac{xai} methods. First, the methods which are integral to the methods introduced in chapter \ref{chapter:methodology} are described in detail, so that we have the necessary information to understand and analyze the behaviour of the methods developed as part of this thesis. Following this, I describe a selection of other \ac{xai} methods, to give a broader overview of the field.

\subsubsection{Local Interpretable Model-Agnostic Explanations (LIME)}

\ac{lime} \cite{lime} is a post-hoc, model independent \ac{xai} method. The method is built on the idea that while the decision function of a large neural network (or any other large model) might be far too complex to easily interpret, it can most likely be approximated quite well by a simpler function, as long as we only look at the feature space around a single data point. For example, we could approximate a large feed forward neural network with a simple linear regression model, which can be intrinsically explained due to its low complexity.

To create a locally interpretable model, we need a neighbourhood of data points around our point of interest. To do this, we can sample a number of points from our dataset and weigh them by their distance to our original point. This sampling can be done in many ways, for example by calculating a mean and variance for each feature and sampling from a normal distribution. For image data, we can create new points similar to the image by masking out different regions of the image \cite{molnar}. The distance measure depends on the type of data we are dealing with. Regardless, the distance values are passed through a smoothing kernel which can be tuned to adjust the size of the "neighbourhood".

With these new data points, we can generate new predictions using the original, complex model. Thus, we now have a series of points, each with a weighting based on their distance to our original point, and each with a predicted score from our original model. With such a dataset, we can train a simpler model, which will then approximate the complex model around the point of interest. By inspecting this simple model (for example: the learned weights of a linear regression model, or the structure of a decision tree), we can learn approximately how the complex model functions in a region around this single data point.

\subsubsection{Occlusion methods}

Occlusion methods are a family of post-hoc, model independent \ac{xai} methods. They function by masking different parts of the image and inspecting the change in output score. If an area leads to a large drop in softmax score for the predicted class when masked, this area must have been important for the network when making the prediction. The mask can be as simple as replacing all masked pixels with a single color, such as gray \cite{occlusion}, or they could use more advanced inpainting methods using generative models, for example by replacing a masked tumor with generated healthy tissue. 

Regardless of the mask, one can easily calculate the importance of any pixel for a prediction by calculating the average change in the output score for all masks which contain the specific pixel \cite{diagnostic}. Occlusion methods have the advantage of being completely model independent, since they do not consider the internals of the model. However, the computation can be expensive, because we need to run a forward pass for each position of the mask on the image. % Figure \ref{fig:occlusion} shows the process visually.

\subsubsection{Class Activation Mapping (CAM)}

\ac{cam} \cite{cam} is a model dependent, post-hoc \ac{xai} method, which is used on Convolutional Neural Nets (\ac{cnn}s). For a specific output node of a model (for example, the one denoting the presence of a specific class, such as "cat"), \ac{cam} outputs a heat map showing which areas of the input image contributed to this node. In this way, \ac{cam} gives a visual explanation to which parts of an image the model focused on when making a decision to classify an image to a specific class. This method is model dependent, because it requires a specific architecture in the final layers of the network to work.

\ac{cam} is a relatively simple method to understand. It exploits the fact that various convolutional layers of \ac{cnn}s actually behave as object detectors, even when the training objective is classification \cite{cam}. As \cite{lenet5} explains, the earlier layers "extract elementary visual features such as oriented edges, end-points [or] corners", which can be used by subsequent layers to detect higher-order features. In this manner, the final convolutional layer will detect very high level visual features, combining the extracted information from all the previous layers. This layer is composed of several feature maps, where each map can be thought of as denoting the presence of some specific feature across the original image. The authors perform global average pooling (GAP) on these feature maps, giving a single value for each map, which is followed by a single dense layer and the Softmax activation function. In this way, each output node in the final layer is a weighted sum of all the global average pooled feature maps from the final convolutional layer. This means that we can represent the areas of the image which were used to perform the classification by performing the same weighted sum on the actual feature maps instead, which gives us a heat map which we can overlay on the original image (after upsampling the feature maps).

Figure \ref{fig:cam} shows the process visually. From this we can see that the resulting Class Activation Map (bottom right) gives an intuitive explanation for why the image in the top left gives a high score for the presence of the class "magpie".

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[x={(1,0)},y={(0,1)},z={({cos(60)},{sin(60)})}, scale=1.5,
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm}]
%
% comment these out if you want to see where the axes point to
% \draw[-latex] (0,0,0) -- (3,0,0) node[below]{$x$};
% \draw[-latex] (0,0,0) -- (0,3,0) node[left]{$y$};
% \draw[-latex] (0,0,0) -- (0,0,3) node[below]{$z$};
% a plane
% \draw pic (box1-1) at (1,-1.6/2,0) {fake box=white!70!gray with dimensions {1/1.6/1.6} and {2*1.6} and {1*1.6} and Input};
%
\foreach \X [count=\Y] in {1.4,1.2,1.2,1}
{
    \draw pic (box1-\Y) at (\Y+1,-\X/2,0) {fake box=white!70!gray with dimensions {1/\X/\X} and {2*\X} and {1*\X} and Conv};
}

\def\xbox{-0.7}
\def\ybox{0}
\def\ubox{0.2}

\node[SIR] (id1) at (0.7, 0.5) {\includegraphics[width=0.15\textwidth, height=0.25\textwidth]{figure/cam/cam.png}};

\node[draw,single arrow, fill=blue!10, single arrow head extend=1ex] at (1.7,0.5,0) {};

\draw[gray!60,thick] (6.4,\ybox,\xbox) coordinate (1-1) -- (6.4,\ybox,\ubox) coordinate (1-2) -- (6.4,1.2,\ubox) coordinate (1-3) -- (6.4,1.2,\xbox) coordinate (1-4) -- cycle;

\foreach \X/\Col in {6.5/blue,6.7/red,6.9/lightgray, 7.1/lightgray, 7.3/green}
{\draw[canvas is yz plane at x = \X, transform shape, fill =
\Col!50!white] (0,0.10) rectangle (1,-0.5);}

\draw[gray!60,thick] (7.35,\ybox,\xbox) coordinate (2-1) -- (7.35,\ybox,\ubox) coordinate (2-2) -- (7.35,1.2,\ubox) coordinate (2-3) -- (7.35,1.2,\xbox) coordinate (2-4) -- cycle;


\foreach \X in {4,1,3}
{\draw[gray!60,thick] (1-\X) -- (2-\X);}

\path[decorate,decoration={text effects along path, text={Conv}}, gray] (6.6,0.65,0) -- (6.6,-0.5,0);


\node[draw,single arrow, fill=blue!10] at (8,0.5,0) {GAP};
\node[circle,draw,blue,fill=blue!30] (A1) at (9,1,0) {~~~};
\node[circle,draw,red,fill=red!30,below=4pt of A1] (A2) {~~~};
\node[circle,draw,green,fill=green!30,below=18pt of A2] (A3) {~~~};
\draw[circle dotted, line width=2pt,shorten <=3pt] (A2) -- (A3);
\node[circle,draw,gray,fill=gray!20] (B1) at (10,1,0) {~~~};
\node[circle,draw,fill=gray!60,below=4pt of B1] (B2) {~~~};
\node[circle,draw,gray,fill=gray!20,below=18pt of B2] (B3) {~~~};
\draw[circle dotted, line width=2pt,shorten <=3pt] (B2) -- (B3);
\begin{scope}[on background layer]
\node[orange,thick,rounded corners,fill=blue!10,fit=(A1) (A3)]{};
\node[gray,thick,rounded corners,fill=gray!10,fit=(B1) (B3)]{};
\end{scope}
\draw[-latex] (A1) -- (B2) node[midway,above, text=blue!70!black] {$w_1$};
\draw[-latex] (A2) -- (B2) node[midway,below, text=red!70!black] {$w_2$};
\draw[-latex] (A3) -- (B2) node[midway,below=5pt, text=green!70!black] {$w_n$};

\node[rectangle, draw=blue!70!black, thick] (h1) at (1.5, -2.9) {\includegraphics[width=0.12\textwidth, height=0.22\textwidth]{figure/cam/h1.png}};
\node[rectangle, draw=red!70!black, thick, right=4em of h1] (h2) {\includegraphics[width=0.12\textwidth, height=0.22\textwidth]{figure/cam/h2.png}};
\node[rectangle, draw=green!70!black, thick, right=6em of h2] (h3) {\includegraphics[width=0.12\textwidth, height=0.22\textwidth]{figure/cam/h3.png}};

\node[rectangle, draw=black, thick, right=3.2em of h3] (h4) {\includegraphics[width=0.12\textwidth, height=0.22\textwidth]{figure/cam/h4.png}};

\node[left=0.2em of h1.west, text=blue!70!black] {$w_1 \, \cdot$};

\node[left=0.2em of h2.west, text=red!70!black] {$w_2 \, \cdot$};
\node[left=0.2em of h3.west, text=green!70!black] {$w_n \, \cdot$};
\node[right=0.1em of h1.east] {$+$};
\node[right=0.1em of h2.east] {$+ \dots +$};
\node[right=0.9em of h3.east] {$=$};

\node[] at (5, -1.45) {Class Activation Mapping for class "magpie"};

\draw [decorate, 
	decoration = {brace,
		raise=5pt,
		amplitude=10pt, mirror, aspect=0.25}, line width=2pt, blue!30] (6,-0.7) --  (10.2,-0.7);

\begin{scope}[on background layer={color=blue!20}]
    \fill (0.1,-1.2) rectangle (10.2,-4.2);
    % \fill[color=blue!10] (6, -0.7) -- (0.1, -1.2) --  (11, -1.2) --(10, -0.7) -- cycle;
\end{scope}

\end{tikzpicture}

    \end{center}
    \caption[CNN example]{Figure showing the steps required to create a Class Activation Map. Recreation of figure from \cite{cam}}
    \label{fig:cam}
\end{figure}

Although \ac{cam} is an intuitive and effective method of visualizing the inner workings of a \ac{cnn}, it has some downsides. Firstly, it is highly model dependent, requiring that the model only have a single dense layer after the convolutions. Although there are some \ac{sota} models which only use a single dense layer, this still places a limit on what models can be used, or requires the simplification of models that use more than a single dense layer. \cite[4]{cam} notes a 1-2\% drop in classification performance when performing this simplification. Secondly, the output of \ac{cam} is simply a weighted sum of all the feature maps after the final convolutional layer. As we move deeper in a \ac{cnn}, we reduce the spatial resolution by downsampling, while increasing the number of channels (increasing the depth of the output while reducing the height and width). Because of this, the \ac{cam} will have a drastically lower resolution than the original image, often less than $10 \times 10$, while the input image may be hundreds of pixels in both dimensions. Because of this, \ac{cam} can only show general areas, as opposed to pixel wise explanations.
\\

\subsubsection{Gradient Class Activation Mapping (GradCAM)} \label{section:gradcam}

\ac{gradcam} \cite{gradcam} is an improvement on \ac{cam}, which generalizes the method to function with any \ac{cnn} architecture, thus making the method much less model dependent and avoiding the performance drop incurred when simplifying the model with \ac{cam}. Instead of using the weights of a final layer to calculate a weighted sum of feature maps in the last convolutional layer, \ac{gradcam} uses gradients flowing from the relevant output node to the activation maps to calculate the weights for each feature map. Furthermore, the authors prove that this method is a strict generalization of \ac{cam} \cite[5]{gradcam}, so that no information is lost by using gradients instead of weights.

Like the simplicity of the \ac{cam} method, the calculation of the weights using the gradients is also quite simple, as seen in Equation \ref{gradcameq}.

\begin{equation}
\alpha^c_k = \frac{1}{Z} \sum_i \sum_j \frac{\delta y^c}{\delta A^k_{ij}}
\label{gradcameq}
\end{equation}

\noindent Here, $c$ represents the index of the class we are interested in, and $y^c$ the element of the output which corresponds to this class. $A$ is the feature map we want to calculate gradients for, and might for example be the output of the final convolutional layer. $i$ and $j$ index the height and width of this feature map, while $k$ indexes the channels of this feature map. $Z$ is equal to the number of elements in each channel of the feature map, and simply normalizes the sum. Thus, we are actually just performing global average pooling of the gradients of each feature map channel $A^k$ with respect to $y^c$, which gives us a single value we can use as the weight for each channel. Doing this for feature map channels for a specific class gives us all the weights we need to calculate a weighted sum of the channels, which we can upsample and visualize to get an explanation for the decision of the \ac{cnn}.

Thus, \ac{gradcam} improves upon \ac{cam} by making the method less model dependent. However, the explanations are still the same low resolution, which may not be ideal in all cases.

\subsubsection{Guided Backpropagation} \label{section:guidebackpropagation}

\ac{gbp} \cite{gbp} is another \ac{xai} method which utilizes the gradients of the network to calculate saliencies. In this case, we do not stop at the final feature map, but backpropagate through the entire network to the input image. Simply backpropagating in this way produces a saliency map for the entire input image. However, \cite{gbp} finds that by only backpropagating positive gradients through \acs{relu} functions, they are able to produce "sharper visualizations of descriptive image regions than the previously known methods". Although the choice to simply neglect negative gradients when backpropagating lacks theoretical justification, \ac{gbp} has been shown to be accurate and trustworthy compared to other methods \cite{arras2022clevr, pianpanit2021parkinson}. Compared to the previous methods, \ac{gbp} differs in the sense that it produces a saliency value for every single input feature (every pixel, for example) as opposed to regions.

\subsubsection{Integrated Gradients} \label{section:integratedgradients}

Integrated Gradients \cite{integratedgradients} is another gradient based \ac{xai} method. The method was developed as part of an effort to create an \ac{xai} method which satisfies two axioms; sensitivity, and implementation invariance. Sensitivity states that if an input and a baseline differ in only one input feature, and have different outputs, then this input feature should have non-zero saliency. \cite{integratedgradients} shows that gradients violate this property because "the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline". Implementation invariance states that if two models are functionally identical (their outputs are equal for all inputs), then their attributions should be identical as well. Gradients satisfy this property, but methods which modify the gradients, such as \ac{gbp} and \ac{lrp} \cite{lrp}, break this axiom.

Both these axioms represent desirable qualities for an \ac{xai} method; we don't want our attribution method to miss any features which lead to a change in the model prediction, and we don't want the internal structure of the model to affect the attribution if it does not affect the output score in any way. Thus, it is problematic that there are few methods which satisfy both criteria. \cite{integratedgradients} finds that by integrating the gradients of the network in the straight line path between a baseline and the input, both these axioms are satisfied. Mathematically, the saliency for a specific input feature $\mathbf{x}_i$ is defined as follows, given an input $\mathbf{x}$, a baseline $\mathbf{x'}$ and a deep learning model $f$:

\begin{equation}
    \text{IntegratedGradients}_i(\mathbf{x}) := (\mathbf{x}_i - \mathbf{x}_i') \times \int_{\alpha = 0}^1 \frac{\delta f(\mathbf{x}' + \alpha \times (\mathbf{x} - \mathbf{x}'))}{\delta \mathbf{x}_i}d\alpha
\end{equation}

\noindent By cumulating the gradients of the network at all points between the baseline and the input, \cite{integratedgradients} manages to combine the implementation invariance of gradients with the sensitivity of techniques like \ac{lrp}. In addition, they prove that if the model $f$ is differentiable "almost everywhere", then the sum of all attributions is equal to the difference in output between the baseline and the input:

\begin{equation}
    \sum_{i=1}^n \text{IntegratedGradients}_i(\mathbf{x}) = f(\mathbf{x}) - f(\mathbf{x'})
\end{equation}

\noindent If we then choose a baseline which has $f(\mathbf{x'}) \approx 0$, we see that integrated gradients is equivalent to distributing the output prediction amongst the input features, a very desirable interpretation for an \ac{xai} attribution method.

\subsubsection{Other methods}

A part from the methods mentioned above, there are many different \ac{xai} methods. Shapley values \cite{shapley} is a post-hoc model agnostic method which calculates the marginal contribution of each feature for the prediction by perturbing the input. \ac{gradcam}PlusPlus \cite{gradcamplusplus}, HiResCAM \cite{hirescam} and Guided \ac{gradcam} \cite{gradcam} are post-hoc model-dependent \ac{xai} methods which all build on the \ac{gradcam} architecture and modify the calculation of saliencies in ways that can be beneficial in specific scenarios. Partial Dependence plots \cite{pdp} and Accumulated Local Effects plots \cite{ale} are global, model agnostic methods which aggregate information over the entire dataset to give information about the relationships between features across many samples. %TODO more cites


\section{Out-of-Distribution Detection} \label{ood_intro}

This section discusses \ac{ood} detection, the field which attempts to tackle the second problem discussed in the introduction and which is the main focus of this thesis; that \ac{ml} models have significantly worse performance on \ac{ood} data points and will often "fail silently", making completely wrong predictions with apparent high confidence \cite{adversarial}. \ac{ood} detection is a developing field, and still in an initial stage \cite{ooddl}. In 2017, \cite{oodbaseline} proposed a baseline \ac{ood} detection method. This section will discuss this method and the methods which follow it.

\subsection{Motivation for Out-of-Distribution Detection}

When training a model using supervised learning, we implicitly use the "closed-world assumption", which means that we assume that test data will be drawn from the same distribution as the training data \cite{oodoverview}. However, when a model is deployed, the data we see may not obey this assumption. Without \ac{ood} detection, the model will behave in the exact same way when encountering \ac{ood} samples or in distribution (\ac{id}) samples, and may even claim to be highly confident in its prediction although the sample is far away from the distribution of the training data \cite[1]{energy}. In any system where models make high impact decisions, this is a huge problem. We do not want a medical \ac{ai} system to attempt to classify a rare disease that was not part of the training data, nor do we want an autonomous car to continue to drive on snowy dirt roads if its training data only contains the sunny streets of San Francisco. Thus, \ac{ood} detection methods are necessary, so that \ac{ood} samples can be caught before the model takes any (potentially catastrophic) action.

Intuitively, one might assume that distinguishing \ac{id} and \ac{ood} samples from each other can be solved by simple binary classification using a dataset of \ac{id} samples and one of \ac{ood} samples. Indeed, if one has sufficient amount of high quality \ac{ood} samples, this can be done. However, this can be difficult to obtain in practice \cite[15]{oodoverview}. A key problem is that \ac{ood} data is, almost by definition, data which we do not have at development and training time of a specific model. \ac{ood} data is unexpected and surprising data, which cannot be predicted beforehand; we cannot possibly predict all situations a self-driving car could end up in, nor account for every single disease in the world when developing a medicinal \ac{ai} system. Thus, we require more sophisticated methods of \ac{ood} detection, which catch \ac{ood} data points in a general manner and do not rely on explicit examples of \ac{ood} data. Indeed, the majority of methods developed in the field of \ac{ood} detection do not rely on such auxiliary datasets \cite{openood}. % TODO check this citation

\subsection{Semantic versus covariate shift}

The first distinction to make in \ac{ood} detection tasks is whether an \ac{ood} sample is \ac{ood} because of {\it semantic} or {\it covariate} shift. Semantic shift refers to samples with different classes than the ones the model is trained on. A picture of a giraffe would represent a semantic shift for a model trained to differentiate between different breeds of dogs, as a giraffe does not belong to any breed of dog. Covariate shift refers to samples which come from a different distribution while still belonging to one of the classes of the original data set. An image of a Beagle puppy could represent covariate shift for a dog breed classifier despite Beagle being one of the classes, if all \ac{id} images were of adult dogs. Likewise, an image of a dog in a dark room could represent covariate shift, if all the \ac{id} images were of dogs outside, in well lit conditions. Figure \ref{fig:semanticcovariate} shows this distinction visually. Here, we can easily see the difference between covariate and semantic shift; covariate shifted images come from the same classes as \ac{id} samples, while semantically shifted images come from completely unknown classes.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center, above=0.2cm]\textbf{In Distribution}\\\textbf{images}}] (id1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id1.jpg}};
    \node[SIR] (id2) [below=of id1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id2.jpg}};
    \node[SIR] (id3) [below=of id2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/id3.jpg}};

    \node[SIR, label={[align=center, above=0.2cm]\textbf{Covariate shifted}\\\textbf{images}}, right=of id1] (cov1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov1.jpg}};
    \node[SIR] (cov2) [below=of cov1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov2.jpg}};
    \node[SIR] (cov3) [below=of cov2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/cov3.jpg}};

    \node[SIR, label={[align=center, above=0.2cm]\textbf{Semantically shifted}\\\textbf{images}}, right=of cov1] (sem1) {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem1.jpg}};
    \node[SIR] (sem2) [below=of sem1] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem2.jpg}};
    \node[SIR] (sem3) [below=of sem2] {\includegraphics[width=0.25\textwidth, height=0.25\textwidth]{figure/semanticcovariate/sem3.jpg}};

    \begin{scope}[on background layer={color=id!50}]
        \fill (-2.5,2.15) rectangle (2.5,-12.1);
    \end{scope}

    \begin{scope}[on background layer={color=near!50}]
        \fill (2.5,2.15) rectangle (7.5,-12.1);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (7.5,2.15) rectangle (12.5,-12.1);
    \end{scope}

    \end{tikzpicture}

    \caption[Mean Saliency visual explanation]{Figure showing in-distribution, covariate shifted and semantically shifted images for an imagined dog breed classifier. Images are taken from the ImageNet dataset}
    \label{fig:semanticcovariate}

    \end{center}
\end{figure}

The detection of semantic shift, as opposed to covariate shift, is the main focus of most \ac{ood} detection tasks \cite{oodoverview}. In many applications, it is expected that the model should be able to generalize its prediction to covariate-shifted data, and therefore the focus is on detecting semantic shift. However, the field of medical image classification is one where detecting covariate shift is also important, as the model should only make predictions on data points which are very similar to its training data \cite{oodoverview}.

Given that the detection of semantic shift has been the main focus of most \ac{ood} literature, my work will primarily deal with semantic shift as well. Thus, unless otherwise specified, when I refer to \ac{ood} data points, I mean data points which are semantically shifted, i.e that come from another class than those the model has been trained on.

\subsection{Benchmarking}

The performance of an \ac{xai} is hard to quantify, because the quality of an explanation is not easily reduced to a number. For \ac{ood} detection, performance is much easier to measure, as the problem can be described as a binary classification problem, with \ac{ood} and \ac{id} samples as the positive and negative class, respectively. Thus, we can calculate many different metrics and compare methods against each other. For \ac{ood} methods, the two most common metrics to report are \ac{fpr95} and the \ac{auroc} (see section \ref{section:aurocfpr95}). It is common to use ImageNet or CIFAR as the \ac{id} dataset, and calculate \ac{fpr95} and \ac{auroc} on other datasets which contain no overlapping class labels. When selecting \ac{ood} datasets, it is common to differentiate between \textbf{Near-\ac{ood}} and \textbf{Far-\ac{ood}}. Far-\ac{ood} samples are samples which are drastically different from the \ac{id} samples, while Near-\ac{ood} samples only differ slightly. For a cat-versus-dog classifier, a tiger and a wolf would represent Near-\ac{ood} semantic shift, while a plane and a car would represent Far-\ac{ood} semantic shift. As one might expect, detecting Near-\ac{ood} samples is much harder than Far-\ac{ood} in most scenarios.

% FPR95 is, as the name implies, the number of false positives (i.e the number of ID data points that the method falsely believes to be \ac{ood}) when 95\% of the \ac{ood} data points are correctly detected. AUROC can defined as the chance that a random ID data point has a higher ID-score than a random \ac{ood} data point \cite{openood}. An AUROC of 0.5 is equivalent to random guessing, while an AUROC of 1 is a perfect model that catches all \ac{ood} data points.

In 2021, \cite{oodoverview} defined a generalized \ac{ood} detection framework, and in 2023 \cite{openood} introduced a comprehensive benchmark called OpenOOD, which evaluates all relevant \ac{ood} methods under this framework. Prior to this work, different methods were tested on different benchmarks, with different image preprocessing procedures, and with other externalities which inhibited effective comparison between methods \cite{openood}. In 2024 OpenOOD was further improved, with the addition of more benchmarks, more methods and the inclusion of vision transformer models \cite{openood15}. OpenOOD is "is the only work that comprehensively evaluates a wide range of OOD detection methods on multiple benchmarks of various sizes" \cite{openood15}, thus making it an obvious choice for this thesis.

OpenOOD includes 11 different benchmarks across Anomaly Detection, Open Set Recognition and \ac{ood} detection, three fields which are very closely related. Of these, 4 benchmarks are used for standard \ac{ood} detection, which are the ones I will be concerned with. Each benchmark is defined by an \ac{id} dataset, with 5 or more corresponding \ac{ood} datasets, separated into Near-\ac{ood} and Far-\ac{ood}. For each benchmark, \ac{auroc}, \ac{aupr} and \ac{fpr95} is reported over all \ac{ood} datasets, and Near-\ac{ood} \ac{auroc} is used to rank methods against each other.

\subsection{Methods}

This section will follow the same outline as section \ref{chapter:xai}; firstly, the overarching categories of methods will be discussed, followed by a more detailed look at a selection of specific methods within the field.
\\

The field of \ac{ood} is separated into four categories of methods \cite{oodoverview}:

\begin{itemize}
  \item Classification-based methods
  \item Density-based methods
  \item Distance-based methods
  \item Reconstruction-based methods
\end{itemize}

All methods can also be categorized by whether they are post-hoc or training based. Post-hoc methods take an already trained network and attempt to extract information which separates \ac{id} and \ac{ood} samples out of the network during inference. These methods have the obvious advantage that they can work out of the box with large pre-trained network without requiring expensive training from scratch. Training based methods train the network in ways which maximize the difference between \ac{id} and \ac{ood} samples. These methods do not necessarily require \ac{ood} samples, but can train using auxiliary loss functions which amplify the differences in network behaviour when faced with \ac{ood} data as opposed to \ac{id}. Regardless, these methods come with a much higher computational requirement than post-hoc methods, as they require training from scratch or at least retraining using the new loss criterion. Given the fact that post-hoc methods can be applied to trained networks out of the box, it is quite common to combine both post-hoc and training strategies to achieve the best performance.

Below follows a short explanation of each the four categories mentioned above.

\subsubsection{Classification-based methods}

Classification-based methods usually use the softmax score or logits of a model to attempt to distinguish \ac{ood} and \ac{id} samples. \cite{oodbaseline} made the observation that while the softmax score may be a poor indication of the actual confidence of the model on a single data point, it is still higher on average for \ac{id} samples as opposed to \ac{ood} samples. By using this simple distinction, they created a baseline model which separated \ac{ood} and \ac{id} samples. Using input perturbations and temperature scaling, \cite{odin} further improved on this method, by amplifying the difference in softmax score of \ac{id} and \ac{ood} data. 

More generally, classification-based methods do not need to use the softmax score, but may attempt to find any metric which separates the distribution of \ac{id} samples from \ac{ood} samples. Figure \ref{fig:ood_metric} shows the probability density for an unspecified metric for both \ac{ood} and \ac{id} samples. The goal of classification-based \ac{ood} detection is to find metrics or training methods which make these probability densities have as little overlap as possible, such that they are easily separated by a threshold. The threshold, often denoted as $\delta$, is a parameter that needs to be set at a specific value based on the values of a validation \ac{id} and validation \ac{ood} data set. Often, one sets the $\delta$ such that a certain percentage of \ac{ood} samples in the validation set are correctly detected, for example 95\%.

\begin{figure}[h]
    \begin{center}
        \input{figure/figure.pgf}
    \end{center}
    \caption[Hypothetical \ac{id}/\ac{ood} distributions for an \ac{ood} detection metric]{Graph showing the distribution of hypothetical \ac{id}, Near-\ac{ood} and Far-\ac{ood} data for an unspecified metric. The shaded region shows the overlap between the \ac{id} and \ac{ood} samples.}
    \label{fig:ood_metric}
\end{figure}

% There are several \ac{sota} methods which utilize a classification-based approach, and these make up a large part of the representative methodologies for \ac{ood} detection today \cite[8]{oodoverview}. As such, I shall devote the majority of section \ref{ood_specific} to classification-based methods.



\subsubsection{Density-based methods}

Density-based methods explicitly try to model the in-distribution, which is then used to detect outliers in low likelihood regions. Although the idea is intuitive, learning the distribution of the data set can often be prohibitively expensive, and thus these methods often lag behind classification-based methods \cite{oodoverview}.
\\

\subsubsection{Distance-based methods} \label{section:distancebasedood}

Distance-based methods attempt to detect \ac{ood} samples by calculating their distance to \ac{id} samples. Many different distance measures are used, such as the Mahalanobis distance to estimated Gaussian distributions of the \ac{id} classes, cosine distance to the first singular vector of the \ac{id} dataset or the Euclidean distance in an embedding space.
\\

\subsubsection{Reconstruction-based methods}

Reconstruction-based methods are based on encoder-decoder frameworks, where the core idea is that the model will be much worse at reconstructing \ac{ood} data than \ac{id}. By measuring the reconstruction loss, we can detect \ac{ood} samples.
\\

\subsection{Specific methods} \label{ood_specific}

Like in section \ref{section:xaimethodsbackground}, I here describe a selection of \ac{ood} detection algorithms which have special relevance for the methods introduced in chapter \ref{chapter:methodology}. These four methods are \acf{msp}, \acf{mls}, \acf{vim} and \texttt{COMBOOD}. After this, I give a short overview of methods which are \ac{sota} in the field.

\subsubsection{Baseline models} \label{section:background_baselines}

The baseline model created by \cite{oodbaseline} is extremely simple, yet effective. It simply compares the softmax score of the predicted class to a threshold, and labels it as \ac{ood} if it falls below this threshold. Let us assume we have a model $f: \bm{x} \rightarrow \R^C$ that takes an input $\bm{x}$ (which may be an image, a vector of values, or something else) and returns a vector of logits with length equal to the number of classes $C$. If we define a softmax score function 

\begin{equation}
    S_i(\bm{x}) = \frac{\text{exp} \, (f_i(\bm{x}))}{\sum^N_{j=1} \text{exp} \, (f_j(\bm{x}))},
\label{softmax}
\end{equation}

\noindent then the \ac{ood} detector has the following simple form, given a threshold $\delta$:

\begin{align}
\label{eq:msp}
    g(\bm{x}; \delta)=\begin{cases} 
        \text{in } & \max_i S(\bm{x})\ge \delta \\
        \text{out} & \max_i S(\bm{x}) < \delta 
   \end{cases},
\end{align}

\noindent As explained previously, the $\delta$ must be set by the user of the system based on results from a validation set of \ac{id} and \ac{ood} data.

This method reasonably well, because the softmax scores for \ac{id} data generally is higher than for \ac{ood} data. Two years later, \cite{mls} showed that this baseline could be improved in settings with larger datasets by forgoing the softmax normalization and instead only looking at the maximum logit, with the even simpler form

\begin{align}
\label{eq:mls}
    g(\bm{x}; \delta)=\begin{cases} 
        \text{in } & \max_i f(\bm{x})\ge \delta \\
        \text{out} & \max_i f(\bm{x}) < \delta 
   \end{cases},
\end{align}

\noindent Perhaps surprisingly, both these methods are quite good at detecting \ac{ood} samples. In fact, when combined with outlier exposure, \ac{msp} is \ac{sota} on several benchmarks.

\subsubsection{Virtual Logit Matching (VIM)} \label{section:vim}

\cite{vim} attempts to improve \ac{ood} detection by calculating a score based on the feature, the logit and the softmax probability at once, as opposed to just one of them. By looking at all three elements in conjunction, they see an increase in performance over models which only rely on a single input source.

The reasoning behind not just looking at the logits or softmax probability is that there is a lot of information that is lost when going from features to logits \cite{vim}. Once we project the features down to logits, we have only class dependent information, and have lost the class agnostic information which is contained within the features. To show how this information is lost, the authors give an example based on null space analysis \cite{nusa}:

Let us assume that we have a simplified network with only a single layer. Then, we have $\hat{\bm{y}} = W \bm{x}$, where $\hat{\bm{y}}$ is the vector containing the logits, $\bm{x}$ is the feature vector of the input (with an additional 1 for the bias term) and $W$ is the matrix containing the weights and biases transforming the feature vector into logits. A null space $\text{Null}(W)$ of a matrix $W$ is the set of all vectors that map to the zero vector, such that $W \bm{a} = \bm{0} \iff \bm{a} \in \text{Null}(W)$. The null space of a matrix may be trivial (empty), but a matrix which projects vectors to a lower dimension will have a non-trivial null space. Given that the final layer of a neural network projects down to logits, which are the same dimension as the number of classes, this will almost always be the case. Because of the distributivity of matrix multiplication, we have the following:

\begin{equation}
W (\bm{x} + \bm{a}) = W \bm{x} + W \bm{a} = W \bm{x} + \bm{0} = W \bm{x}
\label{matrix}
\end{equation}

\noindent The vector $\bm{x}$ can be decomposed into $\bm{x}^W + \bm{x}^{\text{Null}(W)}$, where $\bm{x}^W$ is the projection of $\bm{x}$ onto the column space of $W$ and $\bm{x}^{\text{Null}(W)}$ is the projection of $\bm{x}$ onto the null space of $W$. It follows from this and equation \ref{matrix} that when going from features to logits using the projection $W \bm{x}$, we lose all information contained in $\bm{x}^{\text{Null}(W)}$. \cite{nusa} shows how this can be exploited by adversarial methods, by creating images with added noise derived from the null space of a matrix within the network, which are classified as if the noise was not present, despite having no resemblance to the original image. Figure \ref{dog} shows an example of this.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={Original Image}] (orig) {\includegraphics[width=0.25\textwidth]{figure/OrigImage.png}};
    \node[SIR, label={Additive null space noise}] (noise) [right=of orig] {\includegraphics[width=0.25\textwidth]{figure/PureNoise.png}};
    \node[SIR, label={Image+Noise}] (final) [right=of noise] {\includegraphics[width=0.25\textwidth]{figure/NoiseAdded.png}};

    \end{tikzpicture}
    \caption{Figure showing the findings of \cite{nusa} visually. By sampling null space noise from the network, we can create an image (the image to the right) which is completely distorted, but which is indistinguishable for the network and given the exact same prediction as if the noise was not present. Original image taken from the ImageNet dataset.}
    \label{dog}
    \end{center}
\end{figure}


From this, we can see that potentially large amounts of information can be lost when going from features to logits. Using this information, it is also possible to perform \ac{ood} detection, as shown by \cite{nusa}. Another method which uses the features performs \ac{pca} and looks at the residual information lost when using the first $N$ principal components \cite{subspace}. However, the information in the features is still class agnostic, and \cite{vim} aims to go beyond using just one input source and combine several elements of the network.

To do this, they propose using a {\it Virtual Logit}. The Virtual Logit is calculated as follows: First, they center the feature space, so that "it is bias free in the computation of logits" \cite{vim}. They then perform \ac{pca} as in \cite{subspace}, and calculate the residual of $\bm{x}$ with regards to the principal components, which is the projection $\bm{x}$ onto the null space of the principal subspace $P$. The residual represents the information lost when using the projection $P$.

\begin{equation}
\text{Residual}(x) = || \bm{x}^{\text{Null}(P)}||
\label{virtuallogit}
\end{equation}

\noindent This value is scaled based on the average values of the maximum logit across the dataset, and is appended to the rest of the logits as a Virtual Logit:

\begin{equation}
l_0 := \alpha || \bm{x}^{\text{Null}(P)}||
\label{virtuallogit}
\end{equation}

\noindent This now takes part in the computation of the softmax values, and thus is affected by the size of the rest of the logits. They call the softmax value of the Virtual Logit the {\it ViM score}. In this way, the ViM score represents the size of the residual in comparison with the predictions of the model. If the model is very confident, then the norm of the residual will be small in comparison, and the ViM score will be low. If the residual is very large, the ViM score will be higher, and more indicative of an \ac{ood} sample. In this way, \cite{vim} have combined information from the feature, the logit and the softmax probability level to perform \ac{ood} detection.

\subsubsection{COMBOOD}

\texttt{COMBOOD} \cite{combood} is another \ac{ood} detection method which combines information from different sources to increase performance. Unlike \ac{vim}, which combines different internal signals from the network, \texttt{COMBOOD} combines information from two different metrics calculated from the feature space. The two metrics are Mahalanobis distance and nearest neighbour distance, which have both seen decent performance on their own \cite{nearestneighbour, mahalanobis}. \cite{combood} builds on these works by showing that their combination into one single score can increase performance far above either one. Indeed, the performance of \texttt{COMBOOD} is \acl{sota}, being the highest performing \ac{ood} detector on the ImageNet200 and ImageNet1K benchmarks in the OpenOOD framework.\footnote{https://zjysteven.github.io/OpenOOD/}

To understand how \texttt{COMBOOD} works, we must first understand how Mahalanobis and nearest neighbour distance work as \ac{ood} detectors. Mahalanobis distance assumes that the features generated by the network are distributed as a multivariate Gaussian, and calculates the distance based on the mean and standard deviations of this Gaussian. This method is a generalization of calculating the Z-score for a univariate Gaussian, and has the following form, given a mean $\bm{\mu}$ and covariance matrix $\Sigma$, calculated over the \ac{id} training set:

\begin{equation}
    \text{Mahalanobis}(\bm{x}) = \sqrt{(\bm{x} - \bm{\mu})^T\Sigma^{-1}(\bm{x} - \bm{\mu})}
\end{equation}

\noindent Nearest neighbour distance has the advantage that it does not impose any assumptions about the distribution of the feature space, and is thus non-parametric. The k-nearest neighbour distance, is simply the distance from a given datapoint to the k'th nearest neighbour in feature space. \cite{nearestneighbour}, which used this distance metric for \ac{ood} detection, normalized the features and used Euclidean distance as the distance metric, giving us the following equation:

\begin{equation}
    \text{NN}(\bm{x}) = \| \bm{x^*} - \bm{z}_k \|_2,
\end{equation}

where $\bm{x^*}$ is the normalized feature $\bm{x}$ and $\bm{z}_k$ is the k'th nearest neighbour from the \ac{id} training set.

\texttt{COMBOOD} combines these metrics by computing "confidence scores" for each distance, which are then added together to produce a final score. In addition, they find that by using different feature extracting methods for each of the different methods, their combined performance can be enhanced considerably, concluding that "COMBOOD performs best when the nonparametric and the parametric components use different feature extraction strategies, penultimate layer embeddings for the former and global extrema of the features for the latter" \cite{combood}.

\subsubsection{Other \acl{sota} methods}

As mentioned previously, \ac{ood} detection is a developing field, with no method which definitely defeats all others across different benchmarks. I will now describe a short selection of methods which performs well on one or more benchmarks. AdaSCALE \cite{adascale} is a scaling method which scales the activations of a network based on the \ac{ood} likelihood as calculated by perturbation and achieves \ac{sota} results on the ImageNet200 and ImageNet1K benchmarks. ASH \cite{ash} creates a simplified representation of a specific layer and feeds it through the network, and uses the energy score of the modified prediction for \ac{ood} detection. Augmentation based methods such as RotPred \cite{rotpred} and PixMix \cite{pixmix} have also been used in conjunction with other detectors to achieve \ac{sota} results on CIFAR10. Energy based \ac{ood} detection \cite{energy}, which uses an energy score as opposed to the \ac{msp} also achieves high results on both CIFAR10 and CIFAR100. The methods mentioned above, such as \ac{msp} and \texttt{COMBOOD}, are also \ac{sota} within the field of \ac{ood} detection.

\section{Related work} \label{section:relatedwork}

In a broad sense, this work is about \ac{ood} detection. In this way, the field of \ac{ood} detection, and methods described in the preceding sections, can be thought of as constituting the related work. However, more specifically, I attempt to use \ac{xai} methods to enhance \ac{ood} detection performance, and thus we may look towards previous work which has attempted a similar combination.

While the combination of \ac{xai} and \ac{ood} detection has been explored in many previous works, the majority of them focus on explaining why a data point was marked as \ac{ood}, as opposed to using \ac{xai} to aid the detection itself. \cite{uncertainty}, \cite{generalxaiforood} and \cite{tallon2020explainable} are papers which use \ac{xai} methods to explain \ac{ood} detection decisions. Within network security, \ac{xai} has been as part of anomaly detection systems to detect malicious or faulty network traffic. Here, it has been used to explain detections \cite{idsxai, mahbooba}, but also to aid in detection itself by inspecting the explanations of the detection system \cite{tcydenova2021detection, dnsxai}. These methods thus use \ac{xai} to aid \ac{ood} detection in a similar manner to my work, however they are strictly focused on sequential network traffic data as opposed to images, and are mostly concerned with detection "unnatural" data samples such as intentionally malicious traffic or that generated by faulty equipment, as opposed to natural \ac{ood} data caused by semantic or covariate shift occurring when a model is deployed.

\cite{martinez} is the most relevant previous work. Here, the authors explicitly aim to use \ac{xai} to improve \ac{ood} detection on images. They do this by looking at saliency maps produced by a \ac{gradcam}-based \ac{xai} method (section \ref{section:gradcam}) during inference, i.e the heatmaps that explain which parts of the image was most influential to classify the image as a specific class. Using these heatmaps, they perform distance-based \ac{ood} detection (section \ref{section:distancebasedood}): By collecting all explanations for each image in the \ac{id} dataset, they are able to construct archetypical explanations, and can make clusters of explanations. To perform \ac{ood} detection, they simply compare the explanation of a new data point to the clusters of archetypical explanations, and mark it as \ac{ood} if it has a distance which is over a certain threshold.

This method performs decently on toy benchmarks, achieving scores similar to \ac{sota} methods when using {\it Fashion MNIST} as \ac{id} and {\it MNIST} as \ac{ood}. However, this method fails catastrophically in more complicated scenarios, achieving an \ac{auroc} score of only $52\%$ on {\it CIFAR10} vs {\it SVHN}, which is no better than pure guessing and far below any other method. The paper thus ends with the authors concluding that "OoD detection approaches that are specifically designed for the purpose achieve in general better detection scores at the cost of an additional computational burden in the models construction" \cite{martinez}.

For more potential related work, we can look to OpenOOD \cite{openood, openood15}, which aims to provide a comprehensive benchmark of all relevant methods in the field of \ac{ood} detection. Out of all 41 \ac{ood} detection methods included in this benchmark, there are no methods which use \ac{xai}. However, as many \ac{xai} methods utilize the gradients of the network to generate saliency values, we could also consider \ac{ood} detection models which utilize gradients in some form as tangentially related to this thesis. In this regard GradNorm \cite{gradnorm} is related, as they utilize the norm of the gradients of the network with respect to the Kullback-Leibler distance between the outputs and a uniform distribution to perform \ac{ood} detection.

From the absence of any relevant method utilizing \ac{xai} in OpenOOD and from the poor results of \cite{martinez}, we can see that the potential for a truly effective \ac{ood} detection system using \ac{xai} has not been fully realized in any previous work.

\section{Summary}

In this chapter, I have given a short introduction to machine learning in general, as well as thorough introductions to the fields of \ac{xai} and \ac{ood} detection. Specifically, I have also given detailed descriptions of the \ac{xai} and \ac{ood} detection methods which are relevant to the development of the three frameworks I will introduce in the next section.

\chapter{Methodology} \label{chapter:methodology}

As shown in the preceding chapter, there exists a wide range of \ac{xai} methods. These methods exploit gradient information, differences in output scores when altering model inputs, marginal contributions of input features, as well as many other intricacies of deep learning models. The core idea of this master's thesis is that these methods, in their attempt to explain a model decision, may also inadvertently extract information which is valuable for \ac{ood} detection. Thus, there may be an unexplored potential in these methods to function not just as explanations, but also as classifiers which allow us to separate \ac{id} and \ac{ood} data. Intuitively, we might expect the explanations to be more spread out on \ac{ood} images, given that there are (by definition) no objects of interest in the image that the model can definitely be said to focus on. In contrast, we might expect an explanation on an \ac{id} image to be more focused on a specific area, which contains an object of interest. Furthermore, given that saliency methods give a numerical value to each region of the image, we might be able to extract information about the "\ac{ood}-ness" of an image by inspecting the magnitudes of these values. Intuitively, it may be the case that such values should be lower for \ac{ood} than \ac{id}, reflecting the higher uncertainty present in \ac{ood} data.

% As an example, let us consider \ac{gradcam}. As explained in section \ref{section:gradcam}, \ac{gradcam} uses the gradients of the weights within the final convolutional layer with regard to the output prediction, along with the final feature maps, to generate an explanation.

\section{Proposed \ac{xai} frameworks for \ac{ood} detection} \label{section:methods}

With the preceding intuitions in mind, I present three frameworks for \ac{ood} detection which utilize \ac{xai} methods as part of their detection pipeline.

\subsection{Stand-alone saliency framework: Saliency Aggregation} \label{section:saliencyagg_method}

As we have seen from section \ref{section:relatedwork}, there has been little research into using explanations for \ac{ood} detection, aside from the work of \cite{martinez}. Thus, I begin by presenting a simple framework which uses saliency values generated by \ac{xai} methods to calculate an \ac{ood} score.

As mentioned previously, the field of \ac{ood} detection started with the simple baseline introduced by \cite{oodbaseline}, which simply uses the \ac{msp} (i.e the confidence score of the predicted class) as a way to measure \ac{ood}. Later, it was shown that using the \ac{mls} could also serve as an effective baseline. As such, I propose a similarly simple baseline when using explanations for \ac{ood} detection. The analogue of the maximum logit in an \ac{xai} context can reasonably be said to be the explanation generated of the predicted class (the class corresponding to the maximum logit). As explained previously, this explanation will take the form of an $N \times M$ saliency map. This saliency map is not a single scalar value, and does thus not make a suitable \ac{ood} score by itself. Instead, we may perform some form of aggregation on the saliency map (such as taking the mean, the vector norm, the variance or some other metric), and use this as the \ac{ood} score.

The intuition for this method is informed by the fact that there are many forms of aggregation over saliencies which one might reasonably expect to be different for \ac{id} and \ac{ood} data. As an example, let us consider the implications of aggregating in some way which captures the magnitude of the saliencies, such as the {\it mean}, the {\it vector norm}, or the {\it max value}. When we generate a saliency map using the predicted class, the \ac{xai} saliency method attempts to calculate a measure of importance for each region of the input image, with regard to this class. For \ac{id} data, as long as the model predicted correctly, we know that there really are regions of the input image which contain the predicted class. If we instead are looking at semantically shifted \ac{ood} data, we know that no input image contains any of the \ac{id} classes. Thus, when a neural network makes a prediction on such a data point, it will always be wrong, because it will always predict one of the \ac{id} classes. By generating a saliency map of this prediction, we are asking a method to decide how each region contributed to a false decision. Given that there are no objects of the predicted class, in any region, we may reasonably assume that the saliency values are very different than in an \ac{id} case, where such objects actually are present.

\subsubsection{Saliency Aggregation based on magnitude}

The first intuition which motivates the Saliency Aggregation framework is the idea that the magnitudes of \ac{xai} saliencies generated from \ac{id} and \ac{ood} data may be different. In this section, I will present two hypothetical scenarios which motivate why this might be the case. First, I present a simple example scenario (figure \ref{fig:meansaliency}) where one might expect the \ac{id} saliencies to be higher than the \ac{ood} saliencies. Here, I imagine a model which has been trained to differentiate between different breeds of dogs. In the first case, it is given an image of a dog, and a prediction of "English Foxhound" is made, which happens to be correct. Generating an explanation for this prediction, each region of the image is given a measure of importance, calculated using gradient information, differences in prediction score on counterfactual examples or by other means. As there actually is an English Foxhound in the image, we may expect that these methods generate saliencies which have a magnitude which is higher than if there was no dog present.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]Image with dog present,\\correctly classified as\\"English Foxhound"}] (idimg) {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency0.png}};
    \node[SIR, label={[align=center]Image without dog\\present, wrongly\\classified as "Samyoed"}] (oodimg) [right=of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency2.png}};

    \node[SIR] (idsal) [below=1.5cm of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency1.png}};
    \node[SIR] (oodsal) [below=1.5cm of oodimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliency/meansaliency3.png}};

    \node[SIR] (idmean) [below=1.5cm of idsal] {Mean saliency = $0.152$};
    \node[SIR] (oodmean) [below=1.5cm of oodsal] {Mean saliency = $0.034$};

    \draw[->, very thick] (idimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (idsal.north);
    \draw[->, very thick] (oodimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (oodsal.north);

    \draw[->, very thick] (idsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (idmean.north);
    \draw[->, very thick] (oodsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (oodmean.north);

    \node[SIR, align=left, draw=id, fill=id!30] (iddecision) [below=of idmean] {Above threshold:\\classified as \ac{id}};
    \node[SIR, align=left, draw=far, fill=far!30] (ooddecision) [below=of oodmean] {Below threshold:\\classified as \ac{ood}};

    \draw[->, very thick] (idmean.south)  to node[midway, fill=white] {Compare with threshold} (iddecision.north);
    \draw[->, very thick] (oodmean.south)  to node[midway, fill=white] {Compare with threshold} (ooddecision.north);

    \begin{scope}[shift={($(oodsal.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=0.345,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.173, 0.345},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}
    \end{tikzpicture}
    \caption[Mean Saliency visual explanation]{Figure showing the functioning of the Aggregate of Saliency \ac{ood} detector, using mean as the aggregation, in a hypothetical scenario where a model trained on images of dogs is shown an image with no dogs present. The heatmaps here show the maximum value of both saliency maps as dark red, reflecting the lack of normalization}
    \label{fig:meansaliency}
    \end{center}
\end{figure}

In the second case, the model is given an image without any dogs present. Given that there is no class for images without dogs in them, the model will classify the image as one of the possible dog breeds. In this case, the model predicted the class of "Samoyed", a decision which can be considered essentially arbitrary. When a explanation is generated for this decision, the methods for calculating importance scores will most likely assign saliencies to most regions, given that no region contains a dog. As such, if we calculate mean saliencies for both the image with the dog and the one without, we expect the image with the dog to have a higher mean saliency. As long as we work with semantically shifted \ac{ood} data, it will always be the case that the prediction on \ac{ood} data is wrong, and thus we may also expect that the generated explanations in general output smaller saliencies.

On the contrary, we could also expect that \ac{ood} saliency maps have {\it higher} magnitudes than \ac{id} saliency maps. As has been well documented, neural networks behave unpredictably when exposed to examples far from their training distribution \cite{nguyen2015deepneuralnetworkseasily, geirhos2020shortcut, goodfellow2015explainingharnessingadversarialexamples}. Thus, it is not unreasonable to expect that explanations based on this unpredictable behaviour may also be unstable and unpredictable. This instability could lead to large outliers in saliency maps, which would give larger magnitudes when compared to \ac{id} data points. This lead to a second hypothetical scenario. Figure \ref{fig:meansaliencyless} shows such a scenario. Here, we imagine that the same dog breed classifier is shown two images: the first; an image of a Border Terrier in a park, a scenario which we could assume is quite common in the training dataset. The second is a night-time picture of a illuminated archway, an image which is very far away from the training distribution of the model, and contains many sharp changes in pixel intensity. In such a scenario, it is possible that a network will behave unpredictably, which may be reflected in the saliency map of the model. As explained in section \ref{section:xaimethodsbackground}, methods such as \ac{gradcam}, \ac{gbp} and integrated gradients use gradient information from the network, which may be affected by sharp activations in the internals of the model.


\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]Image with dog present,\\correctly classified as\\"Border Terrier"}] (idimg) {\includegraphics[width=0.25\textwidth]{figure/meansaliencyless/id_less_magnitude-img0.png}};
    \node[SIR, label={[align=center]Image without dog\\present, wrongly\\classified as "Dingo"}] (oodimg) [right=of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliencyless/id_less_magnitude-img2}};

    \node[SIR] (idsal) [below=1.5cm of idimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliencyless/id_less_magnitude-img1.png}};
    \node[SIR] (oodsal) [below=1.5cm of oodimg] {\includegraphics[width=0.25\textwidth]{figure/meansaliencyless/id_less_magnitude-img3.png}};

    \node[SIR] (idmean) [below=1.5cm of idsal] {Mean saliency = $0.23$};
    \node[SIR] (oodmean) [below=1.5cm of oodsal] {Mean saliency = $0.69$};

    \draw[->, very thick] (idimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (idsal.north);
    \draw[->, very thick] (oodimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (oodsal.north);

    \draw[->, very thick] (idsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (idmean.north);
    \draw[->, very thick] (oodsal.south)  to node[midway, fill=white, align=center] {Calculate average value\\of heatmap} (oodmean.north);

    \node[SIR, align=left, draw=id, fill=id!30] (iddecision) [below=of idmean] {Below threshold:\\classified as \ac{id}};
    \node[SIR, align=left, draw=far, fill=far!30] (ooddecision) [below=of oodmean] {Above threshold:\\classified as \ac{ood}};

    \draw[->, very thick] (idmean.south)  to node[midway, fill=white] {Compare with threshold} (iddecision.north);
    \draw[->, very thick] (oodmean.south)  to node[midway, fill=white] {Compare with threshold} (ooddecision.north);

    \begin{scope}[shift={($(oodsal.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=0.82,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.41, 0.82},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}
    \end{tikzpicture}
    \caption[Mean Saliency visual explanation]{Figure showing the functioning of the Aggregate of Saliency \ac{ood} detector, using mean as the aggregation, in a hypothetical scenario where a model trained on images of dogs is shown an image with no dogs present. As opposed to the previous figure, here we assume that \ac{id} samples have lower mean values on average, and as such the thresholding is inverted.}
    \label{fig:meansaliencyless}
    \end{center}
\end{figure}

\subsubsection{Saliency Aggregation based on Statistical Dispersion}

Apart from aggregations which convey information about the magnitude of the saliencies, we may also expect the variation or dispersion of the saliencies to be different between \ac{id} and \ac{ood} data. We might expect the heatmap on \ac{ood} data to be less concentrated and more evenly spread out, given that there are no actual objects of interest present. This would give \ac{ood} data points a low variance, and \ac{id} data points a high variance. Alternatively, we might expect \ac{ood} saliency maps to have large outliers, which could give \ac{ood} saliency maps high variance.

Figure \ref{fig:varsaliency} shows a hypothetical scenario in which calculating the spread could be beneficial in \ac{ood} detection. Like in the previous case, we imagine a model trained on dogs, which is fed two images, one which contains a dog and one which does not. In this case, we do not care about the magnitudes, but instead only the spread of the values in relation to each other, and thus the heatmap is normalized. As we can see, in this case the heatmap is more spread out in the explanation where there is no dog present than for the image with a dog. By calculating a suitable metric, such as the \ac{rmd}, \ac{cv} or another measure of statistical dispersion, we can get a single number which represents how spread out the heatmap is, regardless of its magnitude. Using these values, we can define a threshold which is below most \ac{id} data and above most \ac{ood} data, allowing us to separate these distributions.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, label={[align=center]Image with dog present,\\correctly classified as\\"Golden Retriever"}] (idimg) {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency0.png}};
    \node[SIR, label={[align=center]Image without dog\\present, wrongly\\classified as "Dingo"}] (oodimg) [right=of idimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency2.png}};

    \node[SIR] (idsal) [below=1.5cm of idimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency1.png}};
    \node[SIR] (oodsal) [below=1.5cm of oodimg] {\includegraphics[width=0.25\textwidth]{figure/varsaliency/varsaliency3.png}};

    \node[SIR] (idmean) [below=1.5cm of idsal] {RMD = 0.6};
    \node[SIR] (oodmean) [below=1.5cm of oodsal] {RMD = 0.2};

    \draw[->, very thick] (idimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (idsal.north);
    \draw[->, very thick] (oodimg.south)  to node[midway, fill=white, align=center] {Generate \ac{xai} heatmap\\of model prediction} (oodsal.north);

    \draw[->, very thick] (idsal.south)  to node[midway, fill=white, align=center] {Calculate variance\\of normalized heatmap} (idmean.north);
    \draw[->, very thick] (oodsal.south)  to node[midway, fill=white, align=center] {Calculate variance\\of normalized heatmap} (oodmean.north);

    \node[SIR, align=left, draw=id, fill=id!30] (iddecision) [below=of idmean] {Above threshold:\\classified as \ac{id}};
    \node[SIR, align=left, draw=far, fill=far!30] (ooddecision) [below=of oodmean] {Below threshold:\\classified as \ac{ood}};

    \draw[->, very thick] (idmean.south)  to node[midway, fill=white] {Compare with threshold} (iddecision.north);
    \draw[->, very thick] (oodmean.south)  to node[midway, fill=white] {Compare with threshold} (ooddecision.north);

    \begin{scope}[shift={($(oodsal.east)+(0.1cm,0.25\textwidth / 2)$)}]
        \pgfplotscolorbardrawstandalone[
        colormap name=turbo,
        point meta min=0,
        point meta max=1,
        colorbar style={
            height=0.25\textwidth, % Match the height of the images
            width=0.4cm, % Desired width of the colorbar
            ytick={0, 0.5, 1},
            % yticklabels={Low, Medium, High},
        }]
    \end{scope}

    \end{tikzpicture}

    \caption[Spread of Saliency visual explanation]{Figure showing the functioning of the Spread of Saliency \ac{ood} detector in a hypothetical scenario where a model trained on images of dogs is shown an image with no dogs present. The heatmaps are here based on the normalized values of each saliency map, meaning that magnitude information is ignored}
    \label{fig:varsaliency}
    \end{center}
\end{figure}

On the contrary, we may expect the spread of saliencies to be higher for \ac{ood} data, if the saliency mapping methods are unstable and lack robustness. In such a scenario, we could expect that the saliencies outputted on \ac{ood} data could be highly varied, while the saliencies of \ac{id} data is more stable.

\subsubsection{Saliency Aggregation \ac{ood} detection framework: formal definition}

After having introduced some motivating intuitions, we may formalize the framework. To define this detection framework mathematically, let us first define the necessary components. As in chapter \ref{chapter:background}, we assume we have a model $f: \bm{x} \to \R^C$. In this case, $\bm{x} \in \R^{D \times H \times W}$, i.e a $D$ channel image of height $H$ and width $W$. In addition, we define a general \ac{xai} saliency mapping method $s: (f, \bm{x}) \to \R^{N \times M}$. This function takes the model $f$ and an input $\bm{x}$ and returns a $N$ by $M$ saliency map for the predicted class, i.e. the class corresponding to the highest logit. We also define a general aggregation function $A: \bm{x} \rightarrow \R$, where $\bm{x}$ can be of any shape. Given the fact that we do not know whether the saliency aggregation of a specific \ac{xai} method $s$ and a specific model $f$ will be larger or smaller for \ac{id} data, we must also decide whether larger or smaller values should be considered \ac{id}. To do this, we can calculate the mean value of a given saliency method and aggregation over a validation \ac{id} and validation \ac{ood} dataset, and compare their values. Requiring the presence of a validation set does not impose any actual limitations on the method, as a validation set is required by all \ac{ood} detection methods to be able to set the threshold $\delta$. If we denote the mean value of the aggregation over the \ac{id} validation dataset as $\mu_{id}$, and over the \ac{ood} dataset as $\mu_{ood}$, we can use $\text{sign}(\mu_{id} - \mu_{ood})$ to multiply the \ac{ood} detection score by $1$ or $-1$, respectively. This ensures that the \ac{ood} detector follows the convention of the \ac{ood} detection field, which is that \ac{id} samples should have higher scores than \ac{ood} samples on a given \ac{ood} detection metric.

Thus, the \ac{ood} detector has the following form, given a threshold $\delta$:

\begin{align} \label{eq:salagg_def}
    g(\bm{x}; s, A, \delta)=\begin{cases} 
        \text{in } & \text{sign}(\mu_{id} - \mu_{ood}) \cdot A(s(\bm{x}, f)) \ge \delta \\
        \text{out} & \text{sign}(\mu_{id} - \mu_{ood}) \cdot A(s(\bm{x}, f)) < \delta 
   \end{cases}
\end{align}

An astute reader may note that aggregation functions are permutation invariant, meaning that all positional information from the two-dimensional saliency maps is lost when aggregating. This may seem strange, as it is primarily the positions of the different values that is important when using \ac{xai} methods for explaining model predictions on images. However, there is good reason to believe that for many image classification tasks, the positions of the points of interest in an \ac{xai} saliency map does not carry much discriminative potential. For datasets such as CIFAR or ImageNet, there is a huge variety in the positions of the ground truth class (a dog may appear in the middle, the top right corner, or any other position and still be of the class 'dog'). As such, it is not a given that the removal of positional information will be massively detrimental. In fact, when \cite{martinez} reflects upon the poor results of their saliency heatmap clustering for detecting \ac{ood} samples on CIFAR10, it is exactly this variability in position they highlight: "Indeed, the CIFAR10-C dataset does not afford the positional bias and low intra-class variability observed in the previous case studies: informative objects for the classes to be predicted appear in arbitrary parts of the image and have a high degree of compositional variability".

Given the exploratory nature of this thesis, it is reasonable to try many different forms of aggregation. Even when just considering the magnitude, it would be insufficient to just use the mean or maximum value of each saliency map, as each form of aggregation captures different qualities about the underlying data. The maximum value, for example, is sensitive to outliers, which may be detrimental. The median value is far less sensitive to outliers, but given that one might expect \ac{id} data to have regions which are very important while most regions are relatively unimportant, outlier insensitivity may actually be undesirable. The vector norm and range are invariant to the sign of the saliencies, which means that if there are both large positive and negative values, these aggregates will return large scores. This is in contrast to the mean, median and third quartile, which will be lower if there are many negative values. In summary, we do not have the prerequisite knowledge about the distribution of saliency maps on \ac{id} and \ac{ood} data to make an informed selection, and as such we should cast a wide net when choosing which forms of magnitude aggregation to use.

Similarly, when choosing statistical dispersion aggregations, we should also include several different metrics. In addition, these metrics should also reduce the effects of magnitude, so that the two hypotheses can be tested individually. Metrics like the variance or standard deviation capture the spread of a distribution of values, but are also correlated with the magnitude: the variance of human arm length is far higher than the variance of human finger length, but this difference is mostly because arms are longer than fingers. Among dispersion metrics which are less dependent on magnitude, there are several to choose from: The \ac{cv} is simply the standard deviation divided by the mean, and is a simple and intuitive method of standardizing dispersion values. The \ac{qcd} is another dispersion metric, which uses quartile information as opposed to the mean and standard deviation. This reduces the sensitivity to outliers when compared to the \ac{cv}, but as stated previously, we do not have the requisite knowledge to know whether insensitivity to outliers is a benefit. As such, we should use both metrics. The \ac{qcd} is equal to half of the interquartile range divided by the midhinge, which can be simplified to the difference between the third and first quartile divided by their sum. A third dispersion metric is the \ac{rmd}. The \ac{rmd} is equal to the mean absolute difference divided by the arithmetic mean. This metric has the desirable quality of being invariant to positive scaling. In addition, the \ac{rmd} can be defined in terms of the second L-moment, while the standard deviation can be defined in terms of the second conventional moment. This makes the \ac{rmd} a suitable complementary dispersion metric to the \ac{cv} and \ac{qcd}.

To summarize, the following aggregations have been used for the experiments in chapter \ref{chapter:experiments}. \\

\textbf{Magnitude:}
\begin{itemize}
    \itemsep0em
    \item Mean
    \item Median
    \item Vector norm
    \item Range
    \item Maximum
    \item Third Quartile
\end{itemize}

\textbf{Statistical dispersion:}
\begin{itemize}
    \itemsep0em
    \item \acf{cv}
    \item \acf{qcd}
    \item \acf{rmd}
\end{itemize}

These two categories correspond to the two hypotheses described in the preceding paragraphs, and will test whether either the magnitude or statistical spread of \ac{id} saliency maps differ substantially from those of \ac{ood} saliency maps.

\subsection{Saliency integrated into existing \ac{ood} detection algorithms}

Given the poor results of \cite{martinez}, one might expect that saliency maps on their own are insufficient to differentiate \ac{id} and \ac{ood} data. \cite{combood} has shown that by combining the \ac{ood} detection scores of two different methods, the total performance can be improved considerably. Furthermore, \cite{vim} has shown improvements by considering the softmax, logit and feature space in tandem. Thus, there is reason to believe that if \ac{xai} saliency maps have at least some discriminatory capabilities, these could be combined with traditional \ac{ood} detection methods, resulting in a performance gain. As such, I further present two frameworks which use combine \ac{xai} saliencies and traditional \ac{ood} detection metrics, to investigate if the addition of saliency values can enhance existing methods.

\subsubsection{ Saliency Aggregation plus Logit} \label{section:salpluslogitmethod}

In 2024, \cite{combood} introduced \texttt{COMBOOD}. This \ac{ood} detector combines the \ac{ood} detection scores of two different distance metrics; Mahalanobis distance and nearest neighbour distance. Each distance metric uses a different feature extraction method, which allows the two methods to collect different forms of information and complement each other. The combination is done by a simple unweighted addition of the confidence scores computed from the log distributions of the two metrics. \texttt{COMBOOD} achieves \ac{sota} performance, and is (at the time of writing) by far the best performing method on ImageNet200 and ImageNet1K in the OpenOOD benchmark. Based on these results, I propose a similar method for combining \ac{xai}-based and traditional \ac{ood} detection metrics. If it is the case that \ac{xai} methods extract \ac{ood} information from the model in ways which are substantially different from traditional \ac{ood} detection strategies, we may see improvements similar to those observed by \cite{combood}.

The field of \ac{ood} detection is vast, and in OpenOOD alone there are over 40 different \ac{ood} detection methods. All of these return a single metric and could thus be combined with an \ac{xai} \ac{ood} detection metric similar to how metrics are combined in \cite{combood}. However, investigating all such possible combinations would be a monumental undertaking, and is infeasible under the time constraints of a master's thesis. Instead, I will constrain myself to introducing a simple baseline framework, combining Saliency Aggregation with the \ac{mls}.

Under this framework, the \ac{ood} score is thus a sum of a saliency aggregate and the maximum logit. However, due to the fact that both the logit and saliency values can be of arbitrary magnitude, we must normalize them before summing if we want each part to contribute equally to the final score. Thus, we can sum the Z-scores of each metric instead. This ensures that the values of the maximum logit and the saliency aggregate are distributed in about the same way. To calculate the Z-scores, we can simply subtract the mean and divide by the standard deviation over an entire \ac{id} validation dataset, for each metric. Thus, we calculate the mean and standard deviations of the maximum logit over an \ac{id} validation set $\mu_{\text{MLS}}^{id}$ and $\sigma_{\text{MLS}}^{id}$, as well as the mean and standard deviation of the aggregate of saliencies $\mu_{\text{Agg}}^{id}$ and $\sigma_{\text{Agg}}^{id}$. In addition, we must calculate the mean value of the aggregation metric over a validation \ac{ood} dataset, as we do not know whether a given aggregation is higher or lower for \ac{id} data. We denote this value as $\mu_{\text{Agg}}^{ood}$. We now have the necessary values required to define this framework mathematically:

As in the previous section, we assume we have a model $f: \bm{x} \to \R^C$, an \ac{xai} saliency mapping method $s: (f, \bm{x}) \to \R^{N \times M}$, and an aggregation function $A: \bm{x} \rightarrow \R$. Let us denote $\text{sign}(\mu_{\text{Agg}}^{id} - \mu_{\text{Agg}}^{ood})$ as $S$, for readability. An \ac{ood} detector under this framework then has the following form, given a threshold $\delta$:

{\large
\begin{align}
    g(\bm{x}; s, A, \delta)=\begin{cases} 
    \text{in } &  S \cdot \frac{A(s(\bm{x}, f)) - \mu_{\text{Agg}}^{id}}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x}) - \mu_{\text{MLS}}^{id}}{\sigma_{\text{MLS}}^{id}} \ge \delta \\[10pt]
    \text{out} &  S \cdot \frac{A(s(\bm{x}, f)) - \mu_{\text{Agg}}^{id}}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x}) - \mu_{\text{MLS}}^{id}}{\sigma_{\text{MLS}}^{id}} < \delta \\
   \end{cases}
\label{eq:aggregate}
\end{align}
}

In fact, this detector can be simplified somewhat. Consider the following:

\begin{gather}
S \cdot \frac{A(s(\bm{x}, f)) - \mu_{\text{Agg}}^{id}}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x}) - \mu_{\text{MLS}}^{id}}{\sigma_{\text{MLS}}^{id}} = \\
S \left( \cdot \frac{A(s(\bm{x}, f))}{\sigma_{\text{Agg}}^{id}} - \frac{\mu_{\text{Agg}}^{id}}{\sigma_{\text{Agg}}^{id}} \right) + \frac{\max_i S(\bm{x})}{\sigma_{\text{MLS}}^{id}} - \frac{\mu_{\text{MLS}}^{id}}{\sigma_{\text{MLS}}^{id}} = \\
S \cdot \frac{A(s(\bm{x}, f))}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x})}{\sigma_{\text{MLS}}^{id}} - \left(S \cdot \frac{\mu_{\text{Agg}}^{id}}{\sigma_{\text{Agg}}^{id}} + \frac{\mu_{\text{MLS}}^{id}}{\sigma_{\text{MLS}}^{id}} \right)
\end{gather}

Notice how all the values in the third term of the above summation are constants; they do not depend on $\bm{x}$. Thus, we can disregard these terms, as all they do is shift all outputs by a constant value. The final \ac{ood} detector thus has the following form:

{\large
\begin{align}
    g(\bm{x}; s, A, \delta)=\begin{cases} 
    \text{in } &  S \cdot \frac{A(s(\bm{x}, f))}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x})}{\sigma_{\text{MLS}}^{id}} \ge \delta \\[10pt]
    \text{out} &  S \cdot \frac{A(s(\bm{x}, f))}{\sigma_{\text{Agg}}^{id}} + \frac{\max_i S(\bm{x})}{\sigma_{\text{MLS}}^{id}} < \delta \\[10pt]
   \end{cases}
\label{eq:aggregate}
\end{align}
}

It should be reiterated that the second term in this \ac{ood} detection metric need not be the \ac{mls}. In theory, it can be any metric calculated from the network on a given input, for example \ac{sota} methods such as AdaSCALE \cite{adascale}, RotPred \cite{rotpred} or EBO \cite{energy}.

\subsubsection{SaliencyVIM: Virtual Logit Matching with Saliencies} \label{section:saliencyvim_method}

As a final proof-of-concept framework, I propose adding saliency values directly into preexisting \ac{ood} detection models. Like the framework introduced in the previous section, there are many \ac{ood} detection models which could be the basis for such a framework. However I will limit myself to a single method in this case as well, to allow for rigorous testing within the time constraints of this thesis. Out of the many possible methods, \ac{vim} \cite{vim} is a fitting choice, as \ac{vim} attempts to combine information from different sources, namely from the feature, logit and softmax probability space. As we can recall from section \ref{section:vim}, \ac{vim} uses a "virtual logit" which is calculated from the output features on the penultimate layer and appended to the original logits. As a proof-of-concept integration of saliencies into this method, we may append \ac{xai} saliencies to the features prior to the generation of this virtual logit. The virtual logit in \ac{vim} is calculated as the information lost when performing a \ac{pca} feature reduction on the features. By appending the saliencies, the \ac{pca} takes into account not just how the penultimate features vary together, but also the interplay between the penultimate features and the \ac{xai} saliencies, which may be substantially different between \ac{id} and \ac{ood} datasets. This increase in variance could increase the reconstruction loss when projecting \ac{ood} samples using the principal components calculated from the \ac{id} dataset, increasing the separability of \ac{id} and \ac{ood} data points. Because there is no equivalent to centering the feature space as described in \cite{vim} for saliencies, I keep the saliencies as they are while the features are centered.

To define this \ac{ood} detector mathematically, we follow the definition of \cite{vim} very closely. We assume we have a model $f: \bm{x} \to \R^C$ and a \ac{xai} saliency mapping method $s: (f, \bm{x}) \to \R^{N \times M}$. We further assume we can extract the penultimate features $h \in \R^K$, with a function $g: (f, \bm{x}) \to \R^K$. We denote these extracted features as $\bm{z}$, and the entire set of $L$ feature vectors over the \ac{id} validation dataset as $Z \in \R^{L \times K}$. As in \cite{vim}, we calculate an offset $\bm{o} = -(W^T)^+\bm{b}$, where $W$ and $\bm{b}$ are the weights and biases which transform the features into logits. The feature matrix $Z$ is transformed by this offset: $Z^\dagger = Z + \bm{o}$. In addition to this feature matrix, we also calculate saliencies over the \ac{id} dataset, and flatten them to produce a saliency matrix $S \in \R^{L \times (N \cdot M)}$. These two matrices are concatenated, giving us the matrix $Y = \left[ Z^\dagger, S \right]$. We then perform an eigendecomposition on $Y^TY$, as part of the \ac{pca} and in accordance with the method described by \cite{vim}:

\begin{equation}
    Y^TY = Q\Lambda Q^{-1}
\end{equation}

\noindent The first $D$ columns of $Q$ make up the $D$-dimensional principal subspace $P$. $D$ is a hyperparameter that must be tuned, and is usually some fraction of $K$, the amount of logits in the penultimate layer (for example, if the number of logits is 1024, $D$ may be 512 or 256). For this thesis, $D$ has been fixed at 256, to avoid the extra computational overhead required for hyperparameter tuning. The remaining $K - D$ columns make up the null-space of $P$. Following the notation of \cite{vim}, we denote the matrix making up these remaining columns as $R$. With this matrix, we can calculate the residual of $\bm{x}$ onto $P$ as $\bm{x}^{P^T} = RR^T\bm{x}$. The virtual logit then has the following form:

\begin{equation}
l_0 := \alpha || \bm{x}^{\text{Null}(P)}|| = \alpha \sqrt{\bm{x}^T RR^T\bm{x}}
\end{equation}

\noindent This is exactly the same as the virtual logit for \ac{vim}, with the only difference being that $R$ is calculated from the concatenated matrix $Y$, which includes both saliencies and logits, as opposed to only logits. To make it clear that $R$ no longer depends solely on the network $f$ and the input data, but also on the choice of saliency generator $s$, we can denote $R$ as $R_s$. As with \ac{vim}, $\alpha$ is calculated from the average virtual and maximum logit over the \ac{id} dataset before the \ac{ood} detector is put into use.

The \ac{ood} detection score is the softmax score of the virtual logit when appended to the rest of the logits:

\begin{equation}
\text{SaliencyVIM}(\bm{x}) = \frac{e^{\alpha\sqrt{\bm{x}^TR_sR_s^T\bm{x}}}}{\sum_{i=1}^C e^{l_i} + e^{\alpha\sqrt{\bm{x}^TR_sR_s^T\bm{x}}}}
\end{equation}

\noindent Thus, the formal definition of the SaliencyVIM \ac{ood} detector is as follows, given a threshold $\delta$:

\begin{align}
    g(\bm{x}; \delta)=\begin{cases} 
        \text{in } & \text{SaliencyVIM}(\bm{x}) \ge \delta \\[10pt]
        \text{out} & \text{SaliencyVIM}(\bm{x}) < \delta \\[10pt]
   \end{cases}
\label{eq:aggregate}
\end{align}

\noindent Because we are no longer aggregating the saliencies, the choice of saliency method is no longer as free as with the previous two methods. When we wish to use the saliencies directly, we cannot use methods which output one value for every single pixel in the input image, as this would lead tens of thousands of dimensions over which to compute a \acf{pca}. This is computationally intractable. Instead, we must use methods which calculate saliencies over larger windows, such as occlusion, \ac{lime} or \ac{gradcam}.

\section{Relation to existing methods} \label{section:relation}

To further justify the idea that \ac{xai} saliency values can be used for \ac{ood} detection, I present proof that by choosing a specific \ac{xai} saliency mapping method and aggregation under the Saliency Aggregation framework, the resulting \ac{ood} detection score is equivalent to \ac{mls}, one of the baseline methods used for \ac{ood} detection. The \ac{mls} \ac{ood} detector simply uses the maximum logit as an indicator of \ac{ood}-ness, and has been described in more detail in section \ref{section:background_baselines}.

In this case, we choose mean aggregation, and \ac{gradcam} as the saliency mapping method. In addition, we assume that the classification stage of the \ac{cnn} is a simple \ac{gap} over the feature map followed by a single linear layer. This is not an unreasonable assumption, as this is the classification head of all ResNet models. Finally, we choose to perform \ac{gradcam} on the final layer of the network, which is recommended by \cite{gradcam}. Under these conditions, the following is true:

\begin{theorem} \label{theorem}
The \ac{ood} score of Saliency Aggregation is equal to the \ac{mls}, up to a constant $a$:
\[\text{SalAgg}(\bm{x}) = a \cdot \text{MLS}(\bm{x}) \]
\end{theorem}

\begin{proof}

As defined in equation \ref{eq:salagg_def}, the \ac{ood} detection score of Saliency Aggregation has the following form:

\begin{equation}
    \text{SalAgg}(\bm{x}) = \text{sign}(\mu_{id} - \mu_{ood}) \cdot A(s(\bm{x}, f))
\end{equation}

\noindent In this special case, $A$ is equal to {\it mean} and $s$ is equal to {\it GradCAM}. Following \cite{gradcam}, the saliency map generated by \ac{gradcam} has the following definition:

\begin{equation}
    \text{GradCAM}(\bm{x}) = ReLU\left(\sum_k \left( \frac{1}{N \cdot M} \sum_i \sum_j \frac{\delta y^c}{\delta F_{ij}^k} \right) F^k \right).
\end{equation}

\noindent Here, $F^k$ is the k'th channel of the final convolutional feature map, while $N$ and $M$ are its dimensions.\footnote{The attentive reader will notice that this is the same notation used for the dimensions of saliency maps throughout this thesis. This is intentional, as the saliency map generated by \ac{gradcam} has the same dimensions as the feature map on which the algorithm is performed.} The above equation simply describes averaging the gradients of the logit of class $c$ for each channel, and using these values to perform a weighted sum of the channels in the feature map, as described in section \ref{section:xaimethodsbackground}. While $c$ can be any class index, in this case, we define $c = \max_i f_i(\bm{x})$, i.e we calculate the saliency map for the predicted class, as defined by the framework. As will be explained in more detail later (section \ref{section:methodology_xai}), we do not wish to perform ReLU rectification with any of our saliency mapping methods, as we wish to keep all information extracted by the saliency mapping, not just the values which increase the probability of the predicted class. Using mean as the aggregation, the \ac{ood} score is then:

\begin{equation}
    \text{SalAgg}(\bm{x}) = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \text{mean} \left(\sum_k \left( \frac{1}{N \cdot M} \sum_i \sum_j \frac{\delta y^c}{\delta F_{ij}^k} \right) F^k \right).
\end{equation}

\noindent For the network as described above, the logit $y^c$ for class $c$ is calculated in the following manner:

\begin{equation}
    y^c = \sum_k \text{mean}(F_k) \cdot W_{ck} \\
\end{equation}

\begin{equation} \label{eq:quick}
    = \sum_k \left( \frac{\sum_i \sum_j F^k_{ij}}{N \cdot M} \cdot W_{ck} \right) .
\end{equation}

\noindent This equation simply describes \ac{gap} (all channels are averaged to a single number) followed by a single linear layer (each logit is a weighted sum of the average pooled channels, with the weights defined by a specific row/column in the weight matrix $W$). Given our definition of $c = \max_i f_i(\bm{x})$, $y^c = \text{MLS}(\bm{x})$. We return to the equation for $\text{SalAgg}(\bm{x})$:

\begin{equation} \label{eq:salagg}
    \text{SalAgg}(\bm{x}) = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \text{mean} \left(\sum_k \left( \frac{1}{N \cdot M} \sum_i \sum_j \frac{\delta y^c}{\delta F_{ij}^k} \right) F^k \right).
\end{equation}

\noindent Given equation \ref{eq:quick},

\begin{equation}
    \frac{\delta y^c}{\delta F^k_{ij}} = \frac{W_{ck}}{N \cdot M}.
\end{equation}

\noindent As we can see, the indices $i$ and $j$ have disappeared. This is to be expected, as global average pooling means that all values in each channel are multiplied by the same value when calculating the logit of a specific class. We may now substitute this derivative in equation \ref{eq:salagg}:

\begin{equation}
    \text{SalAgg}(\bm{x}) = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \text{mean} \left(\sum_k \left( \frac{1}{N \cdot M} \sum_i \sum_j \frac{W_{ck}}{N \cdot M} \right) F^k \right).
\end{equation}

\noindent We now perform some simple algebra, exploiting the fact that $\text{mean}(a \cdot \bm{x}) = a \cdot \text{mean}(\bm{x})$ and that $\sum_i c \cdot x_i = c \sum_i x_i$:

\begin{gather}
    \text{SalAgg}(\bm{x}) = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \text{mean} \left(\sum_k \left( \frac{1}{N \cdot M} \sum_i \sum_j \frac{W_{ck}}{N \cdot M} \right) F^k \right) \\
    = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \frac{1}{N \cdot M} \text{mean} \left(\sum_k \left(  \sum_i \sum_j \frac{W_{ck}}{N \cdot M}  \right) F^k \right) \\
    = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \frac{1}{N \cdot M} \text{mean} \left(\sum_k \left( (N \cdot M) \frac{W_{ck}}{N \cdot M} \right) F^k \right) \\
    = \text{sign}(\mu_{id} - \mu_{ood}) \cdot \frac{1}{N \cdot M} \text{mean} \left(\sum_k  W_{ck} \cdot F^k \right) \\
    = \left(\text{sign}(\mu_{id} - \mu_{ood}) \cdot \frac{1}{N \cdot M} \right) \cdot \left( \sum_k  W_{ck} \cdot \text{mean}(F^k) \right).
\end{gather}

% TODO: CHECK THIS EQUATION

\noindent The first factor above is a constant. It does not depend on $\bm{x}$, as all values are calculated before inference, or are themselves constants. As such, we can denote this factor as $a$. We recognize the second factor as $y^c = \text{MLS}(\bm{x})$ as described in equation \ref{eq:quick}. We then have

\begin{equation}
    \text{SalAgg}(\bm{x}) = a \cdot \text{MLS}(\bm{x}),
\end{equation}

\noindent which was what we wanted to prove.

\end{proof}

The constant factor has no effect on the \ac{ood} detection, as it just means that the thresholds $\delta$ will differ by this factor between the two detectors, with all predictions being the same for either method. Thus, theorem \ref{theorem} states that Saliency Aggregation and \ac{mls} \ac{ood} detection are functionally equivalent given the conditions described above.

This theorem shows that \ac{xai} saliency mapping methods, although they have been developed for an entirely different purpose than \ac{ood} detection, also collect information from the network which can be used for \ac{ood} detection.

\section{Benchmarks} \label{section:benchmarks}

After having introduced the three novel \ac{ood} detection frameworks that I plan to test, I will introduce the \ac{ood} detection benchmarks that will be used. To best align this thesis with the broader \ac{ood} detection field, I will use the OpenOOD framework, and their collection of \ac{ood} detection benchmarks. Each OpenOOD benchmark is defined by an \ac{id} dataset, and a selection of corresponding \ac{ood} datasets. As described in \cite{openood15}, the four standard \ac{ood} detection benchmarks included in OpenOOD are CIFAR10, CIFAR100, ImageNet200, and ImageNet1K. I have used all four benchmarks in this thesis, to ensure that the results are robust and easily comparable to the \ac{sota} within the field of \ac{ood} detection.

\subsection{CIFAR10/CIFAR100}

CIFAR10 and CIFAR100 are two general object recognition datasets collected by \cite{cifar}. These datasets are commonly used in \ac{ai} research \cite{pouyanfar2018survey}. Each RGB image has dimension $32 \times 32$, and there are 50 000 training images and 10 000 test images for both datasets. In CIFAR10, each of the ten classes has 5000 training samples and 1000 test samples, while for CIFAR100 each of the one hundred classes has 500 training samples and 100 test samples. For CIFAR10, the ten classes are as follows: airplane, car, bird, cat, deer, dog, frog, horse, ship, and truck. The classes in CIFAR100 are in the same vein, but without any overlap with CIFAR10. OpenOOD presents a series of Near-\ac{ood} and Far-\ac{ood} datasets which are used in conjunction with these datasets. The collection of an \ac{id} dataset and corresponding \ac{ood} datasets constitute a benchmark. Both datasets have the same Far-\ac{ood} datasets, and also use TinyImageNet for Near-\ac{ood} evaluation. In addition, the CIFAR10 benchmark uses CIFAR100 as a Near-\ac{ood} dataset, while the CIFAR100 benchmark uses CIFAR10. Table \ref{table:cifar10} presents a short description of each dataset in the benchmarks, as well as the number of samples in the set for each dataset.

\begin{table}[hbtp]
\begin{center}
\begin{tabular}{ |l|l|l| } 
    \hline
    Dataset & Size & Description \\
    \hline
    \rowcolor{id!50}
    CIFAR10/CIFAR100 & 10 000 & General objects \\ 
    \hline
    \hline
    \multicolumn{3}{|c|}{Near-\ac{ood}} \\
    \hline
    \rowcolor{near!50}
    CIFAR100/CIFAR10 & 10 000 & General objects \\ 
    \rowcolor{near!50}
    TinyImageNet & 7 793 & General objects, downsampled from ImageNet \\ 
    \hline
    \hline
    \multicolumn{3}{|c|}{Far-\ac{ood}} \\
    \hline
    \rowcolor{far!50}
    MNIST & 70 000 & Handwritten digits from 0 to 9 \\ 
    \rowcolor{far!50}
    SVHN & 26 032 & Street view house numbers \\ 
    \rowcolor{far!50}
    Texture & 5 640 & Textural patterns \\ 
    \rowcolor{far!50}
    Places365 & 35 195 & Places, scenes, locations \\ 
    \hline
    \end{tabular}
    \caption[CIFAR benchmark datasets]{Table showing the datasets which constitute the CIFAR10 and CIFAR100 OpenOOD benchmarks}
    \label{table:cifar10}
\end{center}
\end{table}

In addition, a sample of images from CIFAR10 and the corresponding \ac{ood} datasets are shown in figure \ref{fig:cifar10}. Note the low resolution of the images, at only $32 \times 32$. Given that the only difference between the CIFAR10 and CIFAR100 benchmark is that CIFAR10 becomes a Near-\ac{ood} dataset and CIFAR100 becomes the \ac{id} dataset, this figure will suffice to give an impression of both benchmarks.

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]
    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    \def\rowpadding{0.8cm}
    \pgfmathtruncatemacro{\offset}{\cellheight + \rowpadding}
    % Loop through the grid
    \foreach \row in {0, 1, 2, 3, 4, 5, 6} {
        \foreach \col in {0, 1, 2} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 3 + \col}
            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth, -\row * \offset - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/cifar10_examples/image-img\imagenumber.png}};
        }
    }
    \node [above=0.1cm of 1] {\ac{id}: CIFAR10};
    \node [above=0.1cm of 4] {Near-\ac{ood}: CIFAR100};
    \node [above=0.1cm of 7] {Near-\ac{ood}: TinyImageNet};
    \node [above=0.1cm of 10] {Far-\ac{ood}: MNIST};
    \node [above=0.1cm of 13] {Far-\ac{ood}: SVHN};
    \node [above=0.1cm of 16] {Far-\ac{ood}: Texture};
    \node [above=0.1cm of 19] {Far-\ac{ood}: Places365};
    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.2,-3.6);
    \end{scope}
    \begin{scope}[on background layer={color=near!50}]
        \fill (-1.5,-3.6) rectangle (6.2,-9.8);
    \end{scope}
    \begin{scope}[on background layer={color=far!50}]
        \fill (-1.5,-9.8) rectangle (6.2,-22.3);
    \end{scope}
    \end{tikzpicture}
    \caption[CIFAR10 dataset example images]{Figure showing three example images from CIFAR10 and from the corresponding \ac{ood} datasets}
    \label{fig:cifar10}
    \end{center}
\end{figure}



\subsection{ImageNet200/ImageNet1K}

ImageNet is another ubiquitous image classification dataset, which is used in a large amount of computer vision works. In contrast to CIFAR, ImageNet images are substantially larger, at $256 \times 256$.\footnote{It is common to center crop as part of preprocessing on ImageNet, and as such you will more often see the dimensions $224 \times 224$ mentioned in this thesis} This increase in dimensionality could be used to argue that ImageNet tasks are more realistic, as it is rare that one would use $32 \times 32$ images in practical applications. The ImageNet dataset contains 1000 classes of general objects. In addition, there exists smaller versions, such as ImageNet200. As with the other \ac{id} datasets, OpenOOD contains a selection of \ac{ood} datasets which are chosen to test a model's ability to differentiate between different forms of \ac{ood} data. These \ac{ood} datasets, along with the \ac{id} dataset, constitute the \ac{ood} detection benchmark. Here, OpenOOD does not use ImageNet200 as a Near-\ac{ood} dataset for ImageNet1K, nor vice versa. Instead, both datasets use exactly the same \ac{ood} datasets, to facilitate straight comparisons between the performance on ImageNet1K and ImageNet200 \cite{openood15}. Table \ref{table:imagenet200} lists these datasets, and gives a short description for each one.

\begin{table}[hbtp]
\begin{center}
\begin{tabular}{ |l|l|l| } 
    \hline
    Dataset & Size & Description \\
    \hline
    \rowcolor{id!50}
    ImageNet200/ImageNet1K & 9 000/45 000 & General objects \\ 
    \hline
    \hline
    \multicolumn{3}{|c|}{Near-\ac{ood}} \\
    \hline
    \rowcolor{near!50}
    SSB-Hard & 49 000 & General objects taken from ImageNet21K \\ 
    \rowcolor{near!50}
    NINCO & 5 879 & General objects, manually curated by \cite{bitterwolf2023ninco} \\ 
    \hline
    \hline
    \multicolumn{3}{|c|}{Far-\ac{ood}} \\
    \hline
    \rowcolor{far!50}
    iNaturalist & 10 000 & Various plants \\ 
    \rowcolor{far!50}
    Texture & 5 160 & Textural patterns \\ 
    \rowcolor{far!50}
    OpenImage-O & 15 869 & General objects, manually curated by \cite{vim} \\ 
    \hline
    \end{tabular}
    \caption[ImageNet benchmark datasets]{Table showing the datasets which constitute the ImageNet200 and ImageNet1K OpenOOD benchmarks}
    \label{table:imagenet200}
\end{center}
\end{table}

In addition, figure \ref{fig:imagenet200} shows three example images for each dataset in the ImageNet200 benchmark. Like the preceding section, I will not show a separate figure for ImageNet1K, as the only change would be the \ac{id} dataset, which would show slightly different classes.

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]
    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    \def\rowpadding{0.8cm}
    \pgfmathtruncatemacro{\offset}{\cellheight + \rowpadding}
    % Loop through the grid
    \foreach \row in {0, 1, 2, 3, 4, 5} {
        \foreach \col in {0, 1, 2} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 3 + \col}
            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth, -\row * \offset - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/imagenet200_examples/image-img\imagenumber.png}};
        }
    }
    \node [above=0.1cm of 1] {\ac{id}: ImageNet200};
    \node [above=0.1cm of 4] {Near-\ac{ood}: SSB-Hard};
    \node [above=0.1cm of 7] {Near-\ac{ood}: NINCO};
    \node [above=0.1cm of 10] {Far-\ac{ood}: iNaturalist};
    \node [above=0.1cm of 13] {Far-\ac{ood}: Texture};
    \node [above=0.1cm of 16] {Far-\ac{ood}: OpenImage-O};
    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.2,-3.6);
    \end{scope}
    \begin{scope}[on background layer={color=near!50}]
        \fill (-1.5,-3.6) rectangle (6.2,-9.8);
    \end{scope}
    \begin{scope}[on background layer={color=far!50}]
        \fill (-1.5,-9.8) rectangle (6.2,-19.1);
    \end{scope}
    \end{tikzpicture}
    \caption[ImageNet200 dataset example images]{Figure showing three example images from ImageNet200 and from the corresponding \ac{ood} datasets}
    \label{fig:imagenet200}
    \end{center}
\end{figure}


\subsection{Overview of testing environment} \label{section:testing_environment}

After having introduced the specific benchmarks, I will describe the testing environment that has been used, so that it is clear for the reader how the \ac{ood} detection metrics are calculated.

As we have seen, for a given benchmark, we have one \ac{id} dataset and a series of \ac{ood} datasets, which are all semantically shifted in relation to the \ac{id} dataset. These \ac{ood} datasets are categorized into Near- and Far-\ac{ood}, depending on the degree of their semantic shift. The \ac{id} dataset is split into a train, validation and test set, while the \ac{ood} datasets are split into validation and test sets. A network is trained on the train \ac{id} set.  Then, one \ac{ood} validation set is chosen to be used for hyperparameter tuning. Any hyperparameters that the \ac{ood} detection algorithm may have are tuned on this validation set and the \ac{id} validation dataset. After having trained the network and tuned the \ac{ood} detector, we now have every part needed to perform \ac{ood} detection and calculate performance metrics.

First, we calculate \ac{ood} detection scores over the entire \ac{id} test set. By convention, these are expected to be higher than for \ac{ood} samples. For example, if we use the \ac{mls} \ac{ood} detector as described in section \ref{section:background_baselines}, we would record the value of the highest logit for all samples in the \ac{id} test set and store them. Then, for each \ac{ood} test dataset, we would also record the maximum logit for all samples. To calculate \ac{auroc} scores, each set of \ac{ood} test set scores is compared against the \ac{id} test set scores we calculated previously, by denoting all \ac{id} samples as class 0 and all \ac{ood} samples as class 1. After having done this for each \ac{ood} test set individually, we have a series of Near- and Far-\ac{ood} \ac{auroc} scores. The average performance of our classifier on Near- and Far-\ac{ood} can then be calculated by averaging over these individual scores.  Figure \ref{fig:testing_environment} shows a diagram of this process.

\begin{figure}[hbtp]
    \begin{center}

    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]

    \node[SIR, align=center, minimum width=9em] (ooddetector) {Tuned OOD\\Detector};
    \node[SIR, align=center, below=3em of ooddetector.west, anchor=west, minimum width=9em, fill=id!50] (imagenet) {ImageNet200 \\Test set};
    \node[SIR, align=center, minimum width=9em, below=2.95em of imagenet.west, anchor=west, fill=near!50] (ssbhard) {SSB-Hard \\Test set};
    \node[SIR, align=center, minimum width=9em, below=2.95em of ssbhard.west, anchor=west, fill=near!50] (ninco) {NINCO \\Test set};
    \node[SIR, align=center, minimum width=9em, below=2.95em of ninco.west, anchor=west, fill=far!50] (inaturalist) {iNaturalist \\Test set};
    \node[SIR, align=center, minimum width=9em, below=2.95em of inaturalist.west, anchor=west, fill=far!50] (texture) {Texture \\Test set};
    \node[SIR, align=center, minimum width=9em, below=2.95em of texture.west, anchor=west, fill=far!50] (openimage) {OpenImage \\Test set};



    \node[minimum width=9.5em, right=11em of ooddetector.west, anchor=west, align=center] {Calculate scores\\with OOD Detector};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of imagenet.west, anchor=west, fill=id!50] (imagenetscores) {8, 6, 4, 8,\\9, 7, 5, 3};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of ssbhard.west, anchor=west, fill=near!50] (ssbhardscores) {4, 4, 2, 1,\\7, 3, 3, 7};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of ninco.west, anchor=west, fill=near!50] (nincoscores) {5, 1, 3, 8\\1, 2, 1, 8};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of inaturalist.west, anchor=west, fill=far!50] (inaturalistscores) {0, 1, 0, 0\\1, 4, 8, 1};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of texture.west, anchor=west, fill=far!50] (texturescores) {1, 2, 5, 6\\4, 3, 4, 0};
    \node[SIR, align=center, minimum width=9.5em, right=11 em of openimage.west, anchor=west, fill=far!50] (openimagescores) {1, 2, 0, 1\\2, 1, 2, 3};

    \draw[-latex] (ooddetector.east) --+ (0.3, 0) --+ (0.3, -6.831) -- (openimagescores.west) node[] {};

    \draw[-latex] (imagenet.east) -- (imagenetscores.west) node[] {};
    \draw[-latex] (ssbhard.east) -- (ssbhardscores.west) node[] {};
    \draw[-latex] (ninco.east) -- (nincoscores.west) node[] {};
    \draw[-latex] (inaturalist.east) -- (inaturalistscores.west) node[] {};
    \draw[-latex] (texture.east) -- (texturescores.west) node[] {};
    \draw[-latex] (openimage.east) -- (openimagescores.west) node[] {};

    \node[minimum width=6.7em, right=11.5em of imagenetscores.west, anchor=west, align=center] {Calculate\\AUROC};
    \node[SIR, align=center, minimum width=6.7em, right=11.5 em of ssbhardscores.west, anchor=west, fill=near!50] (ssbhardauroc) {AUROC:\\0.84};
    \node[SIR, align=center, minimum width=6.7em, right=11.5 em of nincoscores.west, anchor=west, fill=near!50] (nincoauroc) {AUROC:\\0.86};
    \node[SIR, align=center, minimum width=6.7em, right=11.5 em of inaturalistscores.west, anchor=west, fill=far!50] (inaturalistauroc) {AUROC:\\0.89};
    \node[SIR, align=center, minimum width=6.7em, right=11.5 em of texturescores.west, anchor=west, fill=far!50] (textureauroc) {AUROC:\\0.95};
    \node[SIR, align=center, minimum width=6.7em, right=11.5 em of openimagescores.west, anchor=west, fill=far!50] (openimageauroc) {AUROC:\\0.91};

    \draw[-latex] (ssbhardscores.east) -- (ssbhardauroc.west) node[] {};
    \draw[-latex] (nincoscores.east) -- (nincoauroc.west) node[] {};
    \draw[-latex] (inaturalistscores.east) -- (inaturalistauroc.west) node[] {};
    \draw[-latex] (texturescores.east) -- (textureauroc.west) node[] {};
    \draw[-latex] (openimagescores.east) -- (openimageauroc.west) node[] {};

    \draw[-latex] (imagenetscores.east) --+ (0.3, 0) --+ (0.3, -5.675) -- (openimageauroc.west) node[] {};

    \node[minimum width=7em, right=8.5em of ssbhardauroc.west, anchor=west, align=center] {Average over\\Near and Far};
    \node[SIR, minimum width=7em, right=8.5em of nincoauroc.west, align=center, anchor=west, fill=near!50] (near) {Near-OOD\\AUROC: 0.85};
    \node[SIR, minimum width=7em, right=8.5em of openimageauroc.west, align=center, anchor=west, fill=far!50] (far) {Far-OOD\\AUROC: 0.92};

    \draw[-latex] (ssbhardauroc.east) --+ (0.3, 0) --+ (0.3, -1.135) -- (near.west) node[] {};
    \draw[-latex] (nincoauroc.east) -- (near.west) node[] {};

    \draw[-latex] (inaturalistauroc.east) --+ (0.3, 0) --+ (0.3, -2.27) -- (far.west) node[] {};
    \draw[-latex] (textureauroc.east) --+ (0.3, 0) --+ (0.3, -1.135) -- (far.west) node[] {};
    \draw[-latex] (openimageauroc.east) -- (far.west) node[] {};

    \node[rotate=90] at (-2.7, -4) {ImageNet200 benchmark};
    \draw [decorate, 
        decoration = {brace,
		raise=5pt,
		amplitude=10pt, mirror}, line width=1pt] (-1.7,-0.7) --  (-1.7,-7.3);


    \end{tikzpicture}

    \end{center}
    \caption{Figure showing how the two main performance metrics, Near- and Far-\ac{ood} \ac{auroc}, are calculated for a given \ac{ood} detection method and benchmark}
    \label{fig:testing_environment}
\end{figure}

% TODO: FIGURE

This process describes how the performance of an \ac{ood} detector can be calculated for a given \ac{id} dataset and set of \ac{ood} datasets. However, when developing a new method, one cannot repeatedly perform this process on the same \ac{id} and \ac{ood} datasets, as this will bias the final results. A common option in such a scenario is to simply use some benchmarks for development and some for the final tests. However, for this thesis, this is not sufficient. Firstly, to ensure comparisons with all methods tested under the OpenOOD framework, we should evaluate our performance on all benchmarks included within it. In addition, we have seen from the preceding section that the \ac{ood} datasets are shared between different \ac{id} benchmarks, which means that if we used the ImageNet200 benchmark as a development benchmark, we will have seen all \ac{ood} data of both ImageNet200 and ImageNet1K during development, which would disallow using ImageNet1K as a benchmark during testing. The same problem will arise on CIFAR100 if we use CIFAR10 as the development benchmark.

Because of this, I have separated all benchmarks into validation and test benchmarks. Specifically, all \ac{id} and \ac{ood} validation sets, and all \ac{id} and \ac{ood} test sets have been split in two equally sized subsets by random sampling, of which one half has been used during development and the other half has been used during the final testing. This has been done for all four benchmarks, ensuring that the results reported in chapter \ref{chapter:experiments} have not been biased by repeated testing on the same data. The development of new methods has been done on ImageNet200 and CIFAR10, as there is little benefit to using all four benchmarks during development. The final testing has been done on all benchmarks included in OpenOOD, as mentioned previously.

\section{Networks}

While it may be interesting to see how these new frameworks function on different network architectures, the combination of three novel novel \ac{ood} detection frameworks which utilize five \ac{xai} saliency methods, tested on four different benchmarks, already presents a considerable amount of evaluation. Thus, to focus the thesis on comparing different \ac{ood} detection methods against each other, I believe it is best to fix other parameters such as network architecture. With this in mind, I choose to limit myself to the ResNet \cite{resnet} family of neural networks. In particular, I use ResNet18 for evaluation on CIFAR10, CIFAR100 and ImageNet200, and ResNet50 for evaluation on ImageNet1K. Given the scope of the of the thesis as described in section \ref{section:scope}, I do not use methods which require retraining of the network. As such, all networks are pretrained and I do not perform any training as part of this thesis.

Aside from \acp{cnn}, Vision Transformers also perform exceptionally on computer vision tasks, and achieve \ac{sota} results in many settings \cite{vit}. On ImageNet, they are even dominant, and the top 10 models when considering (top-1) accuracy are all based on vision transformers, as opposed to \acp{cnn}.\footnote{https://paperswithcode.com/sota/image-classification-on-imagenet} As such, one might question the choice of a \ac{cnn} model, when newer and better models have been developed.

However, given that a large part of \ac{xai} methods have been developed under the \ac{cnn} paradigm, many \ac{xai} methods are not easily adapted to vision transformers. Methods such as \ac{gradcam} and \ac{gbp} exploit specific parts of \ac{cnn} architectures when generating explanations \cite{legrad}, and are thus difficult to use with different architectures. To be able to use a broad section of the representative \ac{xai} methods in use today, it is thus preferable to use \acp{cnn} as opposed to vision transformers.

\section{\ac{xai} Saliency Methods} \label{section:methodology_xai}

The theory of each \ac{xai} saliency method used in this thesis has been described in section \ref{section:xaimethodsbackground} of chapter \ref{chapter:background}. In the following section, I will describe the specifics of how I have applied each method in my efforts to better align them with the goal of separating \ac{id} and \ac{ood} data points.

For all saliency methods, one significant difference between my application and those commonly used for model explanation is that I modify all methods to return unnormalized and unrectified saliencies. For most \ac{xai} applications, it is common to rectify the saliencies such that negative saliencies are set to zero. In addition, some methods output normalized saliencies, as \ac{xai} saliency maps are intended to be inspected individually. In these cases, the absolute magnitude can be considered less relevant, and looking at saliencies relative to each other more informative. For my purposes, both negative values and the magnitude may convey important information which should not be discarded.

In addition, any parameters of each \ac{xai} method has been fixed, and no hyperparameter tuning has been performed. Given that I have introduced three different \ac{xai} \ac{ood} detection frameworks which have been tested on four different benchmarks, tuning five \ac{xai} methods was an additional computational burden that could not be afforded.

\subsection{\ac{lime}}

As is common when applying \ac{lime} to images, I have used segmentations of the input image to reduce the dimensionality of the input that is fed into the \ac{lime} algorithm. As explained in section \ref{section:saliencymapbackground}, superpixel segmentation algorithms such as \ac{slic} may increase the accuracy of the explanations, but are CPU-bound and thus introduce a considerable computational overhead (which is problematic when tens of thousands of samples are to be evaluated). Thus, I have chosen to stick to a simple rectangular segmentation instead. Each image is split into $N \times N$ equally sized rectangular regions, with $N$ being set to 4.

\subsection{Occlusion}

Like with \ac{lime}, I have used a rectangular segmentation approach, with each image being split into $N \times N$ regions. As with \ac{lime}, $N$ is set to 4, which ensures that accurate comparisons between \ac{lime} and occlusion can be made.

\subsection{\ac{gradcam}}

When utilizing \ac{gradcam}, a choice must be made about which convolutional layer one should calculate gradients from. As described in section \ref{section:cnn}, earlier layers have higher resolution (more precise spatial information) but lower field of view (less comprehensive semantic information) than later layers. In most cases, the final convolutional layer is seen as most informative, with \cite{gradcam} stating that "convolutional layers naturally retain spatial information which is lost in fully-connected layers, so we can expect the last convolutional layers to have the best compromise between high-level semantics and detailed spatial information". For this reason, I also use the final convolutional layer.

As mentioned previously, I do not apply any \ac{relu} activation function after calculating the saliencies, despite this being an explicit part of the methodology of \cite{gradcam}. This is because information about what the model regards as detrimental to the prediction may also be informative for \ac{ood} detection.

\subsection{Guided Backpropagation}

\ac{gbp} is a very simple method which is completely non-parametric, and as such no specific choices have had to be made. Like with all other methods, no normalization or rectification has been done to the output of \ac{gbp}.

\subsection{Integrated Gradients}

For integrated gradients, the only choice that must be made is the choice of baseline. An ideal baseline should convey no information, and should ideally lead to an output of zero from the network. As \cite{sturmfels2020visualizing} shows, there are no perfect choices for such a baseline, as all methods carry some assumption about our dataset and what it means for an input to "convey no information". However, the common choice for image classification tasks is the zero vector (a completely black image), as recommended by \cite{integratedgradients}. As such, this is the baseline I have used.

\section{Evaluation} \label{section:evaluation}

This section describes the details of how I have evaluated my frameworks on the four OpenOOD benchmarks.

\subsection{Experiment structure}

My experiments are conducted in two parts, corresponding to objective 2 and 3 as described in section \ref{section:problemstatement}. The first part is concerned with analysing the discriminatory power of different aggregations of saliency maps for the five \ac{xai} methods. The purpose of this part is two-fold: First, this analysis gives a good overview of how different \ac{xai} methods extract information which can be used for \ac{ood} detection. As stated in section \ref{section:saliencyagg_method}, it is desirable to cast a wide net when we investigate the different statistical qualities that \ac{xai} saliency maps may have, requiring an analysis of a wide array of aggregations. Secondly, through the analysis of different aggregations on different \ac{xai} methods, a choice of methods under the Saliency Aggregation and Saliency Aggregation plus Logit can be made. These frameworks are highly general, and the total amount of methods which are possible with the number of aggregations and \ac{xai} saliency mapping mentioned in this thesis is 45. Clearly, performing 45 statistical tests is not necessary to investigate whether \ac{xai} methods can be used for \ac{ood} detection, nor is it desirable, as it leads to the {\it multiple comparison problem}. Instead, I choose the best performing aggregation overall for each \ac{xai} method, leading to five methods under test for the Saliency Aggregation and Saliency Aggregation plus Logit framework. Selecting in this way, rather than simply selecting the $N$ highest performing combinations out of the 45, is better suited to the goals of this thesis: This thesis is concerned with investigating the discriminatory power of \ac{xai} methods in an \ac{ood} detection context, not how different aggregations of \ac{xai} saliency maps compare to each other. This part of the experiments is conducted on the two validation benchmarks, as described in section \ref{section:benchmarks}.

The second part of the experiments consists of the evaluation of the three frameworks on the testing benchmarks. This evaluation consists of testing a selection of methods under each framework against baseline methods. For Saliency Aggregation and Saliency Aggregation plus Logit, five methods will be chosen, corresponding to the five \ac{xai} methods introduced in section \ref{section:methodology_xai}, and the best performing aggregation on the validation set (as described in the previous section). For the Saliency\ac{vim} framework, three methods are tested, corresponding to the three compatible \ac{xai} methods \ac{lime}, occlusion and \ac{gradcam}. These methods are tested on each of the four benchmarks across ten bootstraps, enabling statistical analysis.


\subsubsection{Metrics}

\ac{auroc} and \ac{fpr95} are the most common metrics used for \ac{ood} detection \cite{oodbaseline, odin, oodoverview, openood, vim}. In OpenOOD \cite{openood, openood15}, \ac{auroc} is chosen as the primary metric used to rank methods against each other, as mentioned previously. As \cite{openood} by far presents the most comprehensive complete benchmark of all \ac{ood} detection methods to date, I have followed their methodology and used \ac{auroc} when evaluating my methods. Although \ac{fpr95} is reported quite often in \ac{ood} detection literature, I have chosen to omit this metric when reporting the results. Due to the large number of tests that have been performed as part of this thesis, this has been necessary to limit the amount of information conveyed in chapter \ref{chapter:experiments}, which would otherwise be extremely long. When comparing all \ac{ood} detection methods against each other, the \ac{fpr95} is similarly omitted by \cite{openood15}. %TODO put it in the appendix and say here that it is in the appendix

When performing \ac{ood} detection, we must make a choice about whether \ac{ood} samples belong to the positive or negative class. There is no correct answer, but \cite{openood} chooses to consider \ac{ood} samples as the positive class, to "align with ML convention": It is common to consider abnormalities, anomalies, or the unexpected as the positive class (for example, a cancer detection system would consider the presence of cancer to be part of the positive class). This aligns with the goal of \ac{ood} detection, which is to detect abnormal inputs with regard to the data the model is trained on. Thus, I have also followed this convention and considered \ac{ood} samples as positive, and \ac{id} samples as negative.

The \ac{auroc} calculations are done on each \ac{ood} dataset individually, as opposed to comparing all \ac{ood} samples to the \ac{id} dataset. Afterwards, the metrics of all Near-\ac{ood} and Far-\ac{ood} are averaged, giving us two general performance metrics which tell us how a method functions on either Near-\ac{ood} or Far-\ac{ood}. This aligns with the methodology of \cite{openood, openood15}. Similarly, when plotting density plots for a given \ac{ood} detection metric, the densities of all Near- and all Far-\ac{ood} data points are combined, giving a more comprehensible overview than if all \ac{ood} data sets are individually. Figure \ref{fig:full_not_full} shows density plots before and after combining Near- and Far-\ac{ood}, on the maximum logit score of images from the CIFAR10 dataset. As we can see, there is not that much internal variance between the \ac{ood} datasets, allowing us to combine them without losing too much information.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_logits_distribution_full_not_full.pgf}
    \end{center}
    \caption{Density plots of \acs*{mls} on CIFAR10 for all datasets individually and after combining Near- and Far-\acs*{ood}}
    \label{fig:full_not_full}
\end{figure}

% \subsection{Development and Test Sets}
%
% Given the scope of my thesis, which focuses on post-hoc \ac{ood} methods which do not use outlier exposure or require any form of training, there is no need for a training dataset. Despite this, there is still a chance of overfitting, due to the exploratory nature of developing an entirely new class of methods. By trying out different methods and by tuning different parameters to find out how one might best separate \ac{id} and \ac{ood} data using XAI, I am also biasing the results. Therefore, it may be possible that potential improvements in performance are not due to the increased quality of the method, but instead because of peculiarities in the specific datasets that the method is used on.
%
% Thus, I have split all the datasets mentioned in \ref{section:datasets} into development and test sets, and have performed all development of the methods exclusively on the development set. Only when calculating the final metrics have I used the test set, to remove any bias associated with performing multiple experiments on the same data. Due to the large amount of data available in the CIFAR10 and ImageNet200 datasets, and their associated \ac{ood} datasets, I have simply split all datasets into two equally large parts.

\subsubsection{Statistical Analysis of Results} \label{section:ttest_methodology}

When evaluating a new method, it is not enough to simply report the results from a single experiment. Instead one should run the same experiment multiple times and perform statistical analysis to ensure that the results are robust. Therefore, I have bootstrapped each testing benchmark ten times during the calculation of the final test results. This makes it possible to report means and standard deviations of the results as opposed to a single point estimate. With these repeated experiments, it is also possible to perform statistical tests comparing the new methods against baseline \ac{ood} detection methods. If we compute the scores for the baseline methods and the new methods on the same bootstraps, we get paired samples. In this case, we should conduct a paired difference test. The t-test is a natural choice, however, we do not have any reason to assume that the differences in performance between paired samples are normally distributed, nor is the number of bootstraps high enough to assume approximate normality. The Wilcoxon signed-rank test is thus a better choice. For all experiments, I perform a one-sided test, with the null-hypothesis that the tested method is not better than the baseline. I consider a p-value of 0.05 as sufficient to reject the null-hypothesis.

Given that I introduce three general frameworks as opposed to specific methods, it is possible to test several methods under a particular frameworks. Doing this, however, will introduce the {\it multiple comparison problem}, where a p-value of 0.05 will no longer be sufficient for each test, as the chance of randomly receiving results that seem statistically significant is increased by the presence of multiple tests. Thus, I use Bonferroni corrected p-values whenever multiple methods are tested on the same benchmark. I set the threshold for statistical significance at 0.05, leading to a Bonferroni corrected p-value of $0.05 / n$ for a given method, where $n$ is the number of methods tested.

\section{Implementation}

This section goes over the implementation details of my thesis.

\subsection{Basic hardware and software}

Python is the most popular programming language for data science and \ac{ml} research \cite{nguyen2019machine}, and as such it is my language of choice as well. For this thesis, \texttt{Python 3.9} has been used. Below is a table of the key libraries that have been utilized.

\begin{center}
    \begin{tabular}{ |l|c|l| } 
    \hline
    Library & Version number & Short description \\
    \hline
    OpenOOD & 1.5 & Comprehensive \ac{ood} detection framework \\ 
    PyTorch & 2.4.1 & GPU-accelerated \ac{ml} library \\ 
    Captum & 0.7.1 & \ac{xai} methods integrated with PyTorch \\ 
    Scikit-Learn & 1.5.2 & Various \ac{ml} methods \\ 
    NumPy & 1.26.4 & Efficient matrix multiplication and scientific computing \\ 
    Matplotlib & 3.9.2  & Visualization library \\ 
    Seaborn & 0.13.2 & Visualization library built on top of Matplotlib \\
    Pandas & 2.2.3 & Data visualization and manipulation library \\
    SciPy & 1.13.1 & Scientific computing library \\
    \hline
    \end{tabular}
\end{center}

All development and computation was done on a single computer with an \texttt{Intel i7-8700K} CPU and an \texttt{Nvidia GeForce RTX 3090} GPU.

\subsection{Method Evaluation: OpenOOD}

As explained previously, OpenOOD \cite{openood} represents the most comprehensive benchmark for \ac{ood} detection methods. It is also a framework which easily allows for development and benchmarking of new methods, and is thus the ideal framework for the purposes of this thesis.

In particular, OpenOOD includes a "unified, easy to use evaluator" \cite{openood15} that makes evaluating new methods very simple. All that is required is that new methods inherit from a base class (\texttt{BasePostprocessor}\footnote{In OpenOOD, \texttt{Postprocessor}s are the \ac{ood} detection algorithms that generate an \ac{ood} score during inference}), and override the calculation of \ac{ood} scores. Code listing \ref{code:aggregate} shows all the code required to create the Aggregate of Saliency \ac{ood} detector.

\begin{lstlisting}[style=pythonstyle, caption={Source code listing for the Aggregate of Saliency \ac{ood} detector}, captionpos=b, label={code:aggregate}]
class SaliencyAggregatorPostprocessor(BasePostprocessor):
    def __init__(self, config, saliency_generator, aggregator):
        super().__init__(config)

        self.saliency_generator = saliency_generator
        self.aggregator = aggregator

    def postprocess(self, net: nn.Module, data: Any):
        predictions = torch.argmax(net(data), dim=-1)

        saliencies = self.saliency_generator(net, data)
        score_ood = self.aggregator(saliencies, dim=-1)

        return predictions, score_ood
\end{lstlisting}

With this \texttt{postprocessor} defined, evaluating it on a specific benchmark is similarly simple (listing \ref{code:eval}):

\begin{lstlisting}[style=pythonstyle, caption={Source code listing for evaluating methods within the OpenOOD framework}, captionpos=b, label={code:eval}]
resnet18_pretrained = get_network('cifar10')

ood_detector = SaliencyAggregatorPostprocessor(None, GradCAM, torch.mean)

evaluator = Evaluator(
    net=resnet18_pretrained,
    id_name='cifar10',
    postprocessor=ood_detector,
)

metrics = evaluator.eval_ood()

print(metrics)
\end{lstlisting}

This code will calculate the \ac{ood} scores for all data samples in both the \ac{id} and \ac{ood} datasets, and subsequently calculate the \ac{auroc} and \ac{fpr95} for all the \ac{ood} datasets when comparing their \ac{ood} values to the values of the \ac{id} datasets. Code listing \ref{code:msp} shows this output when using the baseline \ac{msp} method.

\begin{lstlisting}[style=pythonstyle, caption={Output of calling \texttt{evaluator.eval\_ood} with CIFAR10 as the benchmark and \ac{msp} as the detector}, captionpos=b, label={code:msp}]
           FPR@95  AUROC  AUPR_IN  AUPR_OUT   ACC
cifar100    61.36  86.51    84.20     85.05 95.56
tin         42.02  88.88    88.81     85.57 95.56
nearood     51.69  87.69    86.51     85.31 95.56 # average of two above
mnist       19.38  93.86    79.72     98.89 95.56
svhn        24.78  91.38    84.26     95.49 95.56
texture     43.31  88.68    91.01     80.97 95.56
places365   41.62  89.21    68.49     96.28 95.56
farood      32.27  90.78    80.87     92.91 95.56 # average of four above
\end{lstlisting}

As we can see, OpenOOD allows for very easy evaluation of new methods. Furthermore, it allows for easy comparisons between methods, one of the stated goals of the framework \cite{openood}. This makes it an ideal framework for this thesis.


\subsubsection{Modifications to OpenOOD}

As explained previously, continually testing new methods on the same benchmarks will bias the final results. As of the writing of this thesis, there are no functionalities in OpenOOD which allow for creating development or testing benchmarks; all evaluations are done with entire datasets. For my purposes, which involve continuous exploration of different methods and careful inspection of the datasets, this is inadequate. Thus, I have made modifications to the \texttt{Evaluator} class such that it takes a \texttt{data\_split} parameter during initialization, and have also modified the function \texttt{get\_id\_ood\_dataloader} to accept this parameter and return the correct split accordingly.

Furthermore, there is no functionality for sampling from the datasets as opposed to using them as they are, which is necessary to perform bootstrapping and calculate the statistical significance of the results. This has been done by passing a seed to the \texttt{Evaluator} class, which, if defined, will be used to seed a random sampling operation on the datasets. By instantiating several \texttt{Evaluator} classes with different seeds, we can bootstrap the evaluation and perform statistical analysis on the results.

\subsection{Implementation of Saliency Methods}

There are many libraries which implement \ac{xai} methods, such as \cite{lime, captum, jacobgilpytorchcam}. When these implementations were suitable for my purposes, I have used them. However, given that I am not using these explanations for their original purpose (elucidating why a model came to a specific decision), there are many cases where the current implementations are inadequate. The two main problems are lack of access to the raw saliency values and slow speeds.

\subsubsection{\ac{lime}}

\ac{lime} has an implementation for Python, written by the original authors \cite{lime}. However, this implementation is not suitable for my purposes. The main issue is that it is far too slow, being implemented with \texttt{NumPy} which restricts the computation to the CPU. Furthermore, \cite{lime} also returns full size heatmaps, although the actual number of saliency values used to create this heatmap is far smaller than the number of pixels in the image. Thus I have implemented \ac{lime} myself using PyTorch.

\subsubsection{Occlusion}

Captum \cite{captum} is a library of \ac{xai} methods implemented in PyTorch, and this library contains a suitable implementation of occlusion. As explained previously, occlusion occludes parts of the image and compares the prediction scores before and after the occlusion. By occluding all parts of the image, we can get a saliency value for all positions. Occlusion is usually done using a sliding window, similar to a convolutional kernel, which is slid over all parts of the image. Such a window is rarely a single pixel, because it is often not interesting to see how a single pixel contributes to a prediction, but rather a larger region. What this means is that the final heatmap, although it is the same size as the input image, actually contains far fewer unique values. To avoid performing computations on thousands of repeated values, I reduce the size of the heatmap by sampling one pixel from each of the positions the sliding window has been applied, which can be efficiently done using a $1 \times 1$ MaxPooling kernel with a stride equal to the stride used during the occlusion.

\subsubsection{\ac{gradcam}}

\ac{gradcam} has been implemented from scratch in \texttt{PyTorch}. There are several libraries which implement \ac{gradcam} \cite{jacobgilpytorchcam, captum}. However, given that these libraries are concerned with simply producing heatmaps that users can inspect, they do not output the raw saliency values, but upscale the saliencies to match the input image dimensions. As explained in \ref{section:gradcam}, \ac{gradcam} uses the final feature map to generate an explanation, which usually has a spatial dimension which is far smaller than the input image (e.g. $7 \times 7$ versus $224 \times 224$). When overlaying these values on the input image, it is common to use bilinear interpolation \cite{jacobgilpytorchcam}, which interpolates all $224 \times 224$ positions based on the original $7 \times 7$ saliency map. For visualizations, this is reasonable. When attempting to use this data to separate \ac{id} and \ac{ood} data however, this is undesirable. Bilinear interpolation introduces new values, which changes many statistical qualities of the saliency values. This may reduce the separability of different samples. Furthermore, given that these new values do not add any new information, it is inefficient to involve these upscaled saliency maps in any computational operation.

Although it is relatively simple to modify the source code of these libraries to remove the interpolation, \ac{gradcam} is not very difficult to implement, and as such I have simply used my own implementation.

\subsubsection{Integrated Gradients and Guided Backpropagation}

Captum also contains a suitable implementation for integrated gradients and guided backpropagation, which I have utilized in essentially unmodified forms. By definition (see section \ref{section:guidebackpropagation}), these methods return saliency maps over the entire input image dimensions. This separates them from the other methods, which return a far lower number of distinct saliencies, which are upscaled or transformed to the entire image (\ac{lime} and Occlusion output the same values for all pixels within a segment, \ac{gradcam} outputs an amount of saliencies corresponding to the final feature map, which is then upscaled). Because of this, the saliencies returned are exactly equivalent to the raw values I require, and no modifications are necessary.

However, this lack of dimensionality reduction also poses a technical challenge when we wish to store these saliency maps for later data analysis. For example, storing all saliencies, without dimensionality reduction, for ImageNet200 and its associated \ac{ood} datasets would require 28 GB. Thus, instead of storing the saliencies themselves, I calculate the aggregate values during saliency generation and store them instead. This has the downside that if a new form of aggregation is to be tested, the whole dataset of saliencies has to be generated again. However, the decrease in storage requirements is immense, being between a 1000 and 10 000 times decrease depending on the number of aggregates stored. This is strictly an implementation detail and has no effect on any analysis or evaluation of methods.

\section{Summary}

In this chapter, I have introduced the methodology that has been used in this thesis. I have described the three proposed \ac{xai} \ac{ood} detection frameworks that will be evaluated in chapter \ref{chapter:experiments}. In addition, I have described the four OpenOOD benchmarks and the datasets which constitute them, as well as the testing environment and the choice of evaluation metrics. Finally, I have described the implementation details of my work.

\chapter{Experiments and Results} \label{chapter:experiments}

This chapter contains the results the of experiments described in chapter \ref{chapter:methodology}. The chapter is split into two main sections, corresponding to the two parts mentioned in section \ref{section:evaluation}: In section \ref{section:saliencyagg}, I show how different forms of aggregation over saliency maps separate \ac{id} and \ac{ood} data points. The results from this part inform the choice of aggregation used in the Saliency Aggregation and Saliency Aggregation plus Logit \ac{ood} detection frameworks. This is done on the validation sets, to avoid biasing the final results.

Section \ref{section:results} contains the results of applying methods from the three frameworks introduced in section \ref{section:methods} on the four testing benchmarks contained in OpenOOD. For Saliency Aggregation and Saliency Aggregation plus Logit, each of the five \ac{xai} methods are tested in combination with the best performing aggregation, informed by the findings from section \ref{section:saliencyagg}. For SaliencyVIM, the testing is done using the three \ac{xai} methods which are compatible with the methodology as described in section \ref{section:saliencyvim_method}. The \ac{auroc} for each method is calculated over ten bootstraps for each benchmark, enabling statistical analyses which investigate whether \ac{xai} \ac{ood} detection methods outperform baseline methods. For each framework, the results are first presented for all four benchmarks in an objective manner, after which the performance of the framework is analyzed and discussed.

\section{Data Analysis of Saliency Maps} \label{section:saliencyagg}

This section will detail how various \ac{xai} methods generate explanations which differ between \ac{id} and \ac{ood}. This is done by generating \ac{xai} explanations using the five methods described in section \ref{section:xaimethodsbackground}, and then using different aggregations to inspect statistical qualities of the generated explanations. The section considers each validation benchmark individually. For each benchmark, I first present the level of separation achieved by the two baseline methods \ac{msp} and \ac{mls}, to give a general intuition about how easily the \ac{id} and \ac{ood} datasets are to separate. Following this, I go through the different \ac{xai} methods and their different statistical qualities.

Given the simplicity of my baseline Saliency Aggregation framework, which simply uses such aggregations to perform \ac{ood} detection, this section also serves as a form of hyperparameter search. Based on the findings of this chapter, a suitable choice of aggregations are chosen for the methods tested in section \ref{section:results}. These aggregations will also be used for methods under the Saliency Aggregation plus Logit framework.

\subsection{ImageNet200}

The ImageNet200 benchmark is a suitable benchmark to begin with, given ImageNet's ubiquity in \ac{ai} research. Figure \ref{fig:imagenet200logits} shows the distribution of the maximum softmax score and the maximum logit. Here, we see that there is a decent amount of separation between \ac{id} and \ac{ood} data points, even when simply using the maximum softmax score or the maximum logit, as introduced by \cite{oodbaseline} and \cite{mls}. Separating the distributions using the \ac{msp}, we get an \ac{auroc} score of 0.834 for Near-\ac{ood} and 0.915 for Far-\ac{ood}. Using the \ac{mls}, we get an \ac{auroc} score of 0.833 for Near-\ac{ood} and 0.903 for Far-\ac{ood}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_logits_distribution.pgf}
    \end{center}
    \caption{Density plot of the maximum softmax probability and maximum logit score on ImageNet200}
    \label{fig:imagenet200logits}
\end{figure}

\subsubsection{\ac{lime}}

With the baselines reported, we turn our attention to the first \ac{xai} saliency method, \ac{lime}. Figure \ref{fig:imagenet200limemeangini} presents graphs representing the two main forms of aggregation that has been applied; those which consider the magnitude of the saliencies and those which consider the statistical spread. These two forms are here represented by the vector norm and the \ac{rmd}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_lime_triple_metrics.pgf}
    \end{center}
    \caption{Vector norm and \acs*{rmd} density plots for \acs*{lime} on ImageNet200}
    \label{fig:imagenet200limemeangini}
\end{figure}

From this figure \ref{fig:imagenet200limemeangini}, we can see that for \ac{lime} on ImageNet200, it is indeed the case that saliencies are higher for \ac{id} data points than for \ac{ood} data points. Measuring the statistical spread using \ac{rmd}, we find that there also seems to be a higher spread in \ac{id} data when compared to \ac{ood} data. However, in this case, the overlap looks to be substantial.

To glean further insights, we may look to table \ref{table:imagenet200_lime_metrics}. This table shows the \ac{auroc} scores for all forms of aggregation on \ac{lime}, as well as the previously mentioned \ac{auroc} scores of the baseline methods. The \ac{auroc} scores are reported as percentages, to avoid having a redundant leading zero in all cells. In addition to the \ac{auroc}, the correlation between the aggregates and the maximum logit and maximum softmax score is reported, which gives insight into how \ac{xai} saliency maps are related to the prediction confidence of the model.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV$\downarrow$ & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 & 78.8 & 69.3 &\textbf{ 81.4 }& 77.1 & 78.1 & 78.0 & 53.7 & 59.4 & 53.2  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 88.1 & 75.2 &\textbf{ 92.5 }& 90.2 & 91.4 & 87.4 & 49.3 & 69.5 & 49.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.75 & 0.60 & 0.71 & 0.45 & 0.54 & 0.72 & -0.01 & 0.08 & -0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.61 & 0.48 & 0.61 & 0.41 & 0.48 & 0.60 & -0.01 & 0.09 & -0.01  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{lime} on ImageNet200]{\ac{auroc} scores for \ac{lime} on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_lime_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}


From this table, we can note several interesting observations.\footnote{As we can see, an average \ac{auroc} score of less than 0.50 was attained in some cases on Far-\ac{ood} datasets. This might seem surprising, given the description given in section \ref{section:aurocfpr95}. However, it is possible to receive \ac{auroc} values below 0.50 if the choice to negate the values was made based on values from another dataset. In this case, scores are negated if the Near-\ac{ood} aggregations are lower than the \ac{id} aggregations. If the Far-\ac{ood} aggregations are higher on average, this negation will lead to an \ac{auroc} score which is lower than 0.50} Firstly, we see that the magnitude based aggregations are all able to separate \ac{id} and \ac{ood} quite well, with vector norm in this case being the most discriminatory. In fact, the vector norm achieves a higher degree of separation than the baseline metrics on Far-\ac{ood}, with an \ac{auroc} of 0.925, compared to 0.915 and 0.903 for \ac{mls} and \ac{msp}, respectively. In contrast, the methods based on statistical dispersion separate \ac{id} and \ac{ood} poorly, putting into question the idea that the saliency maps of \ac{id} data could be more concentrated and less evenly spread out than those on \ac{ood} data. Indeed, we see that while the \ac{rmd} is higher on average for \ac{id} data, the \ac{qcd} and \ac{cv} is lower on average when generating saliencies using \ac{lime}. This further puts doubt on the idea that the spread of saliency maps is a reliable indicator of \ac{ood}-ness.

Secondly, the aggregates which capture information about the magnitude of the saliencies are highly correlated with both the maximum logit and the maximum softmax score of the predicted class. This is not unexpected, as \ac{lime} generates saliencies using differences in prediction values as different parts of the image are masked. If the predicted value is higher on average for \ac{id} data, then it is likely that the drop in predicted value when masking parts of the image is larger as well, leading to higher saliencies. Regardless, it seems clear that it is not only the correlation to the model output which explains the discriminative power of these aggregates, as we see that vector norm aggregation has lower correlation with \ac{mls} and equal correlation with \ac{msp} when compared to the mean, but achieves higher degrees of separation.

In general, the results from these aggregations are promising, and show that there is definite potential for \ac{ood} detection algorithms based on \ac{xai} outputs. However, the reader should note that these results are done on the validation set, and that no statistical tests have been done at this point. The statistical significance of using \ac{xai} saliency maps for \ac{ood} detection will be revealed in section \ref{section:results}, when methods under the frameworks introduced in chapter \ref{chapter:methodology} are developed based on the results reported here.

\subsubsection{Occlusion}

Now, we turn our attention to occlusion saliency mapping. Looking at figure \ref{fig:imagenet200_occlusion_mean_rmd}, we see that there is far more overlap between the \ac{id} and \ac{ood} densities than with \ac{lime}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_occlusion_triple_metrics.pgf}
    \end{center}
    \caption[Density plots of Norm and RMD for occlusion on ImageNet200]{Density plots of Norm and RMD for occlusion on ImageNet200}
    \label{fig:imagenet200_occlusion_mean_rmd}
\end{figure}

From table \ref{table:cifar10_occlusion_metrics} we see that this trend is apparent over all of the different forms of aggregation, not just the vector norm and \ac{rmd}. Regardless, there is still a clear trend \ac{id} saliency magnitudes being higher than \ac{ood} magnitudes, as all \ac{auroc} scores are above 0.50 without negation. With this saliency method, aggregating using range achieves the highest separability the best, with max relatively close behind. Interestingly, we see that for occlusion, all the statistical dispersion aggregates are higher for \ac{id} data, as was the original hypothesis. Regardless, the \ac{auroc} scores are very low, and thus these metrics do not seem suitable to discriminate between \ac{id} and \ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 & 58.6 & 54.2 & 65.7 &\textbf{ 69.1 }& 66.8 & 60.4 & 55.0 & 57.7 & 55.4  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 69.7 & 62.9 & 77.8 &\textbf{ 84.6 }& 83.5 & 72.3 & 63.8 & 64.7 & 62.7  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.37 & 0.31 & 0.42 & 0.44 & 0.47 & 0.41 & 0.01 & 0.14 & 0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.29 & 0.24 & 0.37 & 0.41 & 0.41 & 0.34 & 0.01 & 0.13 & 0.01  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for Occlusion on ImageNet200]{\ac{auroc} scores for Occlusion on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_occlusion_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac{gradcam}}

After having inspected two completely model independent \ac{xai} methods, we turn our attention to the first of the three gradient based methods, \ac{gradcam}. As we have seen density plots for both \ac{lime} and occlusion, I will omit these here, as the table contains the necessary information. From table \ref{table:cifar10_gradcam_metrics}, we see that vector norm aggregation of \ac{gradcam} saliencies actually achieves higher separation than the baseline metrics on both Near- and Far-\ac{ood} detection. The increase on Far-\ac{ood} is particularly substantial, at 1.4 percentage points. However, we should keep in mind that these results are done on the validation set and that the final results and their statistical significance will only be explored in section \ref{section:results}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 & 83.3 & 80.3 &\textbf{ 83.8 }& 80.5 & 81.9 & 83.5 & 50.8 & 51.8 & 51.7  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 91.5 & 87.6 &\textbf{ 92.9 }& 92.2 & 92.8 & 92.7 & 63.6 & 64.9 & 64.8  \\
    \hline
    Correlation with \ac{mls}& - & - & 1.00 & 0.94 & 0.97 & 0.69 & 0.81 & 0.96 & -0.16 & -0.12 & -0.12  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.83 & 0.78 & 0.82 & 0.62 & 0.70 & 0.81 & -0.11 & -0.07 & -0.07  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{gradcam} on ImageNet200]{\ac{auroc} scores for \ac{gradcam} on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_gradcam_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

As was expected, the mean value of the \ac{gradcam} saliencies is completely correlated with the maximum logit. As proven in section \ref{section:relation}, performing mean aggregation on \ac{gradcam} saliency maps created from the final convolutional layer of a ResNet network are equal to the maximum logit divided by the spatial size of the feature map, hence the total correlation. In general, the correlations with the baseline scores are far higher here than with the previous two \ac{xai} methods.

\subsubsection{Integrated Gradients}

Looking at table \ref{table:imagenet200_integratedgradients_metrics}, we see that the trend of larger saliency magnitudes on \ac{id} data continues to hold for integrated gradients as well. With integrated gradients the mean and the vector norm seem to be the most discriminative, with the mean saliency having scores which are around 1 percentage point below the baseline metrics.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV$\downarrow$ & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 &\textbf{ 82.1 }& 55.3 & 67.3 & 63.9 & 63.5 & 64.1 & 66.1 & 51.3 & 50.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 &\textbf{ 90.5 }& 56.0 & 87.8 & 86.7 & 85.9 & 79.6 & 49.8 & 39.1 & 53.4  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.94 & 0.14 & 0.40 & 0.31 & 0.30 & 0.35 & -0.16 & 0.01 & 0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.79 & 0.10 & 0.36 & 0.29 & 0.28 & 0.31 & -0.15 & 0.00 & 0.00  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for IntegratedGradients on ImageNet200]{\ac{auroc} scores for IntegratedGradients on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_integratedgradients_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{\ac*{gbp}}

We now turn our attention to the final \ac{xai} saliency method, \ac{gbp}. Here we see something interesting; as the only method on ImageNet200, \ac{gbp} actually has two magnitude metrics where \ac{ood} data points have higher values than \ac{id} data points. Looking at table \ref{table:imagenet200_gbp_metrics}, we see that both the mean and median saliency is lower on \ac{id} data. Moreover, the mean is considerably lower, with an \ac{auroc} of 0.737 on Near-\ac{ood} and 0.817 on Far-\ac{ood} when choosing lower values as \ac{id} as opposed to higher.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean$\downarrow$ & Median$\downarrow$ & Norm & Range & Max & Q3 & CV & RMD$\downarrow$ & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 & 73.7 & 55.7 &\textbf{ 77.4 }& 71.3 & 71.8 & 71.0 & 50.2 & 52.8 & 51.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 81.7 & 51.1 &\textbf{ 92.3 }& 90.7 & 90.6 & 68.7 & 43.5 & 72.3 & 50.7  \\
    \hline
    Correlation with \ac{mls}& - & - & -0.33 & -0.08 & 0.45 & 0.25 & 0.25 & 0.36 & 0.00 & -0.01 & 0.01  \\
    \hline
    Correlation with \ac{msp}& - & - & -0.30 & -0.08 & 0.42 & 0.25 & 0.24 & 0.35 & 0.00 & -0.00 & 0.01  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{gbp} on ImageNet200]{\ac{auroc} scores for \ac{gbp} on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_gbp_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

To investigate this further, we may plot the densities of the mean value for each dataset in the ImageNet200 benchmark. Looking at figure \ref{fig:imagenet200_gbp_mean_full}, we see that the mean saliency value is indeed lower for \ac{id} data when compared to all the \ac{ood} data sets, as opposed to being lower than some \ac{ood} datasets and higher than others.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_gbp_Mean_full.pgf}
    \end{center}
    \caption[Density plot of mean saliency for GBP on ImageNet200]{Density plot of the mean saliency for GBP for all datasets in the ImageNet200 benchmark}
    \label{fig:imagenet200_gbp_mean_full}
\end{figure}

The vector norm, range and maximum aggregations are still higher for \ac{id} data. These aggregations are the ones which are not affected by large negative values. The conclusion to draw from these results is as follows: For \ac{gbp}, \ac{id} \ac{xai} saliency maps exhibit higher magnitudes, but they are not restricted to positive attributions.

\subsubsection{Overall results on ImageNet200}

Next, we consider the overall discriminatory power of the different aggregations and \ac{xai} saliency methods on ImageNet200. Figure \ref{fig:imagenet200_all_metrics_barplot} shows the average Near- and Far-\ac{auroc} for magnitude aggregations. From this, we see that the vector norm, range and maximum aggregations performed the best over all \ac{xai} saliency methods. All statistical dispersion aggregations performed poorly, and as such I do not plot them. In general, saliency aggregation seem to enable far higher degrees of separability than those reported by \cite{martinez}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_all_metrics_barplot.pgf}
    \end{center}
    \caption[Average AUROC scores for magnitude aggregations on ImageNet200]{Barplot of average \ac{auroc} scores for each metric on ImageNet200. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet200_all_metrics_barplot}
\end{figure}


Table \ref{table:imagenet200_all_metrics} shows the underlying data for the figure above, as well as the mean \ac{auroc} for both Near and Far. Here, we can see that the mean performed the best on Near-\ac{ood} while the range performed the best on Far-\ac{ood}. However, as we know, the mean was lower for \ac{gbp} and lower scores had to be considered \ac{id}, which makes it less desirable than the other aggregations, which were consistently larger for \ac{id} data. The vector norm was only slightly worse at separating \ac{id} and \ac{ood} than the mean on Near-\ac{ood} and only slightly worse than the range on Far-\ac{ood}, making it the best aggregation overall. As we can see, all the statistical dispersion aggregations showed poor discriminatory power.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 83.3 & 83.4 &\textbf{ 75.3 }& 63.0 & 75.1 & 72.4 & 72.4 & 71.4 & 55.2 & 54.6 & 52.5  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.5 & 90.3 & 84.3 & 66.6 & 88.6 &\textbf{ 88.9 }& 88.8 & 80.1 & 54.0 & 62.1 & 56.3  \\
    \hline
    Mean \ac{auroc} & 87.4 & 86.9 & 79.8 & 64.8 &\textbf{ 81.9 }& 80.6 & 80.6 & 75.7 & 54.6 & 58.4 & 54.4  \\
    \hline
    \end{tabular}
    \caption[Average \ac{auroc} scores over all \ac{xai} saliency methods on ImageNet200]{Average \ac{auroc} scores for all \ac{xai} saliency methods on ImageNet200. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:imagenet200_all_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

If we instead look at the most discriminatory aggregation for each \ac{xai} saliency method (figure \ref{fig:imagenet200_all_generators_barplot}), and compare these to the baseline metrics, we find that \ac{lime}, \ac{gradcam} and \ac{gbp} achieve higher degrees of separation than the baselines on Far-\ac{ood}, while \ac{gradcam} is the only method who has a higher degree of separation than the baselines on Near-\ac{ood}. Except for occlusion, all \ac{xai} saliency maps contains information which enable us to discriminate between \ac{id} and \ac{ood} reasonably well, given a correct choice of aggregation. Despite having lower scores overall, aggregating occlusion saliencies still gives \ac{auroc} values which show some discriminatory power, and could still be valuable when used in frameworks which combine saliency aggregation with other information from the network.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_all_generators_barplot.pgf}
    \end{center}
    \caption[Highest AUROC score for each XAI saliency method on ImageNet200]{Barplot of the highest \ac{auroc} scores per XAI saliency method on ImageNet200. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet200_all_generators_barplot}
\end{figure}

Finally, figure \ref{fig:imagenet200_heatmap} shows a heatmap of the average performance over both Near-\ac{ood} and Far-\ac{ood} for all combinations of aggregation and \ac{xai} saliency method. From this we can get a visual overview over what combinations achieve the highest degree of separation between \ac{id} and \ac{ood}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_heatmap.pgf}
    \end{center}
    \caption[Overall performance on ImageNet200]{Heatmap of overall performance \ac{auroc} performance for all combinations of \ac{xai} methods and magnitude aggregations}
    \label{fig:imagenet200_heatmap}
\end{figure}

From this, we can see that \ac{gradcam} separates \ac{id} and \ac{ood} well overall (likely due to its high correlation with \ac{mls}) while occlusion performs poorly, as we saw previously. Apart from this, we see that some promising combinations are \ac{lime}+Norm, \ac{gbp}+Norm and IG+Mean.

The results above demonstrate that saliency maps themselves, without any other information from the network, seem to be able to adequately separate \ac{id} from \ac{ood} data points, contrary to what was found by \cite{martinez}. In the next section, I will explore whether these results hold on CIFAR10 as well. After these tests, we will have an informed opinion about what \ac{xai} saliency methods and aggregations best separate the data on the validation set, and we can choose a selection of combinations and gather final results on the testing benchmarks.

\subsection{CIFAR10}

CIFAR10 differs from ImageNet200 in some important respects, making it an ideal second benchmark to investigate. While ImageNet200 has 200 classes, CIFAR10 has only 10. This means that ImageNet200 is a much broader classification task, with networks that are trained to recognize a wide variety of different objects. With a more narrow \ac{id} dataset of only 10 classes, we may see different behaviour when a network is exposed to \ac{ood} data. Another important change is the size of the images. ImageNet200 images are $224 \times 224$ pixels, while CIFAR10 images are only $32 \times 32$. This reduction in resolution may also affect how saliency maps are generated, as each pixel now covers a much larger area of the total image and is thus more important compared to each pixel in ImageNet200 images. First, let us inspect the baseline metrics.

From figure \ref{fig:cifar10_logits_distribution}, we can see that there is a decent degree of separation when using the maximum logit and the maximum softmax score. The maximum softmax score is highly concentrated around 0.95-1.00 for \ac{id} data, to a much higher degree than with ImageNet200. This is not surprising, given the much smaller number of classes in CIFAR10. With fewer and more easily separated classes, it is much easier for the network to saturate the maximum softmax score.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_logits_distribution.pgf}
    \end{center}
    \caption[CIFAR10 maximum logit and softmax score distribution]{Density plot of the maximum logit and softmax score on CIFAR10}
    \label{fig:cifar10_logits_distribution}
\end{figure}

Separating the distributions using \ac{msp}, we get an \ac{auroc} score of 0.877 for Near-\ac{ood} and 0.908 for Far-\ac{ood}. Using \ac{mls}, we get an \ac{auroc} score of 0.867 for Near-\ac{ood} and 0.914 for Far-\ac{ood}. With the baselines reported, we turn our attention to the first \ac{xai} saliency method, \ac{lime}.

\subsubsection{\ac{lime}}

Table \ref{table:cifar10_lime_metrics} shows the results for the aggregations on the saliency maps generated by \ac{lime}. From this table, we see a similar trend as when we applied \ac{lime} to ImageNet200: the magnitude of saliency methods all show a clear trend of higher values on \ac{id} data, while the statistical dispersion methods are poor and uninformative. However, the degree of separation compared to the baselines is worse on CIFAR10, with over 5 percentage points lower scores on both Near- and Far-\ac{ood}. This is far worse when considering that \ac{lime} actually achieved a better \ac{auroc} than the baseline metrics on ImageNet200 when using vector norm aggregation. It may be that because of the lower resolution of the images, each occluded region used to generate \ac{lime} explanations carries less information, which introduces some instability in when generating explanations.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV$\downarrow$ & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 & 87.7 &\textbf{ 81.2 }& 73.0 & 77.7 & 67.1 & 76.1 & 76.6 & 63.6 & 61.0 & 59.0  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 &\textbf{ 86.1 }& 77.9 & 84.0 & 74.5 & 81.9 & 83.9 & 59.0 & 61.2 & 53.2  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.40 & 0.28 & 0.34 & 0.22 & 0.31 & 0.37 & -0.00 & 0.11 & -0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.29 & 0.20 & 0.23 & 0.15 & 0.22 & 0.26 & 0.01 & 0.08 & -0.00  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{lime} on CIFAR10]{\ac{auroc} scores for \ac{lime} on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_lime_metrics}
\end{center}

\setlength\tabcolsep{6pt}
\end{table}

Regardless of the lower scores when compared to ImageNet200, the scores still show that the magnitudes of \ac{xai} saliency maps differ between \ac{id} and \ac{ood} data when using \ac{lime} to generate them.

\subsubsection{Occlusion}

Table \ref{table:cifar10_occlusion_metrics} shows the results of using occlusion to generate saliency maps. Here we see something quite surprising: all magnitude metrics are lower for \ac{id} data. This result forces us to reconsider the theory that the magnitude of saliencies is higher on average for \ac{id} data, as we see a complete inversion of the results we got when using occlusion on ImageNet200. % Furthermore, the results here are quite poor, over 10 percentage points below the Near-\ac{ood} \ac{auroc} baseline and over 15 percentage points below on Far-\ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean$\downarrow$ & Median$\downarrow$ & Norm$\downarrow$ & Range$\downarrow$ & Max$\downarrow$ & Q3$\downarrow$ & CV & RMD & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 & 87.7 & 63.4 & 63.1 &\textbf{ 76.6 }& 74.2 & 67.3 & 75.5 & 52.4 & 54.7 & 50.3  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 & 61.4 & 63.1 &\textbf{ 74.6 }& 71.6 & 63.8 & 74.6 & 51.8 & 57.2 & 51.4  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.22 & 0.11 & 0.13 & 0.05 & 0.27 & 0.13 & 0.00 & 0.34 & 0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.22 & 0.12 & 0.14 & 0.07 & 0.25 & 0.15 & 0.00 & 0.26 & 0.00  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for Occlusion on CIFAR10]{\ac{auroc} scores for Occlusion on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_occlusion_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

Looking at the distribution for the vector norm (figure \ref{fig:cifar10_occlusion_norm}), we find a consistently lower \ac{id} value when compared to all \ac{ood} datasets. Essentially the same plot can be seen with the other magnitude metrics as well.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_occlusion_Norm_.pgf}
    \end{center}
    \caption[CIFAR10 occlusion vector norm density plot]{Density plot of the vector norm of occlusion saliencies for all datasets in the CIFAR10 benchmark}
    \label{fig:cifar10_occlusion_norm}
\end{figure}

These results are very interesting, as they show that there is no guarantee that \ac{xai} saliency maps will be of higher magnitude on \ac{id} data. Even more interesting is the fact that when they are lower, they are lower across all \ac{ood} datasets and still allow for separation between \ac{id} and \ac{ood} by considering low saliency magnitudes as \ac{id} as opposed to high saliency magnitudes. These results concur with the second hypothetical scenario given in section \ref{section:saliencyagg_method}.

\subsubsection{\ac{gradcam}}

Saliency maps made from \ac{gradcam}, with their mathematical equivalence to the maximum logit when applying the mean, unsurprisingly do not suffer from the same problems as occlusion. As we can see from table \ref{table:cifar10_gradcam_metrics}, the results for mean are equivalent to the maximum logit.\footnote{The 0.1 difference between \ac{mls} and mean aggregation on Near-\ac{ood} is most likely due to floating point imprecision}

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm & Range & Max & Q3 & CV$\downarrow$ & RMD$\downarrow$ & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 & 87.7 &\textbf{ 86.8 }& 85.9 & 86.7 & 71.8 & 86.5 & 85.2 & 75.3 & 76.7 & 74.2  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 &\textbf{ 91.4 }& 91.2 & 91.4 & 80.7 & 91.2 & 90.6 & 78.8 & 80.1 & 74.5  \\
    \hline
    Correlation with \ac{mls}& - & - & 1.00 & 0.99 & 1.00 & 0.68 & 0.96 & 0.98 & -0.47 & -0.48 & -0.45  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.79 & 0.78 & 0.79 & 0.57 & 0.77 & 0.78 & -0.42 & -0.42 & -0.41  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{gradcam} on CIFAR10]{\ac{auroc} scores for \ac{gradcam} on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_gradcam_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Integrated Gradients}

With integrated gradients we see another surprising result. Here, the mean and median are higher for \ac{id} data, while the vector norm, range, maximum and third quartile are lower.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean & Median & Norm$\downarrow$ & Range$\downarrow$ & Max$\downarrow$ & Q3$\downarrow$ & CV$\downarrow$ & RMD$\downarrow$ & QCD$\downarrow$  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 & 87.7 & 83.1 & 59.2 & 78.6 & 76.0 & 74.8 & 78.4 &\textbf{ 84.6 }& 51.0 & 52.7  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 &\textbf{ 88.4 }& 57.9 & 68.6 & 65.1 & 64.0 & 70.7 & 80.0 & 57.4 & 51.0  \\
    \hline
    Correlation with \ac{mls}& - & - & 0.65 & 0.08 & -0.10 & -0.07 & -0.06 & -0.11 & -0.03 & -0.01 & 0.00  \\
    \hline
    Correlation with \ac{msp}& - & - & 0.50 & 0.06 & -0.07 & -0.05 & -0.05 & -0.06 & -0.02 & 0.01 & -0.00  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for IntegratedGradients on CIFAR10]{\ac{auroc} scores for IntegratedGradients on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_integratedgradients_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

This implies that \ac{id} data has higher positive saliencies on average than \ac{ood} data, but that the magnitude of both positive and negative saliencies is higher on \ac{ood} data. Looking at figure \ref{fig:cifar10_integratedgradients_mean_norm}, we see this difference clearly.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_integratedgradients_Mean_Norm.pgf}
    \end{center}
    \caption[CIFAR10 mean and norm density plots for Integrated Gradients]{Mean and vector norm density plots for Integrated Gradients saliencies.}
    \label{fig:cifar10_integratedgradients_mean_norm}
\end{figure}

\subsubsection{\ac*{gbp}}

\ac{gbp} saliency maps separate \ac{id} and \ac{ood} data decently, with the best saliency aggregation (vector norm) achieving separability scores which are a few percentage points below the baseline metrics. The only surprise here is that the mean is lower on average for \ac{id} data, which it also was when applying \ac{gbp} to ImageNet200.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{2}{c|}{Baselines} & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & \ac{mls} & \ac{msp} & Mean$\downarrow$ & Median & Norm & Range & Max & Q3 & CV$\downarrow$ & RMD$\downarrow$ & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 86.7 & 87.7 & 56.0 & 58.6 &\textbf{ 83.4 }& 81.2 & 80.6 & 65.5 & 53.1 & 64.6 & 51.8  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 91.4 & 90.8 & 50.0 & 70.6 &\textbf{ 89.6 }& 88.7 & 87.5 & 79.1 & 61.6 & 69.2 & 60.9  \\
    \hline
    Correlation with \ac{mls}& - & - & -0.11 & 0.03 & 0.30 & 0.22 & 0.22 & 0.18 & -0.04 & -0.03 & -0.02  \\
    \hline
    Correlation with \ac{msp}& - & - & -0.08 & 0.01 & 0.21 & 0.15 & 0.15 & 0.11 & -0.02 & -0.02 & -0.01  \\
    \hline
    \end{tabular}
    \caption[\ac{auroc} scores for \ac{gbp} on CIFAR10]{\ac{auroc} scores for \ac{gbp} on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. $\downarrow$ denotes that \ac{id} data points more often have a lower score with this aggregation, and thus the output values have been negated (as described in section \ref{section:aurocfpr95})}
    \label{table:cifar10_gbp_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Overall results on CIFAR10} \label{section:salagg_val_cifar10}

Figure \ref{fig:cifar10_all_metrics_barplot} shows the average scores for each aggregation over the 5 different \ac{xai} saliency methods. From this we see that again, vector norm has the highest degree of separation overall, and occlusion the worst. 

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_all_metrics_barplot.pgf}
    \end{center}
    \caption[Average AUROC scores for magnitude aggregations on CIFAR10]{Barplot of average \ac{auroc} scores for each metric on CIFAR10. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar10_all_metrics_barplot}
\end{figure}

Table \ref{table:cifar10_all_metrics} shows the results in more detail. Here, we see that not only is the vector norm the most discriminative when averaging across Near and Far, but also individually on each of the categories as well.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |p{5.1em}|c c c c c c|c c c| }
    \hline
     \centering Aggregation type & \multicolumn{6}{c|}{Magnitude of saliencies} & \multicolumn{3}{p{8em}|}{\centering Statistical dispersion} \\
    \hline
    Aggregate & Mean & Median & Norm & Range & Max & Q3 & CV & RMD & QCD  \\
    \hline
    \rowcolor{near!50}
    Near-\ac{ood} \ac{auroc} & 74.1 & 68.0 &\textbf{ 80.6 }& 74.1 & 77.1 & 76.2 & 65.8 & 61.6 & 57.6  \\
    \hline
    \rowcolor{far!50}
    Far-\ac{ood} \ac{auroc} & 75.4 & 72.1 &\textbf{ 81.6 }& 76.1 & 77.7 & 79.8 & 66.2 & 65.0 & 58.2  \\
    \hline
    Mean \ac{auroc} & 74.8 & 70.1 &\textbf{ 81.1 }& 75.1 & 77.4 & 78.0 & 66.0 & 63.3 & 57.9  \\
    \hline
    \end{tabular}
    \caption[Average \ac{auroc} scores over all \ac{xai} saliency methods on CIFAR10]{Average \ac{auroc} scores for all \ac{xai} saliency methods on CIFAR10. The highest non-baseline value for Near- and Far-\ac{ood} is highlighted in bold. }
    \label{table:cifar10_all_metrics}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

Figure \ref{fig:cifar10_all_generators_barplot} shows the results when we instead look at the most discriminative aggregation for each \ac{xai} saliency method. Here we see that, as with ImageNet200, \ac{gradcam}, integrated gradients and \ac{gbp} separate \ac{id} and \ac{ood} reasonably well, while occlusion does not. Unlike ImageNet200, \ac{lime} does not achieve high degrees of separation on CIFAR10, and lags behind the gradient based methods.

\begin{figure}[h]
    \begin{center}
        \input{figure/cifar10_all_generators_barplot.pgf}
    \end{center}
    \caption[Highest AUROC score for each XAI saliency method on CIFAR10]{Barplot of the highest \ac{auroc} scores per XAI saliency method on CIFAR10. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar10_all_generators_barplot}
\end{figure}

Finally, we can look at all combinations of aggregation and \ac{xai} method when we combine both Near- and Far-\ac{ood} separation. From figure \ref{fig:cifar10_heatmap}, we see that many of the combinations which separated \ac{id} and \ac{ood} well on ImageNet200 also do so on CIFAR10: the vector norm with \ac{gbp} and \ac{gradcam} and the mean with integrated gradients all perform very well. Contrasting with ImageNet200, we find that on CIFAR10, using mean aggregation is actually more discriminative than the vector norm on \ac{lime} saliencies. In addition, we also see that the mean leads to an every slightly higher score for \ac{gradcam}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_heatmap.pgf}
    \end{center}
    \caption[Overall performance on CIFAR10]{Heatmap of overall performance \ac{auroc} performance for all combinations of \ac{xai} methods and magnitude aggregations on CIFAR10}
    \label{fig:cifar10_heatmap}
\end{figure}

\subsection{Overall analysis of results}

From the results reported in this section, we have seen clearly that saliency maps carry discriminative information across a wide range of \ac{xai} methods. This information can be captured by using simple aggregations, with several aggregations based on magnitude achieving high degrees of separability. Aggregations which are based on calculating (magnitude-invariant) statistical dispersion, however, do not capture this information well, and it seems unlikely that \ac{id} and \ac{ood} saliency maps are distributed differently when we do not consider the magnitude.

As described previously, the frameworks introduced in chapter \ref{chapter:methodology} could be used with any of the combinations of \ac{xai} saliency mapping method and aggregation. To limit the number of methods, we choose the aggregation which achieves the highest degree of separability between \ac{id} and \ac{ood} data points for each \ac{xai} method. Figure \ref{fig:both_heatmap} shows the average degree of separation over both Near-\ac{ood} and Far-\ac{ood} for both ImageNet200 and CIFAR10 for all combinations of \ac{xai} method and magnitude aggregation. As stated previously, the statistical dispersion methods are omitted due to their low degree of separability.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/both_heatmap.pgf}
    \end{center}
    \caption[Overall performance on ImageNet200 and CIFAR10]{Heatmap of overall performance \ac{auroc} performance for all combinations of \ac{xai} methods and magnitude aggregations on ImageNet200 and CIFAR10}
    \label{fig:both_heatmap}
\end{figure}


From this figure, we can see that the most discriminative aggregation overall is vector norm for \ac{lime}, \ac{gradcam} and \ac{gbp}, range for occlusion, and mean for integrated gradients. In the next section, I will denote these combinations as \ac{lime}Norm, OcclusionRange, \ac{gradcam}Norm, IGMean and \ac{gbp}Norm.

\section{Evaluation of XAI OOD detectors} \label{section:results}

This section contains the final tests conducted on the testing benchmarks. This section will be divided into three parts, corresponding to the three proposed \ac{ood} detection frameworks introduced in the methodology: Saliency Aggregation, Saliency Aggregation plus Logit and SaliencyVIM. Each of these sections will detail the performance of the corresponding framework on the four testing benchmarks, and statistically compare the results to the baseline methods. For Saliency Aggregation and Saliency Aggregation plus Logit, the tests are conducted using the combinations of \ac{xai} methods and aggregations that performed the best on the validation benchmarks, as described in the preceding section. The statistical analysis is based on ten bootstraps of each benchmarks, which has been performed on each method, as well as the baselines. As mentioned in section \ref{section:ttest_methodology}, the Bonferroni-corrected threshold for statistical significance is set at $0.05 / n$, where $n$ is the number of experiments conducted on each method (5 for Saliency Aggregation and Saliency Aggregation plus Logit, 3 for SaliencyVIM).

When reporting the p-values, I follow the \texttt{R}-standard for appending "significance stars", which give a quick visual indication of the statistical significance of a result. In \texttt{R}, any p-value lower than 0.05 has an asterisk appended, any p-value lower than 0.01 has two asterisks appended, and any p-value lower than 0.001 has three asterisks appended. I also append these asterisks, but use Bonferroni corrected p-values when determining if a value should have an asterisk appended. This means that whenever a p-value has at least one asterisk appended, the reader knows that the corresponding Wilcoxon signed-rank test showed statistical significance according to a Bonferroni corrected level of significance of $0.05 / n$. It should be noted that due to the discrete nature of the Wilcoxon signed-rank test, there is a minimal p-value that can be attained, based on the number of experiments. With ten bootstraps, the minimal p-value is 0.001\footnote{In reality it is 0.000976, however I round p-values to four digits}, which is achieved when all baseline \ac{auroc} scores are lower than the method tested. This means that whenever a method achieves a p-value of 0.001, this method outperformed the baseline on all ten bootstraps.

For each experiment, I first report the results for each of the four benchmarks in an objective manner, by simply describing the scores attained by the different methods and comparing them to the baseline methods. At the end of each experiment, I give an analysis of the results and attempt to connect the results to the problem statement.

\subsection{Results for Saliency Aggregation} \label{section:salagg_results}

Saliency Aggregation, as described in \ref{section:saliencyagg_method}, is the first \ac{ood} detection framework that will be tested. As we have seen from the validation benchmarks, saliency aggregation on its own can sometimes perform on par with the baselines on Near-\ac{ood} and can sometimes surpass the baselines on Far-\ac{ood}. The Saliency Aggregation framework requires a choice of \ac{xai} saliency mapping method $s$ and aggregation function $A$. From the results in section \ref{section:saliencyagg}, the combinations I will use are \ac{lime}, \ac{gradcam} and \ac{gbp} with vector norm aggregation, and integrated gradients with mean aggregation.

In this section, the generation of saliency maps and aggregation will be done again, on ten bootstraps of the four testing benchmarks, and \ac{auroc} scores will be calculated for each bootstrap for each method.


\subsubsection{ImageNet200}

As in the preceding section, we first investigate the results on the ImageNet200 testing benchmark. As explained in section \ref{section:testing_environment}, this benchmark consists of held out samples from the ImageNet200 dataset and all corresponding \ac{ood} datasets, and thus allows us to calculate unbiased results. 

Figure \ref{fig:imagenet200_salagg_bootstrap_barplot} shows the mean \ac{auroc} for the baselines and the three combinations of \ac{xai} methods and aggregation functions mentioned above. In addition, the confidence intervals for each mean value is plotted as whiskers.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_salagg_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet200 Saliency Aggregation Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation on the bootstrapped ImageNet200 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet200_salagg_bootstrap_barplot}
\end{figure}

As we can see, the vector norm of \ac{lime}, \ac{gradcam} and \ac{gbp} actually seems to outperform the baselines on Far-\ac{ood}, while the mean of the integrated gradients is about on par with the Far-\ac{ood} performance of \ac{msp}. When it comes to Near-\ac{ood}, only the norm of \ac{gradcam} seems to compete with the baselines. OcclusionRange performs very poorly, as it did on the validation set.

To get a more precise understanding of the performance of the different models, we turn to table \ref{table:imagenet200_salagg_ttest}, which shows the results of the Wilcoxon signed-rank tests done against the baseline methods. From this table we see that the Far-\ac{ood} results for \ac{lime}Norm, \ac{gradcam}Norm and \ac{gbp}Norm were indeed statistically significantly higher than both baselines.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 80.95 & -1.792 & 1.000 & -2.329 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.39 & +1.113 & 0.001 ** & +2.195 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 69.38 & -13.361 & 1.000 & -13.898 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 85.23 & -6.051 & 1.000 & -4.969 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 83.26 & +0.511 & 0.001 ** & -0.026 & 0.839 \\
    \rowcolor{far!50}
    Far-OOD & 92.62 & +1.339 & 0.001 ** & +2.421 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 81.59 & -1.156 & 1.000 & -1.694 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 90.38 & -0.894 & 1.000 & +0.188 & 0.002 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 76.90 & -5.845 & 1.000 & -6.382 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.98 & +0.706 & 0.001 ** & +1.788 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for Saliency Aggregation on ImageNet200]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on ImageNet200, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet200_salagg_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{CIFAR10}

Next, we turn our attention to CIFAR10. Figure \ref{fig:cifar10_salagg_bootstrap_barplot} shows the bootstrapped means and the confidence intervals. Here, we can see that the results are in general worse than the baselines, mirroring the initial findings from the validation benchmark (section \ref{section:salagg_val_cifar10}). In this case, only \ac{gradcam}Norm can compete with the baseline methods.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_salagg_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR10 Saliency Aggregation Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation on the bootstrapped CIFAR10 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar10_salagg_bootstrap_barplot}
\end{figure}

From this plot, we do not expect to see many statistically significant improvements. Indeed, table \ref{table:cifar10_salagg_ttest} shows that no method outperforms both baselines on either Near- or Far-\ac{ood}, and in general all methods have a mean \ac{auroc} which is far lower than the baseline methods.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 76.67 & -10.294 & 1.000 & -11.008 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 83.74 & -8.078 & 1.000 & -7.490 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 73.75 & -13.221 & 1.000 & -13.935 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 71.13 & -20.690 & 1.000 & -20.102 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 86.86 & -0.108 & 1.000 & -0.822 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.78 & -0.039 & 1.000 & +0.549 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 83.33 & -3.642 & 1.000 & -4.357 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 88.99 & -2.832 & 1.000 & -2.244 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 83.77 & -3.197 & 1.000 & -3.912 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 89.92 & -1.898 & 1.000 & -1.310 & 1.000 \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for Saliency Aggregation on CIFAR10]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on CIFAR10, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar10_salagg_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{ImageNet1K}

Next, we look at the first benchmark which was not used during the data analysis, ImageNet1K. 

From figure \ref{fig:imagenet_salagg_bootstrap_barplot}, we see that three of the \ac{xai} saliency aggregation methods comfortably outperform the baselines on Far-\ac{ood}; \ac{lime}Norm, \ac{gradcam}Norm and \ac{gbp}Norm. On Near-\ac{ood}, as with ImageNet200, their performance is not competitive with the baselines.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet_salagg_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet1K Saliency Aggregation Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation on the bootstrapped ImageNet testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet_salagg_bootstrap_barplot}
\end{figure}

Looking at table \ref{table:imagenet_salagg_ttest}, the three methods mentioned above all report considerable \ac{auroc} improvements over both baselines on Far-\ac{ood}

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 68.84 & -7.492 & 1.000 & -7.199 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.24 & +1.721 & 0.001 ** & +6.225 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 58.65 & -17.676 & 1.000 & -17.384 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 77.43 & -12.080 & 1.000 & -7.576 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 76.15 & -0.185 & 0.999 & +0.107 & 0.065 \\
    \rowcolor{far!50}
    Far-OOD & 92.10 & +2.587 & 0.001 ** & +7.091 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 73.95 & -2.386 & 1.000 & -2.093 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 87.54 & -1.973 & 1.000 & +2.531 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 72.20 & -4.136 & 1.000 & -3.843 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.34 & +1.825 & 0.001 ** & +6.329 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for Saliency Aggregation on Imagenet]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on Imagenet, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet_salagg_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{CIFAR100}

Finally, we turn to CIFAR100, the second benchmark not used during the analysis. Figure \ref{fig:cifar100_salagg_bootstrap_barplot} shows the average \ac{auroc} scores on this benchmark. Like with CIFAR10, the results here are worse than on ImageNet, and \ac{lime}Norm again performs poorly.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar100_salagg_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR100 Saliency Aggregation Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation on the bootstrapped CIFAR100 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar100_salagg_bootstrap_barplot}
\end{figure}

From table \ref{table:cifar100_salagg_ttest}, we see that both \ac{gradcam}Norm and IGMean perform well on Far-\ac{ood}, while \ac{lime}Norm and \ac{gbp}Norm fall far behind, with double digit percentage point losses compared to the baselines.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 70.51 & -10.728 & 1.000 & -10.175 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 77.58 & -2.285 & 1.000 & -0.344 & 0.998 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 67.46 & -13.774 & 1.000 & -13.221 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 74.57 & -5.294 & 1.000 & -3.353 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 81.22 & -0.020 & 0.958 & +0.533 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 79.97 & +0.106 & 0.001 ** & +2.047 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 78.71 & -2.532 & 1.000 & -1.979 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 81.48 & +1.616 & 0.001 ** & +3.557 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 64.20 & -17.039 & 1.000 & -16.486 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 71.35 & -8.509 & 1.000 & -6.568 & 1.000 \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for Saliency Aggregation on CIFAR100]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on CIFAR100, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar100_salagg_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{Overall analysis of Saliency Aggregation}

% These first results show that \ac{xai} explanations can be used to perform \ac{ood} detection, and may actually beat baseline methods in certain settings. However, we see that on Near-\ac{ood}, the performance of Saliency Aggregation falls short, with inconsistent results across benchmarks. Despite the sub par results on Near-\ac{ood}, these results show that simple methods under the baseline Saliency Aggregation framework are able to discriminate between \ac{id} and \ac{ood} samples.

% As we can see, the performance of saliency aggregation on CIFAR10 is far less impressive than that on ImageNet200. These results agree with the findings from section \ref{section:saliencyagg}, which also found lower discriminative power of saliency aggregates on CIFAR10. However, the corresponding results on CIFAR10 from \cite{martinez} was an \ac{auroc} of 0.52 when using SVHN as the \ac{ood} dataset, meaning that all these methods vastly outperform this work, even if they do not beat the baseline methods.

% Concurring with the previous two benchmarks, ImageNet1K also shows a clear pattern of better performance on Far-\ac{ood} than Near-\ac{ood}.

% These results are quite similar with those found on CIFAR10, which is unsurprising, given that they utilize the same \ac{ood} datasets.

Overall, the results from simply aggregating the saliency maps of different \ac{xai} methods are very promising, especially considering that the only other work which has attempted to use \ac{xai} saliency maps for \ac{ood} detection barely achieved \ac{auroc} scores above 0.50. These results show that \ac{xai} methods extract valuable information from the network, which can be used to effectively discriminate between \ac{id} and \ac{ood} data samples. The results on ImageNet200 and ImageNet1K are particularly interesting, as \ac{lime}Norm, \ac{gradcam}Norm and \ac{gbp}Norm all outperform the baselines on Far-\ac{ood}. The results on CIFAR10 and CIFAR100 are less impressive, and show that these methods may not achieve consistently good results on all benchmarks. However, it should be noted that this inconsistency is not unusual amongst \ac{ood} detectors, as one of the takeaways from from \cite{openood15} was that there is "no single winner that always outperforms others across multiple data sets".

\subsection{Results for Saliency Aggregation plus Logit} \label{section:salagglogits_results}

Given the fact that Saliency Aggregation has shown itself to be capable of differentiating \ac{id} and \ac{ood} samples in many cases, especially for Far-\ac{ood}, we might expect good results from Saliency Aggregation plus Logit. As mentioned in section \ref{section:salpluslogitmethod}, \texttt{COMBOOD} \cite{combood} achieved \ac{sota} results on ImageNet200 and ImageNet1K when combining two complementary distance metrics. As we have seen from section \ref{section:salagg_results}, most \ac{xai} methods (except for \ac{gradcam}) have relatively low correlation with either \ac{msp} or \ac{mls}, which could give similar benefits as those found by \texttt{COMBOOD}.

\subsubsection{ImageNet200}

Again, we start with the ImageNet200 testing benchmark. Figure \ref{fig:imagenet200_salpluslogit_bootstrap_barplot} shows the results of the 10 bootstraps. We can see that the poor performances on Near-\ac{ood} have mostly disappeared when combining saliency aggregation with \ac{mls}: Instead, all methods perform quite closely to the baselines on Near-\ac{ood}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_salpluslogit_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet200 Saliency Aggregation plus Logit Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation plus Logit on the bootstrapped ImageNet200 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet200_salpluslogit_bootstrap_barplot}
\end{figure}

In addition to the Near-\ac{ood} improvements, \ac{gbp}Norm+Logit achieves a very high Far-\ac{ood} performance on ImageNet200. \ac{lime}Norm+Logit improves considerably over \ac{lime}Norm, and also beats the baselines on Far-\ac{ood}. \ac{gradcam}Norm+Logit also outperforms the baselines, but we should remember that \ac{gradcam}Norm (without the addition of logits) also performed well, so these results may not be better than simply using \ac{gradcam}Norm alone. Let us look closer at the performance of each method, considering the results of the Wilcoxon signed-rank tests performed against the baseline methods.

As we see from table \ref{table:imagenet200_salpluslogit_ttest}, we see far more improvements when combining our saliency aggregation method with the \ac{mls}. Where we previously saw multiple percentage points lower Near-\ac{ood} on several methods with Saliency Aggregation, we now see that no method except for OcclusionRange sees any more than a single percentage point drop against either baseline. Furthermore, even OcclusionRange, which previously saw consistent double digit drops in performance, has improved considerably, and even outperforms both baselines on Far-\ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 83.46 & +0.715 & 0.001 ** & +0.178 & 0.007 * \\
    \rowcolor{far!50}
    Far-OOD & 93.12 & +1.845 & 0.001 ** & +2.927 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 80.21 & -2.536 & 1.000 & -3.073 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.20 & +0.923 & 0.001 ** & +2.005 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 83.15 & +0.407 & 0.001 ** & -0.130 & 0.968 \\
    \rowcolor{far!50}
    Far-OOD & 92.09 & +0.808 & 0.001 ** & +1.890 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 82.36 & -0.384 & 1.000 & -0.922 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.04 & -0.233 & 1.000 & +0.848 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 82.96 & +0.217 & 0.010 * & -0.320 & 0.990 \\
    \rowcolor{far!50}
    Far-OOD & 93.95 & +2.677 & 0.001 ** & +3.759 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyAggregation plus Logit on ImageNet200]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on ImageNet200, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet200_salpluslogit_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage
% Let us consider each method in turn. For \ac{lime}Norm+Logit, we see that the poor Near-\ac{ood} performance is improved substantially, leading to statistically significant improvements over \ac{mls} (but not over \ac{msp}). In addition, the Far-\ac{ood} performance is improved further, giving us an \ac{ood} detector which performs well on both Near- and Far-\ac{ood}.
%
% The performance of \ac{gradcam}Norm+Logit is good, but not any better than simply using \ac{gradcam}Norm saliency aggregation. 
%
% For IGMean+Logit, the performance is increased over IGMean on both Near- and Far-\ac{ood}. This is most likely due to the extra information which is being added by including the logit. However, the performance is not better than simply using \ac{mls}. This is surprising, given that the performance of \ac{lime}Norm was far worse than IGMean, yet its performance when combined with \ac{mls} was far higher.
%
% Finally, for \ac{gbp}Norm+Logit, the improvement is considerable, especially on Far-\ac{ood}, where the performance is actually quite close to the \ac{sota} models, which is around 0.95. However, the Near-\ac{ood} performance is not statistically significantly better than either \ac{mls} or \ac{msp}.

\subsubsection{CIFAR10}

Next, we turn to CIFAR10. As we can see from figure \ref{fig:cifar10_salpluslogit_bootstrap_barplot}, \ac{gbp}Norm+Logit not only beats the baselines in the Far-\ac{ood} category, but also on Near-\ac{ood}, where the performance is considerably higher. In addition, \ac{lime}Norm+Logit and IGMean+Logit outperform the baselines on Far-\ac{ood}.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_salpluslogit_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR10 Saliency Aggregation plus Logit Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation plus Logit on the bootstrapped CIFAR10 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar10_salpluslogit_bootstrap_barplot}
\end{figure}

Like in the previous sections, let look at the results of the Wilcoxon signed-rank tests to gain a more robust understanding of the performance of the different methods. As table \ref{table:cifar10_salpluslogit_ttest} shows, we now see statistically significant results on Far-\ac{ood} for all methods except for \ac{gradcam}Norm+Logit and OcclusionRange+Logit. In addition, \ac{gbp}Norm+Logit sees a large improvement on both Near- and Far-\ac{ood} datasets. 

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 87.80 & +0.831 & 0.001 ** & +0.116 & 0.053 \\
    \rowcolor{far!50}
    Far-OOD & 93.35 & +1.532 & 0.001 ** & +2.120 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 88.00 & +1.036 & 0.001 ** & +0.321 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 90.54 & -1.277 & 1.000 & -0.689 & 1.000 \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 86.92 & -0.046 & 1.000 & -0.760 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.81 & -0.013 & 1.000 & +0.575 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 87.21 & +0.240 & 0.001 ** & -0.475 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.47 & +0.647 & 0.001 ** & +1.236 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 89.91 & +2.940 & 0.001 ** & +2.225 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 94.68 & +2.858 & 0.001 ** & +3.446 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyAggregation plus Logit on CIFAR10]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on CIFAR10, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar10_salpluslogit_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{ImageNet1K}

Next, we turn to ImageNet1K. Again, we see high \ac{auroc} values from combining saliency aggregation and the maximum logit, with several methods outperforming the baselines by several percentage points.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet_salpluslogit_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet200 Saliency Aggregation plus Logit Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation plus Logit on the bootstrapped ImageNet1K testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet_salpluslogit_bootstrap_barplot}
\end{figure}

Looking at table \ref{table:imagenet_salpluslogit_ttest}, we see that the performance on Far-\ac{ood} is high across all methods except for IGMean+Logit and OcclusionRange+Logit, with \ac{gbp}Norm+Logit especially reporting a substantial improvement on both Near-\ac{ood} and Far-\ac{ood}. 


\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 74.23 & -2.098 & 1.000 & -1.806 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.17 & +2.656 & 0.001 ** & +7.160 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 70.95 & -5.384 & 1.000 & -5.092 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 88.98 & -0.534 & 1.000 & +3.970 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 76.68 & +0.348 & 0.001 ** & +0.641 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 91.25 & +1.739 & 0.001 ** & +6.243 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 75.37 & -0.960 & 1.000 & -0.668 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 88.85 & -0.665 & 1.000 & +3.839 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 78.25 & +1.920 & 0.001 ** & +2.212 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 93.83 & +4.320 & 0.001 ** & +8.824 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyAggregation plus Logit on Imagenet]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on Imagenet, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet_salpluslogit_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{CIFAR100}

Finally, we turn to CIFAR100. Here we find that all \ac{xai} based methods outperform the baselines on Far-\ac{ood}, while the performance on Near-\ac{ood} is competitive with baseline methods.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar100_salpluslogit_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR100 Saliency Aggregation plus Logit Bootstrap]{Barplot of average \ac{auroc} scores for Saliency Aggregation plus Logit on the bootstrapped CIFAR100 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar100_salpluslogit_bootstrap_barplot}
\end{figure}

Looking at table \ref{table:cifar100_salpluslogit_ttest}, we see that all methods have statistically significantly higher \ac{auroc} scores than both baseline methods on Far-\ac{ood}. For Near-\ac{ood}, none of the methods surpass the both baselines.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c!{\vrule width 1pt}c|c!{\vrule width 1pt}c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{mls} & P-value \ac{mls} & $\Delta$\ac{auroc} \ac{msp} & P-value \ac{msp} \\
    \hline
    \hline
    \multicolumn{6}{|c|}{LIMENorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 78.92 & -2.316 & 1.000 & -1.763 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 82.13 & +2.273 & 0.001 ** & +4.214 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{OcclusionRange} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 78.56 & -2.678 & 1.000 & -2.125 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 81.39 & +1.531 & 0.001 ** & +3.472 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GradCAMNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 81.24 & +0.006 & 0.116 & +0.559 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 79.93 & +0.066 & 0.001 ** & +2.007 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{IGMean} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 80.58 & -0.662 & 1.000 & -0.108 & 0.981 \\
    \rowcolor{far!50}
    Far-OOD & 81.95 & +2.089 & 0.001 ** & +4.030 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{6}{|c|}{GBPNorm} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 77.70 & -3.542 & 1.000 & -2.989 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 80.46 & +0.599 & 0.001 ** & +2.540 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyAggregation plus Logit on CIFAR100]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{mls} and \ac{msp}, showing the mean \ac{auroc} over 10 runs on CIFAR100, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar100_salpluslogit_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Overall analysis of Saliency Aggregation plus Logit}

Overall, the results build on the Saliency Aggregation framework and improve the performance considerably. While the results of Saliency Aggregation showed a clear deficit on Near-\ac{ood}, the addition of the \ac{mls} into the \ac{ood} detector consistently reduced this deficit and equalized the performance when compared to the baselines. These results concur with those found by \cite{combood}, which also found that the subpar Near-\ac{ood} performance of their Mahalanobis \ac{ood} detector was improved when combined with nearest neighbour \ac{ood} detection.

The performance of \ac{gbp}Norm+Logit is a particularly interesting example, beating both baselines by several percentage points on all four benchmarks on Far-\ac{ood}. In fact, the performance is actually quite close to \ac{sota} scores on all benchmarks: Table \ref{table:gbpnorm} shows the difference in \ac{auroc} scores between \ac{gbp}Norm+Logit and \ac{sota} methods as reported by \cite{openood, openood15}. 


\begin{table}[hbtp]
\begin{center}
    \begin{tabular}{|l|l|l|l|l|} 
    \hline
    Benchmark & SoTA method & $\Delta$\acs{auroc} & Non-retraining SoTA & $\Delta$\acs{auroc} \\
    \hline
    \hline
    \multicolumn{5}{|c|}{ImageNet200} \\
    \hline
    \rowcolor{near!50}
        Near-\ac{ood} & COMBOOD \cite{combood} & -12.78 & COMBOOD & -12.78 \\
    \hline
    \rowcolor{far!50}
        Far-\ac{ood} & PixMix+ASH \cite{pixmix, ash} & \textbf{-1.06} & AdaSCALE-L \cite{adascale} & \textbf{-0.91} \\
    \hline
    \hline
    \multicolumn{5}{|c|}{CIFAR10} \\
    \hline
    \rowcolor{near!50}
        Near-\ac{ood} & RotPred+PixMix \cite{rotpred, pixmix} & \textbf{-4.95} & COMBOOD & \textbf{-1.22} \\
    \hline
    \rowcolor{far!50}
        Far-\ac{ood} & RotPred+PixMix & \textbf{-3.50} & COMBOOD & \textbf{+0.03} \\
    \hline
    \hline
    \multicolumn{5}{|c|}{ImageNet1K} \\
    \hline
    \rowcolor{near!50}
        Near-\ac{ood} & COMBOOD & -16.97 & COMBOOD & -16.97 \\
    \hline
    \rowcolor{far!50}
        Far-\ac{ood} & AdaSCALE-L \cite{adascale} & \textbf{-4.02} & AdaSCALE-L & \textbf{-4.02} \\
    \hline
    \hline
    \multicolumn{5}{|c|}{CIFAR100} \\
    \hline
    \rowcolor{near!50}
        Near-\ac{ood} & OE+MSP \cite{oe, oodbaseline} & -10.60 & WeiPer+KLD \cite{weiper} & \textbf{-3.67} \\
    \hline
    \rowcolor{far!50}
        Far-\ac{ood} & ReWeight+ASH \cite{reweight, ash} & -10.66 & NAC \cite{nac} & -6.52 \\
    \hline
    \end{tabular}
    \caption[GBPNorm+Logit performance]{Table showing the performance of \ac{gbp}Norm+Logit compared to \ac{sota} methods. The best performing method for each benchmark has been chosen based on the tests conducted by \cite{openood, openood15}.\footnote{https://zjysteven.github.io/OpenOOD/} In addition, the best performing method that does not perform any retraining (i.e the best post-hoc method) has also been included. Differences between \ac{gbp}Norm+Logit and \ac{sota} methods which are less than 5 percentage points have been highlighted in bold.}
    \label{table:gbpnorm}
\end{center}
\end{table}

As we can see, \ac{gbp}Norm+Logit performs within 5 percentage points of \ac{sota} methods which do not require training on all benchmarks in at least one category (Near/Far). On ImageNet200, CIFAR10 and ImageNet1K Far-\ac{ood}, the performance is even within 5 percentage points of all \ac{ood} detection methods tested by \cite{openood, openood15}, not just the ones which do not retrain. These results are very impressive, and show that \ac{xai} based methods have the potential to perform on the level of \ac{sota} \ac{ood} detection methods.

Considering the results from the other methods, \ac{lime}Norm+Logit OcclusionRange+Logit and IGMean+Logit also showcased the clear benefit of combining saliency aggregation with \ac{mls}, with scores that were much higher than in the Saliency Aggregation framework. The results for \ac{gradcam}Norm+Logit were no better than the equivalent results without the addition of the logit. This is not unexpected, given the extremely high correlation between \ac{gradcam} saliency aggregations and the \ac{mls}. There is little to be gained from combining two metrics which are almost entirely the same, as we do not gain any supplementary information over just using one of them. \cite{combood} found best results when using different feature extraction strategies for the two distance metrics, which allows for complementary information to be extracted by each metric. With such a high correlation, it is not surprising that no performance was gained when combining \ac{mls} and \ac{gradcam} vector norms.

In general, these results clearly show that \ac{xai} based \ac{ood} detectors are not just adequate, but also competitive with the \ac{sota} in the \ac{ood} detection field. Though the Near-\ac{ood} performance is lower than \ac{sota} methods in many cases, the Far-\ac{ood} performance of methods under the Saliency Aggregation plus Logit framework is promising.

\subsection{Results for SaliencyVIM}

For the final tests, we consider SaliencyVIM, the framework described in section \ref{section:saliencyvim_method}. As described in section \ref{section:saliencyvim_method}, SaliencyVIM requires saliency methods which output lower dimensional saliency maps, as opposed to methods such as \ac{gbp} and integrated gradients, which output a value for each pixel. Thus, the applicable \ac{xai} methods included in this thesis are \ac{lime}, occlusion and \ac{gradcam}. Although occlusion performed very poorly on the validation set when aggregating saliencies, SaliencyVIM uses every saliency value and could thus capture information which has not been captured by aggregation methods. Thus, occlusion should be included in the testing. The three methods from the SaliencyVIM framework are then OcclusionVIM, \ac{lime}\ac{vim} and \ac{gradcam}\ac{vim}. 

In this case, it does not make sense to use \ac{mls} or \ac{msp} as the baselines to compare against, since the method is based on \ac{vim}. Instead, we compare our results with \ac{vim}, to see whether the addition of saliencies can improve this method.

\subsubsection{ImageNet200}

We start with the ImageNet200 benchmark. From figure \ref{fig:imagenet200_salvim_bootstrap_barplot}, we see that the differences are extremely small between the baseline and the \ac{xai} methods. However, the confidence intervals are also very small, which might mean that there are statistically significant performance increases. Thus, we look to the results of the Wilcoxon signed-rank tests.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet200_salvim_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet200 SaliencyVIM Bootstrap]{Barplot of average \ac{auroc} scores for SaliencyVIM on the bootstrapped ImageNet200 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet200_salvim_bootstrap_barplot}
\end{figure}

From table \ref{table:imagenet200_salvim_ttest}, we see that both Occlusion\ac{vim} and \ac{lime}\ac{vim} outperform \ac{vim} on Far-\ac{ood}. In addition, \ac{lime}\ac{vim} outperforms the baseline on Near-\ac{ood}. \ac{gradcam}\ac{vim} sees no increases in performance over the baseline on either Near- or Far-\ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{vim} & P-value \ac{vim} \\
    \hline
    \hline
    \multicolumn{4}{|c|}{LIMEVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 74.72 & +0.178 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 88.83 & +0.298 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{OcclusionVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 74.63 & +0.094 & 0.312 \\
    \rowcolor{far!50}
    Far-OOD & 89.12 & +0.580 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{GradCAMVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 74.41 & -0.130 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 88.51 & -0.022 & 0.993 \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyVIM on ImageNet200]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{vim}, showing the mean \ac{auroc} over 10 runs on ImageNet200, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet200_salvim_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{CIFAR10}

Next, we perform \ac{ood} detection using \ac{vim} and Saliency\ac{vim} on CIFAR10. From figure \ref{fig:cifar10_salvim_bootstrap_barplot}, Saliency\ac{vim} with \ac{gradcam} as the saliency generator seems to be the best performer here, above the baseline. Occlusion and \ac{lime} on the other hand, seem to perform worse.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar10_salvim_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR10 SaliencyVIM Bootstrap]{Barplot of average \ac{auroc} scores for SaliencyVIM on the bootstrapped CIFAR10 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar10_salvim_bootstrap_barplot}
\end{figure}

As we see from table \ref{table:cifar10_salvim_ttest}, \ac{gradcam}\ac{vim} outperforms the baseline on both Near- and Far-\ac{ood}. Both occlusion and \ac{lime} see drops in performance across the board, contrary to the results on ImageNet200.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{vim} & P-value \ac{vim} \\
    \hline
    \hline
    \multicolumn{4}{|c|}{LIMEVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 87.03 & -1.352 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 91.83 & -1.260 & 1.000 \\
    \hline
    \hline
    \multicolumn{4}{|c|}{OcclusionVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 87.76 & -0.621 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.34 & -0.747 & 1.000 \\
    \hline
    \hline
    \multicolumn{4}{|c|}{GradCAMVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 88.70 & +0.315 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 93.37 & +0.280 & 0.001 ** \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyVIM on CIFAR10]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{vim}, showing the mean \ac{auroc} over 10 runs on CIFAR10, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar10_salvim_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{ImageNet1K}

Next, we look at the results on the ImageNet1K testing benchmark. From figure \ref{fig:imagenet_salvim_bootstrap_barplot}, we see that the confidence intervals are extremely small, and we must clearly look at the results from the statistical tests to glean any information from this benchmark.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/imagenet_salvim_bootstrap_barplot.pgf}
    \end{center}
    \caption[ImageNet1K SaliencyVIM Bootstrap]{Barplot of average \ac{auroc} scores for SaliencyVIM on the bootstrapped ImageNet1K testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:imagenet_salvim_bootstrap_barplot}
\end{figure}

From table \ref{table:imagenet_salvim_ttest}, we see that both \ac{lime}\ac{vim} and Occlusion\ac{vim} are statistically significantly better than \ac{vim} on Far-\ac{ood}, while \ac{gradcam}\ac{vim} is worse on both Near- and Far-\ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{vim} & P-value \ac{vim} \\
    \hline
    \hline
    \multicolumn{4}{|c|}{LIMEVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 72.76 & -0.237 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.77 & +0.053 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{OcclusionVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 72.78 & -0.212 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.79 & +0.077 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{GradCAMVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 72.73 & -0.263 & 1.000 \\
    \rowcolor{far!50}
    Far-OOD & 92.65 & -0.063 & 1.000 \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyVIM on Imagenet]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{vim}, showing the mean \ac{auroc} over 10 runs on Imagenet, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:imagenet_salvim_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\newpage

\subsubsection{CIFAR100}

Finally, we consider the CIFAR100 benchmark. Again, the margins are very close, and we should look to the results of the Wilcoxon signed-rank tests.

\begin{figure}[hbtp]
    \begin{center}
        \input{figure/cifar100_salvim_bootstrap_barplot.pgf}
    \end{center}
    \caption[CIFAR100 SaliencyVIM Bootstrap]{Barplot of average \ac{auroc} scores for SaliencyVIM on the bootstrapped CIFAR100 testing benchmark. 95\% confidence intervals are plotted as whiskers. Note that the y-axis starts at 0.50, the practical floor for an \ac{auroc} score.}
    \label{fig:cifar100_salvim_bootstrap_barplot}
\end{figure}

Looking at table \ref{table:cifar100_salvim_ttest}, we see that both \ac{lime}\ac{vim} and Occlusion\ac{vim} are statistically significantly better than \ac{vim} on both Near- and Far-\ac{ood}, while \ac{gradcam}\ac{vim} is better on Near-\ac{ood}.

\begin{table}[hbtp]
\setlength\tabcolsep{3pt}
\begin{center}
\begin{tabular}{ |c|c|c|c| }
    \hline
    Dataset & \ac{auroc} & $\Delta$\ac{auroc} \ac{vim} & P-value \ac{vim} \\
    \hline
    \hline
    \multicolumn{4}{|c|}{LIMEVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 70.45 & +0.298 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 79.16 & +0.147 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{OcclusionVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 70.44 & +0.283 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 79.08 & +0.060 & 0.001 ** \\
    \hline
    \hline
    \multicolumn{4}{|c|}{GradCAMVIM} \\
    \hline
    \rowcolor{near!50}
    Near-OOD & 70.29 & +0.134 & 0.001 ** \\
    \rowcolor{far!50}
    Far-OOD & 78.97 & -0.048 & 1.000 \\
    \hline
    \end{tabular}
    \caption[Wilcoxon signed-rank test for SaliencyVIM on CIFAR100]{Results of performing a Wilcoxon signed-rank test on the \ac{auroc} means of against \ac{vim}, showing the mean \ac{auroc} over 10 runs on CIFAR100, the difference in means compared to the baselines, and the corresponding p-values. Each p-value is appended a significance code which follows the \texttt{R}-standard.}
    \label{table:cifar100_salvim_ttest}
\end{center}
\setlength\tabcolsep{6pt}
\end{table}

\subsubsection{Overall analysis of SaliencyVIM}

Overall, the results of integrating \ac{xai} saliencies directly into the already developed \ac{vim} \ac{ood} detector also shows potential, with multiple instances of several methods under this framework beating the baseline method. However, compared to Saliency Aggregation plus Logit, the increase in \ac{auroc} scores is much lower, with the highest increase over the baseline across all benchmarks being only 0.580 percentage points. Regardless, the results are statistically significant, and show that integrating saliencies directly into already developed \ac{ood} detection methods has potential. It is also interesting to see that the inclusion of occlusion saliencies lead to statistically significant improvements under this framework, when the discriminative power of aggregating occlusion was so low, as reported in section \ref{section:saliencyagg}.

\section{Summary}

In this chapter, I have presented the results from inspecting saliency aggregation over the validation benchmarks, as well results from the three different \ac{xai} based \ac{ood} detection algorithms I introduced in chapter \ref{chapter:methodology}.

In section \ref{section:saliencyagg}, I have analyzed the discriminative power of different saliency aggregation methods on the validation sets of ImageNet200 and CIFAR10. From this, I have found that methods based on the magnitude of saliencies can effectively separate \ac{id} and \ac{ood} samples. On the contrary, aggregations which measure the statistical spread of saliencies independent of the magnitude, such as \ac{cv}, \ac{rmd} or \ac{qcd}, do not have sufficient discriminative power and are not suitable for \ac{ood} detection. In addition, I have found that aggregations based on occlusion saliencies also lack discriminative power compared to those based on other methods. At the end of this section, I have analyzed the average performance of the different combinations of \ac{xai} saliency methods and aggregation functions. Based on this analysis, I have chosen a selection of aggregations to use with the five \ac{xai} methods mentioned in this thesis, under the Saliency Aggregation and Saliency Aggregation plus Logit frameworks. This lead to the following combinations: \ac{lime}Norm, OcclusionRange, \ac{gradcam}Norm, IGMean and \ac{gbp}Norm.

In section \ref{section:results} I tested the selected methods on bootstrapped testing benchmarks, and compared the methods to baseline \ac{ood} detectors. For Saliency Aggregation and Saliency Aggregation plus Logit, I used the aggregations which were found to be most discriminative for each \ac{xai} method on the validation sets, and created five combinations based on these. For Saliency\ac{vim}, I used \ac{lime}, occlusion and \ac{gradcam}, as these \ac{xai} methods generate saliencies which are suited for integration into the method developed by \cite{vim}, while \ac{gbp} and integrated gradients do not. 

The tests were conducted on four benchmarks, ImageNet200, CIFAR10, ImageNet1K and  CIFAR100. From these tests, I have found that there is definite potential for using \ac{xai} methods for \ac{ood} detection. Across the first two frameworks (Saliency Aggregation and Saliency Aggregation plus Logit), \ac{xai} methods perform well on Far-\ac{ood}, surpassing the baselines on many occasions. On Near-\ac{ood}, the results are more mixed, but some methods achieve impressive results here as well. Under the Saliency\ac{vim} framework, the results also show potential, with significant results on multiple datasets. In general, \ac{xai} methods show better performance on Far-\ac{ood} than Near-\ac{ood}, but high performance is attained in both scenarios on several occasions. \ac{gbp}Norm+Logit is one such example, performing almost at the level of \ac{sota} methods on CIFAR10, ImageNet200 and CIFAR100.

\chapter{Discussion} \label{chapter:discussion}

This chapter will discuss the findings of chapter \ref{chapter:experiments} in more depth and in relation to the problem statement. In addition, I will analyse the results further, attempting to explain some of the tendencies that have been revealed during the experiments.

\section{Answering the problem statement}

As we have now seen, \ac{xai} methods can indeed perform \ac{ood} detection at a level competitive with \ac{ood} detection baselines, and can in some cases even perform close to \ac{sota} models amongst methods which do not retrain the underlying classifying network. More specifically, \ac{gbp}Norm+Logit, under the Saliency Aggregation plus Logit framework, performs within five percentage points of \ac{sota} methods which do not retrain on all benchmarks on either Near- or Far-\ac{ood}.

The experiments have been conducted on all four OpenOOD \ac{ood} detection benchmarks, making the results solid and generalizable. Considering the problem statement, we can conclude that \ac{xai} based \ac{ood} detection algorithms can perform at a high level, and that the inclusion of \ac{xai} saliency maps during \ac{ood} detection can lead to higher performance than without them. These results are especially apparent on Far-\ac{ood}, where \ac{xai} methods beat the baselines by several percentage points on several benchmarks. These results contradict previous research \cite{martinez}, which found that \ac{xai} based \ac{ood} detection methods had little to offer. Although the results are not \ac{sota}, the frameworks introduced in this thesis are a valuable addition to the field of \ac{ood} detection, and could be built upon in many ways, as will be discussed in section \ref{section:future}.

\section{Limitations of the Results}

Because the methods have been benchmarked on all four \ac{ood} benchmarks included in OpenOOD, the results are relatively solid and generalizable. However, the limitations of OpenOOD itself transfer to the results of this thesis. The experiments have only been done on image classification problems, which limits the number applications the results apply to. Tasks such as segmentation, object detection, regression or natural language processing are not considered in this thesis. In addition, the results only consider semantic \ac{ood} shift, as opposed to covariate shift.

\section{Deeper analysis of the results}

In the preceding section, we have answered the problem statement by concluding that \ac{xai} based \ac{ood} detectors can perform very competitively in several instances. This chapter will further discuss some of the findings in more detail, and attempt to explain some of the results.

\subsection{Analysing the reasons for the effectiveness of \ac{xai} based \ac{ood} detection}

The first and most obvious question to ask after having shown that \ac{xai} based \ac{ood} detectors are effective, is to ask why this is the case. Based on the findings of chapter \ref{chapter:experiments}, especially section \ref{section:saliencyagg}, I make the claim that it is primarily the magnitude information collected by \ac{xai} methods which make them effective \ac{ood} detectors. These results are interesting, because magnitude information is often disregarded in \ac{xai} saliency methods. As described in \ref{section:saliencymapbackground}, \ac{xai} methods applied to images are usually normalized and displayed visually, often without conveying any information about the magnitude of the saliencies. This may be reasonable when attempting to illuminate the areas which a model is focused on, but is insufficient to discriminate between \ac{id} and \ac{ood} samples. The results from section \ref{section:saliencyagg} show this clearly: Here, the statistical dispersion aggregations, which are essentially invariant to the magnitude, perform far worse than the majority of magnitude based aggregations. Furthermore, \cite{martinez}, which used normalized \ac{gradcam} heatmaps, attained far worse results than my \ac{gradcam} aggregations. The comparatively weak improvements of methods under the SaliencyVIM framework, which uses all saliency values, compared to the saliency aggregation methods further strengthen the idea that positional information in explanations is secondary to magnitude information.

The next question to ask is why \ac{xai} saliencies carry such discriminative magnitudal information. From the performance three gradient based methods, it is clear that the gradient information from a given prediction is highly discriminative in an \ac{ood} detection context. The value of gradients for \ac{ood} detection has already been shown by works such as GradNorm (\cite{gradnorm}) and ODIN (\cite{odin}), which both use gradients as part of their \ac{ood} detection methodology. However, my methods actually outperform GradNorm on every OpenOOD benchmark, and methods under the Saliency Aggregation plus Logit framework sometimes outperform ODIN, showing that the way gradients are used for detection has a large effect on the final results.

Clearly, in the search for explanations which fulfill desirable qualities such as sensitivity, fidelity and implementation invariance, gradient based \ac{xai} methods have inadvertently transformed the gradient information of a network in ways which also offer \ac{ood} discriminative power. With \ac{lime}, \cite{lime} also had the stated goals of creating a high fidelity implementation invariant (model independent) \ac{xai} method. We can posit that the training of a locally interpretable model on the differences in logits before and after masking extracts saliency magnitudes in a way that is similar to the gradient based \ac{xai} methods, although the implementation of the different methods is completely different.

\subsection{Analysing the difference in performance between magnitude and dispersion aggregation}

In section \ref{section:saliencyagg_method}, two intuitions were put explained: The first intuition was that the magnitudes of \ac{xai} saliencies could be distributed differently on \ac{id} datasets than on \ac{ood} datasets. The second intuition was that the statistical dispersion of saliencies were distributed differently. As we have seen from section \ref{section:saliencyagg}, the first intuition seems to be correct, and in the previous section I attribute most of the performance of the \ac{xai} frameworks to the difference in magnitude.

However, statistical dispersion aggregates do not seem to be distributed sufficiently differently between \ac{id} and \ac{ood}. This is an interesting result. As described in \ref{chapter:methodology}, one might intuitively expect that \ac{id} samples have high saliency values around regions of interest, while \ac{ood} samples have more evenly dispersed values because there are no regions in the image which correspond to the training classes. This would lead \ac{id} samples to have a higher dispersion of values. Contrarily, one could imagine that \ac{ood} samples lead to \ac{xai} saliency maps which are noisy and spread out because \ac{xai} methods are not designed to handle \ac{ood} data and lack robustness. This would lead to \ac{ood} data samples having higher dispersion than \ac{id}. However, as we have seen in section \ref{section:saliencyagg}, neither hypothesis is correct, and there is only rarely enough difference in statistical dispersion between \ac{id} and \ac{ood} to sufficiently discriminate between them. From the results on the validation data sets, the \ac{auroc} scores of statistical dispersion are often between 0.50 and 0.60, barely better than pure guessing. In some cases, they may be higher, however there is no clear pattern of either \ac{id} or \ac{ood} being larger across datasets. Instead, the \ac{id} data points may have higher dispersion with a particular \ac{xai} method on a particular dataset, while they may be substantially lower than \ac{ood} data points on another dataset. This, combined with the low scores overall, show that there is no consistent difference in statistical dispersion across \ac{id} and \ac{ood} data sets.

To get a better understanding of why this is the case, we may look to some example images from the ImageNet1K benchmark and its associated \ac{ood} datasets. In figure \ref{fig:heatmaps_dispersion}, I show the normalized heatmaps of \ac{gradcam} on ImageNet1K and OpenImage\_O, one of the Far-\ac{ood} datasets from this benchmark.

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]
    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    % Loop through the grid
    \foreach \row in {0, 1, 2} {
        \foreach \col in {0, 1, 2, 3, 4, 5} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 6 + (\col == 4 ? 1 : (\col == 1 ? 4 : \col))}
            \pgfmathtruncatemacro{\pad}{(\col > 2 ? 1 : 0)}

            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth + \pad * 1cm, -\row * \cellheight - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/imagenet_heatmaps/imagenet_gradcam_heatmaps_normalized-img\imagenumber.png}};
        }
    }
    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.3,-8.3);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (6.3,-0.4) rectangle (14,-8.3);
    \end{scope}

    \node [above=0.1cm of 4] {\ac{id}: ImageNet1K};

    \node [above=0.1cm of 1] {Far-\ac{ood}: OpenImage\_O};

    \end{tikzpicture}
    \caption[Statistical dispersion example heatmaps]{Figure showing normalized heatmaps from ImageNet1K and OpenImage\_O}
    \label{fig:heatmaps_dispersion}
    \end{center}
\end{figure}

As we can see from the figure, there are some clear problems with using statistical dispersion to separate \ac{id} and \ac{ood}. Firstly, we see that there is a great amount of variation in the position and size of objects of interest in \ac{id} data, which in turn makes it less likely that \ac{id} data will have a consistent amount of spread. An object of interest may be relatively far away from the camera, leading to only a small region being salient (with an associated high dispersion), but it may just as easily be very close to the camera, leading a large and spread out saliency. We can see this when comparing the bird in the middle right of the \ac{id} images with the dog in the middle left. These investigations concur with the findings of \cite{martinez}, who also pointed to the fact that objects "appear in arbitrary parts of the image and have a high degree of compositional variability" when explaining the poor performance of their heatmap clustering. In addition, the spread of \ac{ood} saliency maps does not seem to show any clear pattern, either of being evenly spread out because no objects of interest are present, or of being unstable and highly dispersed due to a lack of robustness in the \ac{xai} saliency method when faced with \ac{ood} data. Instead, the \ac{ood} saliency maps look about the same as the \ac{id} saliency maps, and it is hard to find any notable differences in the two.

To contrast, we may look at the same images but without normalizing the saliency values. From figure \ref{fig:heatmaps_magnitude}, we see a much clearer distinction between \ac{id} and \ac{ood} data; all of the Far-\ac{ood} have considerably lower saliency values. As we have seen from chapter \ref{chapter:experiments}, aggregations such as the mean and vector norm have a moderate to strong correlation with the outputs of baseline \ac{ood} detection methods such as \ac{mls} and \ac{msp}, which is reflected in these results.

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]
    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    % Loop through the grid
    \foreach \row in {0, 1, 2} {
        \foreach \col in {0, 1, 2, 3, 4, 5} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 6 + (\col == 4 ? 1 : (\col == 1 ? 4 : \col))}
            \pgfmathtruncatemacro{\pad}{(\col > 2 ? 1 : 0)}

            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth + \pad * 1cm, -\row * \cellheight - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/imagenet_heatmaps/imagenet_gradcam_heatmaps_unnormalized-img\imagenumber.png}};
        }
    }
    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.3,-8.3);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (6.3,-0.4) rectangle (14,-8.3);
    \end{scope}

    \node [above=0.1cm of 4] {\ac{id}: ImageNet1K};

    \node [above=0.1cm of 1] {Far-\ac{ood}: OpenImage\_O};

    \end{tikzpicture}
    \caption[Saliency magnitude example heatmaps]{Figure showing unnormalized heatmaps from ImageNet1K and OpenImage\_O}
    \label{fig:heatmaps_magnitude}
    \end{center}
\end{figure}

From these figures, we can see visually how statistical dispersion falls short compared to magnitude aggregation, giving a strong indication as to why these two types of aggregation differ so strongly in discriminative power.

In addition, these results also point to a weakness of how \ac{xai} methods are utilized in general, given that most libraries perform normalization and display heatmaps without regard for the magnitude of the saliencies. As \cite{noxaiblackbox} has pointed out, the saliency maps of a specific input image for two completely different classes may look essentially the same, casting doubt on the validity of the explanations. However, their example uses normalized heatmaps. As has been shown, this normalization removes important information about the model's response to an input, and by displaying heatmaps with regard to some baseline value as opposed to simply comparing all saliency values to each other, some of the issues which have been pointed out by \cite{noxaiblackbox} could be remedied. Though not a part of the research question, the findings from section \ref{section:saliencyagg} could be used to gain a better understanding of \ac{xai} methods and could be used to improve the reliability of \ac{xai}.

\subsection{Analysing the poor performance of occlusion saliency aggregation}

From the results found in section \ref{section:saliencyagg} and section \ref{section:results}, occlusion is an outlier amongst the five \ac{xai} methods utilized, and falls consistently behind the other methods. This is somewhat surprising, given the similarity between \ac{lime} and occlusion. As explained in section \ref{section:xaimethodsbackground}, both \ac{lime} and occlusion calculate saliency by occluding parts of the image and looking at the change in confidence of the model. The major difference is that occlusion simply occludes one region at a time and sets the saliency to the drop in confidence, while \ac{lime} calculates saliencies in a more involved fashion where several regions are occluded at once and a new, less complex model is trained to approximate the network.

To understand why the performance of occlusion is poor, we may compare it to \ac{lime}, given their similarities. Figure \ref{fig:lime_occ_comp} shows the unnormalized heatmaps of occlusion and \ac{lime} on the same images as in the previous figures.

\begin{figure}[hbtp]
    \begin{center}
    \begin{tikzpicture}[
    SIR/.style={rectangle, draw=black!80, thick, minimum size=5mm},
    ]
    % Define the size of each image cell
    \def\cellwidth{2.3cm}
    \def\cellheight{2.3cm}
    % Loop through the grid
    \foreach \row in {0, 1, 2} {
        \foreach \col in {0, 1, 2, 3, 4, 5} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 6 + (\col == 4 ? 1 : (\col == 1 ? 4 : \col))}
            \pgfmathtruncatemacro{\pad}{(\col > 2 ? 1 : 0)}

            % Draw the image
            \node[inner sep=0] (\imagenumber) at (\col * \cellwidth + \pad * 1cm, -\row * \cellheight - \cellheight) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/occ_heatmaps/imagenet_occlusion_heatmaps_unnormalized-img\imagenumber.png}};
        }
    }

    \foreach \row in {0, 1, 2} {
        \foreach \col in {0, 1, 2, 3, 4, 5} {
            % Calculate the image number
            \pgfmathtruncatemacro{\imagenumber}{\row * 6 + (\col == 4 ? 1 : (\col == 1 ? 4 : \col))}
            \pgfmathtruncatemacro{\pad}{(\col > 2 ? 1 : 0)}

            % Draw the image
            \node[inner sep=0] (2\imagenumber) at (\col * \cellwidth + \pad * 1cm, -\row * \cellheight - \cellheight - 7.5cm) 
                {\includegraphics[width=\cellwidth, height=\cellheight]{figure/lime_heatmaps/imagenet_lime_heatmaps_unnormalized-img\imagenumber.png}};
        }
    }

    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-0.4) rectangle (6.3,-8.3);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (6.3,-0.4) rectangle (14,-8.3);
    \end{scope}

    \begin{scope}[on background layer={color=id!50}]
        \fill (-1.5,-8.4) rectangle (6.3,-15.8);
    \end{scope}

    \begin{scope}[on background layer={color=far!50}]
        \fill (6.3,-8.4) rectangle (14,-15.8);
    \end{scope}

    \node [above=0.1cm of 4] {\ac{id}: ImageNet1K};
    \node [above=0.1cm of 1] {Far-\ac{ood}: OpenImage\_O};

    \node [rotate=90] at (-1.8, -5.1) {Occlusion};
    \node [rotate=90] at (-1.8, -12.2) {\ac{lime}};

    \end{tikzpicture}
        \caption[LIME and Occlusion heatmap comparison]{Figure showing unnormalized heatmaps from ImageNet1K and OpenImage\_O for \ac{lime} and occlusion. The heatmaps are displayed using bilinear interpolation of sliding window centers, as opposed to how they are displayed in figure \ref{fig:segmentationcomp}, because this makes them somewhat easier to interpret}
    \label{fig:lime_occ_comp}
    \end{center}
\end{figure}

From this figure, we see that on \ac{id} data, \ac{lime} and occlusion are pretty similar, highlighting about the same regions. However, on \ac{ood} data, the occlusion heatmaps are more unstable than \ac{lime}. We see that for some images, almost the entire heatmap is filled with high saliency values, and in general the saliencies for \ac{ood} data seem to be higher than for \ac{lime}. There may be many reasons for why this is case, however, I will posit one preliminary theory: 

Occlusion calculates the saliency for a specific region by calculating the drop in confidence of a single pass on an image where this region is masked. In our case, the mask is a simple rectangular region, and the masking is achieved by replacing all values with a single value (0, which is equivalent to a brownish gray after ResNet normalization). An implicit assumption we have when we use occlusion is that the drop in confidence from this masking represents the network's reaction to the absence of whichever part of the image is masked. However, as \cite{roar} has shown, removing information with a simple mask has drastic changes on the distribution of the image, and this assumption may not be correct. For example, an image of a dog in park, whose head has been replaced by a rectangular black square, is not equivalent to an image of a park with no dog present. This means that the drop in confidence does not exclusively represent the absence of the masked region, but also the unpredictable changes that a model may exhibit when large regions of an image are replaced by a constant value. Combining the destructive changes of masking with exposure to \ac{ood} data may have unpredictable effects which inhibit occlusion from capturing the differences between \ac{id} and \ac{ood} data. While \ac{lime} uses the same masking strategy, the saliencies returned by \ac{lime} are not based on a single forward pass, but based on many permutations of the input image where different regions are masked in tandem. This probably increases the stability of \ac{lime} when compared to occlusion, which could explain the difference in performance.

Interestingly, the unstable reaction to \ac{ood} images was one intuition put forth as to why saliency magnitudes might be higher for \ac{ood} data points and could therefore be used to separate them from \ac{id} data points. Occlusion could thus have been an ideal choice for \ac{ood} detection exploiting such behaviour. Indeed, as we have seen from section \ref{section:saliencyagg}, the saliency magnitudes of \ac{ood} data were higher on CIFAR10. However, the difference in distributions was not very high compared to other methods (based on the \ac{auroc} scores), and on ImageNet200, the magnitudes of \ac{id} data were higher.

\chapter{Conclusion} \label{chapter:conclusion}

For the final chapter of this thesis, I give short summary of the thesis, followed by a recapitulation of the main contributions, and finally a description of possible ways forward.

\section{Thesis summary}

This thesis has been an investigation into the potential benefits of using \ac{xai} for \ac{ood} detection. To investigate this potential, I have developed three different frameworks for utilizing \ac{xai} saliencies for \ac{ood} detection. These three frameworks are {\it Saliency Aggregation}, {\it Saliency Aggregation plus Logit} and {\it SaliencyVIM}.

Saliency Aggregation is a framework which takes a saliency mapping \ac{xai} method and an aggregation function and uses the aggregate of the saliencies as an \ac{ood} detection score. Saliency Aggregation plus Logit builds upon the previous framework by combining saliency aggregation with \ac{mls}, a baseline \ac{ood} detection method. The two metrics are combined by adding their respective Z-scores, inspired by the work of \cite{combood}. Finally, SaliencyVIM is a framework which integrated \ac{xai} saliencies directly into the \ac{ood} detection method \ac{vim}. The saliencies are integrated by concatenating them with the features of the penultimate layer, before a \ac{pca} is performed on the concatenated matrices. The \ac{ood} score is based on the \ac{pca} reconstruction error in comparison with the logits.

To perform a selection of methods under these frameworks, an analysis of the statistical qualities of different \ac{xai} saliencies has been performed. These analyses have shown that saliency magnitudes are sufficiently differently distributed between \ac{id} and \ac{ood} datasets to allow for effective \ac{ood} detection. The statistical dispersion, on the other hand, is not sufficiently discriminative of \ac{ood} data, and should not be used for \ac{ood} detection.

Based on the analysis described above, the three frameworks were rigorously tested on all four \ac{ood} detection benchmarks included in OpenOOD. Based on these tests, it was found that methods developed under all three frameworks could outperform baseline \ac{ood} detection methods. Out of the three frameworks, Saliency Aggregation plus Logit was the best performing, with several methods achieving results which were several percentage points over the baselines, on multiple occasions. In addition, the method \ac{gbp}Norm+Logit achieved results which were somewhat close to the \ac{sota} on both CIFAR10 and ImageNet1K, showing the definite potential of \ac{xai} \ac{ood} detection.

\section{Main contributions}

The main contribution of this thesis is the development of three different frameworks for using \ac{xai} saliencies for \ac{ood} detection. These three frameworks have been rigorously tested on all four OpenOOD \ac{ood} detection benchmarks, and all three showcase statistically significant improvements over baseline methods. In addition, one method under the Saliency Aggregation plus Logit framework reports \ac{auroc} scores within 4.1 percentage points of \ac{sota} post-hoc \ac{ood} detection methods on all four benchmarks. Furthermore, the three frameworks are highly general, opening the door for potential improvements when using different \ac{xai} methods or in combination with different \ac{ood} detectors.

Secondarily, comprehensive analysis has also been done on the statistical qualities of \ac{xai} saliency maps on \ac{id} and \ac{ood} data (chapter \ref{chapter:experiments} section \ref{section:saliencyagg}). These findings give deep insight into how \ac{xai} methods behave when faced with \ac{ood} data, and show that while the statistical dispersion of saliencies is not very different between \ac{id} and \ac{ood} data, the magnitude of saliencies is. These findings could be used to further explore the integration of \ac{xai} and \ac{ood} detection.


\section{Future work} \label{section:future}

Although comprehensive investigations and tests have been conducted as part of this thesis, there are many avenues which could be explored further. As I have introduced general frameworks for integrating \ac{xai} into \ac{ood} detection, as opposed to specific methods, there is great potential for further studies into different combinations of \ac{xai} methods, aggregations and other \ac{ood} detection metrics. For example, one could perform saliency aggregation on other \ac{xai} methods such as \ac{gradcam}PlusPlus \cite{gradcamplusplus}, Shapley \cite{shapley} or DeepLift \cite{deeplift}. In addition, instead of combining saliency aggregation with logits, one could combine it with any other \ac{ood} detection metric, such as \ac{msp}, the energy score as described in \cite{energy} or the modified softmax score as described in \cite{odin}. To further explore the integration of saliencies directly into \ac{ood} detection methods, other methods than \ac{vim} can be used, for example SCALE \cite{scale} or other \ac{ood} detection methods which work on logits.

As described in section \ref{section:scope}, the scope of this thesis has also restricted the types of \ac{ood} detection taxonomies which are used. Notably, the lack of methods which retrain the network excludes data augmentation methods such as PixMix \cite{pixmix} and RotPred \cite{rotpred}. Both of these methods are used in combination with \ac{ood} detectors to great effect, achieving \ac{sota} results on CIFAR10 and ImageNet200. A possible further path of research could then be to expand the scope and explore how \ac{xai} methods could be used in pre-training as well.

Finally, the hyperparameters chosen for each \ac{xai} method have been fixed in this thesis, as opposed to being chosen through hyperparameter tuning. This has been done to reduce the computational burden, but also represents a restriction of the scope of the testing that has been performed. In future work, different sliding window dimensions or segmentation methods could be chosen for \ac{lime} and occlusion, different layers could be chosen with \ac{gradcam} and different baselines could be used with integrated gradients. Performing such experiments could further illuminate the robustness and potential of using these methods for \ac{ood} detection.

As we can see from the preceding paragraphs, this thesis is merely a first step towards integrating \ac{xai} into \ac{ood} detection. The three frameworks that have been developed and the analysis that has been conducted as part of this thesis lays the groundwork for further research in a field which is essentially unexplored.

\backmatter{}
\printbibliography{}
\end{document}
