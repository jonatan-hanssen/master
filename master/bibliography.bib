@article{gradcam,
   title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization},
   volume={128},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-019-01228-7},
   DOI={10.1007/s11263-019-01228-7},
   number={2},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
   year={2019},
   month=oct, pages={336–359}
}

@article{openood,
    title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
    author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Zhou, Kaiyang and Zhang, Wayne and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Hai, Li},
    journal={arXiv preprint arXiv:2306.09301},
    year={2023},
}

@article{rosenblatt,
  added-at = {2017-07-19T15:29:59.000+0200},
  author = {Rosenblatt, F.},
  biburl = {https://www.bibsonomy.org/bibtex/214ee8da21c66cd4d00d7ab6eca2d96a9/andreashdez},
  citeulike-article-id = {13697582},
  citeulike-linkout-0 = {http://dx.doi.org/10.1037/h0042519},
  doi = {10.1037/h0042519},
  interhash = {dc0cef9dc06033a04f525efdcde7a660},
  intrahash = {14ee8da21c66cd4d00d7ab6eca2d96a9},
  issn = {0033-295X},
  journal = {Psychological Review},
  keywords = {imported},
  number = 6,
  pages = {386--408},
  posted-at = {2016-05-02 20:23:36},
  priority = {2},
  timestamp = {2017-07-19T15:31:02.000+0200},
  title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
  url = {http://dx.doi.org/10.1037/h0042519},
  volume = 65,
  year = 1958
}

@book{mitchell,
  added-at = {2022-09-08T09:41:39.000+0200},
  author = {Mitchell, Tom M},
  biburl = {https://www.bibsonomy.org/bibtex/2ea9f893d9d19c182bcf2822eb590fe4f/msteininger},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {ea9f893d9d19c182bcf2822eb590fe4f},
  keywords = {book ml},
  number = 9,
  publisher = {McGraw-hill New York},
  timestamp = {2022-09-08T17:32:10.000+0200},
  title = {Machine learning},
  volume = 1,
  year = 1997
}

@ARTICLE{tingsim,
  title    = "Machine learning in medicine: what clinicians should know",
  author   = "Ting Sim, Jordan Zheng and Fong, Qi Wei and Huang, Weimin and
              Tan, Cher Heng",
  abstract = "With the advent of artificial intelligence (AI), machines are
              increasingly being used to complete complicated tasks, yielding
              remarkable results. Machine learning (ML) is the most relevant
              subset of AI in medicine, which will soon become an integral part
              of our everyday practice. Therefore, physicians should acquaint
              themselves with ML and AI, and their role as an enabler rather
              than a competitor. Herein, we introduce basic concepts and terms
              used in AI and ML, and aim to demystify commonly used AI/ML
              algorithms such as learning methods including neural
              networks/deep learning, decision tree and application domain in
              computer vision and natural language processing through specific
              examples. We discuss how machines are already being used to
              augment the physician's decision-making process, and postulate
              the potential impact of ML on medical practice and medical
              research based on its current capabilities and known limitations.
              Moreover, we discuss the feasibility of full machine autonomy in
              medicine.",
  journal  = "Singapore Med J",
  volume   =  64,
  number   =  2,
  pages    = "91--97",
  month    =  may,
  year     =  2021,
  address  = "India",
  keywords = "Algorithms; artificial intelligence; deep learning; machine
              learning; neural networks",
  language = "en"
}

@article{dlmed,
author = {Alvin Rajkomar  and Jeffrey Dean  and Isaac Kohane },
title = {Machine Learning in Medicine},
journal = {New England Journal of Medicine},
volume = {380},
number = {14},
pages = {1347-1358},
year = {2019},
doi = {10.1056/NEJMra1814259},
URL = {https://www.nejm.org/doi/full/10.1056/NEJMra1814259},
eprint = {https://www.nejm.org/doi/pdf/10.1056/NEJMra1814259},
abstract = { In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. These data are collected and curated to provide the latest evidence-based assessment and recommendations. }
}

@article{hyperkvasir,
  title = {{HyperKvasir, a comprehensive multi-class
    image and video dataset for gastrointestinal endoscopy}},
  author = {
    Borgli, Hanna and Thambawita, Vajira and
    Smedsrud, Pia H and Hicks, Steven and Jha, Debesh and
    Eskeland, Sigrun L and Randel, Kristin Ranheim and
    Pogorelov, Konstantin and Lux, Mathias and
    Nguyen, Duc Tien Dang and Johansen, Dag and
    Griwodz, Carsten and Stensland, H{\aa}kon K and
    Garcia-Ceja, Enrique and Schmidt, Peter T and
    Hammer, Hugo L and Riegler, Michael A and
    Halvorsen, P{\aa}l and de Lange, Thomas
  },
  doi = {10.1038/s41597-020-00622-y},
  issn = {2052-4463},
  journal = {Scientific Data},
  number = {1},
  pages = {283},
  url = {https://doi.org/10.1038/s41597-020-00622-y},
  volume = {7},
  year = {2020}
}

@misc{uncertainty,
      title={Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual Explanations: Pitfalls and Solutions}, 
      author={Eoin Delaney and Derek Greene and Mark T. Keane},
      year={2021},
      eprint={2107.09734},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.09734}, 
}

@InProceedings{generalxaiforood,
author="Sipple, John
and Youssef, Abdou",
editor="Ceci, Michelangelo
and Flesca, Sergio
and Masciari, Elio
and Manco, Giuseppe
and Ra{\'{s}}, Zbigniew W.",
title="A General-Purpose Method for Applying Explainable AI for Anomaly Detection",
booktitle="Foundations of Intelligent Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="162--174",
abstract="The need for explainable AI (XAI) is well established but relatively little has been published outside of the supervised learning paradigm. This paper focuses on a principled approach to applying explainability and interpretability to the task of unsupervised anomaly detection. We argue that explainability is principally an algorithmic task and interpretability is principally a cognitive task, and draw on insights from the cognitive sciences to propose a general-purpose method for practical diagnosis using explained anomalies. We define Attribution Error, and demonstrate, using real-world labeled datasets, that our method based on Integrated Gradients (IG) yields significantly lower attribution errors than alternative methods.",
isbn="978-3-031-16564-1"
}

@article{tallon2020explainable,
  title={Explainable AI: Using Shapley Value to Explain Complex Anomaly Detection ML-Based Systems},
  author={Tall{\'o}n-Ballesteros, AJ and Chen, C},
  journal={Machine Learning and Artificial Intelligence: Proceedings of MLIS 2020},
  volume={332},
  pages={152},
  year={2020},
  publisher={IOS Press}
}

@article{tcydenova2021detection,
  title={Detection of adversarial attacks in AI-based intrusion detection systems using explainable AI},
  author={Tcydenova, Erzhena and Kim, Tae Woo and Lee, Changhoon and Park, Jong Hyuk},
  journal={Human-Centric Comput Inform Sci},
  volume={11},
  year={2021}
}

@INPROCEEDINGS{martinez,
  author={Martinez-Seras, Aitor and Ser, Javier Del and Garcia-Bringas, Pablo},
  booktitle={2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Can Post-hoc Explanations Effectively Detect Out-of-Distribution Samples?}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  keywords={Heating systems;Analytical models;Training data;Machine learning;Predictive models;Data models;Stress measurement;Explainable Artificial Intelligence;Out-of-Distribution (OoD) detection;local explanations},
  doi={10.1109/FUZZ-IEEE55066.2022.9882726}}


@misc{pixmix,
      title={PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures}, 
      author={Dan Hendrycks and Andy Zou and Mantas Mazeika and Leonard Tang and Bo Li and Dawn Song and Jacob Steinhardt},
      year={2022},
      eprint={2112.05135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.05135}, 
}

@article{kaminski2010quality,
  title={Quality indicators for colonoscopy and the risk of interval cancer},
  author={Kaminski, Michal F and Regula, Jaroslaw and Kraszewska, Ewa and Polkowski, Marcin and Wojciechowska, Urszula and Didkowska, Joanna and Zwierko, Maria and Rupinski, Maciej and Nowacki, Marek P and Butruk, Eugeniusz},
  journal={New England journal of medicine},
  volume={362},
  number={19},
  pages={1795--1803},
  year={2010},
  publisher={Mass Medical Soc}
}

@misc{lime,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.04938}, 
}


@ARTICLE{dnsxai,
  author={Zebin, Tahmina and Rezvy, Shahadate and Luo, Yuan},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={An Explainable AI-Based Intrusion Detection System for DNS Over HTTPS (DoH) Attacks}, 
  year={2022},
  volume={17},
  number={},
  pages={2339-2349},
  keywords={Tunneling;Servers;Security;Cryptography;Protocols;Computer crime;Feature extraction;Secure computing;machine learning;intrusion detection system;explainable AI},
  doi={10.1109/TIFS.2022.3183390}}


@article{mahbooba,
author = {Mahbooba, Basim and Timilsina, Mohan and Sahal, Radhya and Serrano, Martin},
title = {Explainable Artificial Intelligence (XAI) to Enhance Trust Management in Intrusion Detection Systems Using Decision Tree Model},
journal = {Complexity},
volume = {2021},
number = {1},
pages = {6634811},
doi = {https://doi.org/10.1155/2021/6634811},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/6634811},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1155/2021/6634811},
abstract = {Despite the growing popularity of machine learning models in the cyber-security applications (e.g., an intrusion detection system (IDS)), most of these models are perceived as a black-box. The eXplainable Artificial Intelligence (XAI) has become increasingly important to interpret the machine learning models to enhance trust management by allowing human experts to understand the underlying data evidence and causal reasoning. According to IDS, the critical role of trust management is to understand the impact of the malicious data to detect any intrusion in the system. The previous studies focused more on the accuracy of the various classification algorithms for trust in IDS. They do not often provide insights into their behavior and reasoning provided by the sophisticated algorithm. Therefore, in this paper, we have addressed XAI concept to enhance trust management by exploring the decision tree model in the area of IDS. We use simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS. We experimented with this approach by extracting rules in a widely used KDD benchmark dataset. We also compared the accuracy of the decision tree approach with the other state-of-the-art algorithms.},
year = {2021}
}


@ARTICLE{idsxai,
  author={Arreche, Osvaldo and Guntur, Tanish R. and Roberts, Jack W. and Abdallah, Mustafa},
  journal={IEEE Access}, 
  title={E-XAI: Evaluating Black-Box Explainable AI Frameworks for Network Intrusion Detection}, 
  year={2024},
  volume={12},
  number={},
  pages={23954-23988},
  keywords={Explainable AI;Closed box;Network intrusion detection;Robustness;Malware;Network security;Detection algorithms;Internet security;Telecommunication traffic;Data science;Data models;Knowledge discovery;Data mining;NSL-KDD;XAI evaluation;intrusion detection systems;SHAP;explainable AI;network security;LIME;black-box AI;NSL-KDD;CICIDS-2017;RoEduNet-SIMARGL2021},
  doi={10.1109/ACCESS.2024.3365140}}



@misc{cam,
      title={Learning Deep Features for Discriminative Localization}, 
      author={Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
      year={2015},
      eprint={1512.04150},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{lrp,
    doi = {10.1371/journal.pone.0130140},
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},
}

@ARTICLE{places365,
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Places: A 10 Million Image Database for Scene Recognition}, 
  year={2018},
  volume={40},
  number={6},
  pages={1452-1464},
  keywords={Object recognition;Deep learning;Image recognition;Leearning (artificial intelligence);Image classification;Image analysis;Scene classification;visual recognition;deep learning;deep feature;image dataset},
  doi={10.1109/TPAMI.2017.2723009}}


@inproceedings{stanforddogs,
author = "Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and Li Fei-Fei",
title = "Novel Dataset for Fine-Grained Image Categorization",
booktitle = "First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition",
year = "2011",
month = "June",
address = "Colorado Springs, CO",
}


@inproceedings{cats,
author = {Zhang, Weiwei and Sun, Jian and Tang, Xiaoou},
year = {2008},
month = {10},
pages = {802-816},
title = {Cat Head Detection - How to Effectively Exploit Shape and Texture Features},
volume = {5305},
isbn = {978-3-540-88692-1},
doi = {10.1007/978-3-540-88693-8_59}
}

@misc{imagewoof,
    author    = "Jeremy Howard",
    title     = "Imagewoof",
    url       = "https://github.com/fastai/imagenette/"
}

@article{evalxai,
author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583558},
doi = {10.1145/3583558},
abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {295},
numpages = {42},
keywords = {Explainable artificial intelligence, interpretable machine learning, evaluation, explainability, interpretability, quantitative evaluation methods, explainable AI, XAI}
}

@article{xaioverview,
title = {Explainable artificial intelligence (XAI) in deep learning-based medical image analysis},
journal = {Medical Image Analysis},
volume = {79},
pages = {102470},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102470},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001177},
author = {Bas H.M. {van der Velden} and Hugo J. Kuijf and Kenneth G.A. Gilhuijs and Max A. Viergever},
keywords = {Explainable artificial intelligence, Interpretable deep learning, Medical image analysis, Deep learning, Survey},
abstract = {With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.}
}

@article{xaisurvey,
  title={Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks},
  author={Sajid Nazir and Diane M. Dickson and Muhammad Usman Akram},
  journal={Computers in biology and medicine},
  year={2023},
  volume={156},
  pages={
          106668
        },
  url={https://api.semanticscholar.org/CorpusID:257067347}
}

@misc{intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{performance,
  author    = {Dargan, Shaveta and Kumar, Munish and Ayyagari, Maruthi Rohit and Kumar, Gulshan},
  title     = {A Survey of Deep Learning and Its Applications: A New Paradigm to Machine Learning},
  journal   = {Archives of Computational Methods in Engineering},
  volume    = {27},
  number    = {4},
  pages     = {1071--1092},
  year      = {2020},
  month     = {09},
  day       = {01},
  doi       = {10.1007/s11831-019-09344-w},
  issn      = {1886-1784},
  abstract  = {Nowadays, deep learning is a current and a stimulating field of machine learning. Deep learning is the most effective, supervised, time and cost efficient machine learning approach. Deep learning is not a restricted learning approach, but it abides various procedures and topographies which can be applied to an immense speculum of complicated problems. The technique learns the illustrative and differential features in a very stratified way. Deep learning methods have made a significant breakthrough with appreciable performance in a wide variety of applications with useful security tools. It is considered to be the best choice for discovering complex architecture in high-dimensional data by employing back propagation algorithm. As deep learning has made significant advancements and tremendous performance in numerous applications, the widely used domains of deep learning are business, science and government which further includes adaptive testing, biological image classification, computer vision, cancer detection, natural language processing, object detection, face recognition, handwriting recognition, speech recognition, stock market analysis, smart city and many more. This paper focuses on the concepts of deep learning, its basic and advanced architectures, techniques, motivational aspects, characteristics and the limitations. The paper also presents the major differences between the deep learning, classical machine learning and conventional learning approaches and the major challenges ahead. The main intention of this paper is to explore and present chronologically, a comprehensive survey of the major applications of deep learning covering variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects.},
  url       = {https://doi.org/10.1007/s11831-019-09344-w}
}

@misc{nullspace,
      title={Outlier Detection through Null Space Analysis of Neural Networks}, 
      author={Matthew Cook and Alina Zare and Paul Gader},
      year={2020},
      eprint={2007.01263},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{energy,
      title={Energy-based Out-of-distribution Detection}, 
      author={Weitang Liu and Xiaoyun Wang and John D. Owens and Yixuan Li},
      year={2021},
      eprint={2010.03759},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{vos,
      title={VOS: Learning What You Don't Know by Virtual Outlier Synthesis}, 
      author={Xuefeng Du and Zhaoning Wang and Mu Cai and Yixuan Li},
      year={2022},
      eprint={2202.01197},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oodoverview,
      title={Generalized Out-of-Distribution Detection: A Survey}, 
      author={Jingkang Yang and Kaiyang Zhou and Yixuan Li and Ziwei Liu},
      year={2024},
      eprint={2110.11334},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{doshivelez,
      title={Towards A Rigorous Science of Interpretable Machine Learning}, 
      author={Finale Doshi-Velez and Been Kim},
      year={2017},
      eprint={1702.08608},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@book{molnar,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book},
  publisher  = {Independently published}
}

@misc{gradnorm,
      title={On the Importance of Gradients for Detecting Distributional Shifts in the Wild}, 
      author={Rui Huang and Andrew Geng and Yixuan Li},
      year={2021},
      eprint={2110.00218},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{react,
      title={ReAct: Out-of-distribution Detection With Rectified Activations}, 
      author={Yiyou Sun and Chuan Guo and Yixuan Li},
      year={2021},
      eprint={2111.12797},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@Article{diagnostic,
AUTHOR = {Thunold, Håvard Horgen and Riegler, Michael A. and Yazidi, Anis and Hammer, Hugo L.},
TITLE = {A Deep Diagnostic Framework Using Explainable Artificial Intelligence and Clustering},
JOURNAL = {Diagnostics},
VOLUME = {13},
YEAR = {2023},
NUMBER = {22},
ARTICLE-NUMBER = {3413},
URL = {https://www.mdpi.com/2075-4418/13/22/3413},
PubMedID = {37998548},
ISSN = {2075-4418},
ABSTRACT = {An important part of diagnostics is to gain insight into properties that characterize a disease. Machine learning has been used for this purpose, for instance, to identify biomarkers in genomics. However, when patient data are presented as images, identifying properties that characterize a disease becomes far more challenging. A common strategy involves extracting features from the images and analyzing their occurrence in healthy versus pathological images. A limitation of this approach is that the ability to gain new insights into the disease from the data is constrained by the information in the extracted features. Typically, these features are manually extracted by humans, which further limits the potential for new insights. To overcome these limitations, in this paper, we propose a novel framework that provides insights into diseases without relying on handcrafted features or human intervention. Our framework is based on deep learning (DL), explainable artificial intelligence (XAI), and clustering. DL is employed to learn deep patterns, enabling efficient differentiation between healthy and pathological images. Explainable artificial intelligence (XAI) visualizes these patterns, and a novel “explanation-weighted” clustering technique is introduced to gain an overview of these patterns across multiple patients. We applied the method to images from the gastrointestinal tract. In addition to real healthy images and real images of polyps, some of the images had synthetic shapes added to represent other types of pathologies than polyps. The results show that our proposed method was capable of organizing the images based on the reasons they were diagnosed as pathological, achieving high cluster quality and a rand index close to or equal to one.},
DOI = {10.3390/diagnostics13223413}
}


@InProceedings{occlusion,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}

@misc{subspace,
      title={Out-Of-Distribution Detection With Subspace Techniques And Probabilistic Modeling Of Features}, 
      author={Ibrahima Ndiour and Nilesh Ahuja and Omesh Tickoo},
      year={2020},
      eprint={2012.04250},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nusa,
      title={Outlier Detection through Null Space Analysis of Neural Networks}, 
      author={Matthew Cook and Alina Zare and Paul Gader},
      year={2020},
      eprint={2007.01263},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{vim,
      title={ViM: Out-Of-Distribution with Virtual-logit Matching}, 
      author={Haoqi Wang and Zhizhong Li and Litong Feng and Wayne Zhang},
      year={2022},
      eprint={2203.10807},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gdpr,
  title = {Article 71: European Data Protection Board},
  author = {{European Union}},
  year = {2016},
  url = {https://www.privacy-regulation.eu/en/r71.htm},
  note = {Accessed: February 13, 2024}
}

@article{lenet5,
author = {Lecun, Yann and Bottou, Leon and Bengio, Y. and Haffner, Patrick},
year = {1998},
month = {12},
pages = {2278 - 2324},
title = {Gradient-Based Learning Applied to Document Recognition},
volume = {86},
journal = {Proceedings of the IEEE},
doi = {10.1109/5.726791}
}


@article{ooddl,
AUTHOR = {Cui, Peng and Wang, Jinjia},
TITLE = {Out-of-Distribution (OOD) Detection Based on Deep Learning: A Review},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {21},
ARTICLE-NUMBER = {3500},
URL = {https://www.mdpi.com/2079-9292/11/21/3500},
ISSN = {2079-9292},
ABSTRACT = {Out-of-Distribution (OOD) detection separates ID (In-Distribution) data and OOD data from input data through a model. This problem has attracted increasing attention in the area of machine learning. OOD detection has achieved good intrusion detection, fraud detection, system health monitoring, sensor network event detection, and ecosystem interference detection. The method based on deep learning is the most studied in OOD detection. In this paper, related basic information on OOD detection based on deep learning is described, and we categorize methods according to the training data. OOD detection is divided into supervised, semisupervised, and unsupervised. Where supervised data are used, the methods are categorized according to technical means: model-based, distance-based, and density-based. Each classification is introduced with background, examples, and applications. In addition, we present the latest applications of OOD detection based on deep learning and the problems and expectations in this field.},
DOI = {10.3390/electronics11213500}
}

@misc{mls,
      title={Scaling Out-of-Distribution Detection for Real-World Settings}, 
      author={Dan Hendrycks and Steven Basart and Mantas Mazeika and Andy Zou and Joe Kwon and Mohammadreza Mostajabi and Jacob Steinhardt and Dawn Song},
      year={2022},
      eprint={1911.11132},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1911.11132}, 
}

@misc{adversarial,
      title={Explaining and Harnessing Adversarial Examples}, 
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{odin,
      title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}, 
      author={Shiyu Liang and Yixuan Li and R. Srikant},
      year={2020},
      eprint={1706.02690},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{oodbaseline,
      title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2018},
      eprint={1610.02136},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

# v1.5 report
@article{openood15,
  title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
  author={Zhang, Jingyang and Yang, Jingkang and Wang, Pengyun and Wang, Haoqi and Lin, Yueqian and Zhang, Haoran and Sun, Yiyou and Du, Xuefeng and Li, Yixuan and Liu, Ziwei and Chen, Yiran and Li, Hai},
  journal={arXiv preprint arXiv:2306.09301},
  year={2023}
}

# full-spectrum OOD detection
@article{yang2022fsood,
    title = {Full-Spectrum Out-of-Distribution Detection},
    author = {Yang, Jingkang and Zhou, Kaiyang and Liu, Ziwei},
    journal={arXiv preprint arXiv:2204.05306},
    year = {2022}
}

# generalized OOD detection framework & survey
@article{yang2021oodsurvey,
    title={Generalized Out-of-Distribution Detection: A Survey},
    author={Yang, Jingkang and Zhou, Kaiyang and Li, Yixuan and Liu, Ziwei},
    journal={arXiv preprint arXiv:2110.11334},
    year={2021}
}

# OOD benchmarks
# NINCO
@inproceedings{bitterwolf2023ninco,
    title={In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation},
    author={Julian Bitterwolf and Maximilian Mueller and Matthias Hein},
    booktitle={ICML},
    year={2023},
    url={https://proceedings.mlr.press/v202/bitterwolf23a.html}
}

# SSB
@inproceedings{vaze2021open,
    title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
    author={Vaze, Sagar and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
    booktitle={ICLR},
    year={2022}
}
